\documentclass[dvipdfm]{book}
\newcommand{\VolumeName}{Volume 10.1: Axiom Algebra: Theory}
\input{bookheader.tex}
\mainmatter
\setcounter{chapter}{0} % Chapter 1
\chapter{Interval Arithmetic}
Lambov \cite{Lamb06} defines a set of useful formulas for 
computing intervals using the IEEE-754 floating-point standard.

The first thing to note is that IEEE floating point defaults to 
{\bf round-to-nearest}. However, Lambov sets the rounding mode
to {\bf round to $-\infty$}. Computing lower bounds directly uses
the hardware floating point operations but computing upper bounds
he uses the identity
\[\Delta(x) = -\nabla(-x)\]
so that the upper bound of the pair of bounds is always negated.
That is,
\[ x = \left[\underline{x},\overline{x}\right] = 
\left<\underline{x},-\overline{x}\right> \]

Given that convention
\begin{itemize}
\item the sum of $x$ and $y$ is evaluated by
\[\left<\nabla(\underline{x} + \underline{y}),
  -\nabla(-\overline{x} - \overline{y})\right> \]
\item changing the sign of an interval $x$ is achieved by swapping
the two bounds, that is $\left<-\overline{x},\underline{x}\right>$
\item joining two intervals (that is, finding an interval containing
all numbers in both, or finding the minimum of the lower bounds
and the maximum of the higher bounds) is performed as
\[\left<min(\underline{x},\underline{y}),
  -min((-\overline{x}),(-\overline{y}))\right>\]
\end{itemize}

Lambov defines operations which, under the given rounding condition,
give the tightest bounds.

\section{Addition}
\[ x + y = \left[\underline{x}+\underline{y},\overline{x}+\overline{y}\right]
\subseteq \left<\nabla(\underline{x}+\underline{y})
-\nabla((-\overline{x})+(-\overline{y}))\right>\]
The negated sign of the higher bound ensures the proper direction
of the rounding.

\section{Sign Change}
\[ -x = \left[-\overline{x},-\underline{x}\right] =
\left<-\overline{x},\underline{x}\right> \]
This is a single swap of the two values. No rounding is performed.

\section{Subtraction}
\[ x-y = \left[\underline{x}-\overline{y},\overline{x}-\underline{y}\right]
\subseteq \left<\nabla(\underline{x}+(-\overline{y})),
-\nabla((-\overline{x})+\underline{y})\right>\]
Subtraction is implemented as $x+(-y)$.

\section{Multiplication}
\[ xy = \left[min(\underline{x}\underline{y},
                  \underline{x}\overline{y},
                  \overline{x}\underline{y},
                  \overline{x}\overline{y}),
              max(\underline{x}\underline{y},
                  \underline{x}\overline{y},
                  \overline{x}\underline{y},
                  \overline{x}\overline{y})\right]
\]
The rounding steps are part of the operation so all 8 multiplications
are required. Lambov notes that since
\[\Delta(\nabla(r)+\epsilon) \ge \Delta(r) \]
for $\epsilon$ being the smallest representable positive number, one
can do with 4 multiplications at the expense of some accuracy.

In Lambov's case he makes the observation that
\[
xy=\left\{
\begin{array}{l}
\left[min(\underline{x}\underline{y},\overline{x}\underline{y}),
      max(\overline{x}\overline{y},\underline{x}\overline{y})\right],
      {\rm\ if\ }0 \le \underline{x} \le \overline{x}\\
\left[min(\underline{x}\overline{y},\overline{x}\underline{y}),
      max(\overline{x}\overline{y},\underline{x}\underline{y})\right],
      {\rm\ if\ }\underline{x} < 0 \le \overline{x}\\
\left[min(\underline{x}\overline{y},\overline{x}\overline{y}),
      max(\overline{x}\underline{y},\underline{x}\underline{y})\right],
      {\rm\ if\ }\underline{x} \le \overline{x} < 0
\end{array}
\right.
\]
from which he derives the formula actually used
\[xy \subseteq \left<min(\nabla(a\underline{x}),\nabla(b(-\overline{x}))),
-min(\nabla(c(-\overline{x})),\nabla(d\underline{x}))\right>
\]
where 
\[
\begin{array}{rcl}
a & = & \left\{ 
\begin{array}{rl}
\underline{y} & {\rm if\ } 0 \le \underline{x}\\
-(-\overline{y}) & {\rm otherwise}
\end{array}\right.\\
b & = & \left\{ 
\begin{array}{rl}
-\underline{y} & {\rm if\ } (-\overline{x}) \le 0\\
(-\overline{y}) & {\rm otherwise}
\end{array}\right.\\
c & = & \left\{
\begin{array}{rl}
-(-\overline{y}) & {\rm if\ } (-\overline{x}) \le 0\\
\underline{y} & {\rm otherwise}
\end{array}\right.\\
d & = & \left\{
\begin{array}{rl}
(-\overline{y}) & {\rm if\ } 0 \le \underline{x}\\
-\underline{y} & {\rm otherwise}
\end{array}\right.
\end{array}
\]
which computes the rounded results of the original multiplication
formula but achieves better performance.

\section{Multiplication by a positive number}
If one of the numbers is known to be positive (e.g. a constant) then 
\[ {\rm if\ }x > 0 {\rm\ then\ } xy \equiv
\left[min(\underline{x}\underline{y},\overline{x}\underline{y}),
       max(\overline{x}\overline{y},\underline{x}\overline{y})
\right]
\]
This formula is faster than the general multiplication formula.

\section{Multiplication of Two Positive Numbers}

If both multiples are positive simply change the sign of the 
higher bound on one of the arguments prior to multiplication.
If one of the numbers is a constant this can be arranged to
skip the sign change.

\section{Division}
Division is an expensive operation.
\[\frac{x}{y} =
\left[
min\left(\frac{\underline{x}}{\underline{y}},
         \frac{\underline{y}}{\overline{y}},
         \frac{\overline{x}}{\underline{y}},
         \frac{\overline{x}}{\overline{y}}\right),
max\left(\frac{\underline{x}}{\underline{y}},
         \frac{\underline{x}}{\overline{y}},
         \frac{\overline{x}}{\underline{y}},
         \frac{\overline{x}}{\overline{y}}\right)
\right]\]
which is undefined if $0 \in y$. To speed up the computation Lambov
uses the identity
\[ \frac{x}{y} = x\frac{1}{y} \]

Lambov does a similar analysis to improve the overall efficiency.
\[\frac{x}{y} = \left\{
\begin{array}{rl}
\left[min(\frac{\underline{x}}{\underline{y}},
          \frac{\underline{x}}{\overline{y}}),
      max(\frac{\overline{x}}{\underline{y}},
          \frac{\overline{x}}{\overline{y}})\right], &
{\rm if\ }0 < \underline{y} \le \overline{y}\\
{\rm exception} & 
{\rm if\ }\underline{y} \le 0 \le \overline{y}\\
\left[min(\frac{\overline{x}}{\underline{y}},
          \frac{\overline{x}}{\overline{y}}),
      max(\frac{\underline{x}}{\underline{y}},
          \frac{\underline{x}}{\overline{y}})\right], &
{\rm if\ }\underline{y} \le \overline{y} \le 0
\end{array}
\right.
\]

The formula he uses is
\[\frac{x}{y} \subseteq
\left<min\left(\nabla\left(\frac{a}{\underline{y}}\right),
               \nabla\left(\frac{-a}{(-\overline{y})}\right)\right),
     -min\left(\nabla\left(\frac{-b}{(-\overline{y})}\right),
               \nabla\left(\frac{b}{\underline{y}}\right)\right)
\right>
\]
where
\[
\begin{array}{rcl}
a & = & \left\{
\begin{array}{rl}
\underline{x} & {\rm if\ }(-\overline{y}) \le 0\\
-(-\overline{x}) & {\rm otherwise}
\end{array}\right.\\
b & = & \left\{
\begin{array}{rl}
(-\overline{x}) & {\rm if\ }0 \le \underline{y}\\
-\underline{x} & {\rm otherwise}
\end{array}\right.
\end{array}
\]

\section{Reciprocal}
\[\frac{1}{x} = \left[\frac{1}{\overline{x}},\frac{1}{\underline{x}}\right]
\subseteq 
\left<\nabla\left(\frac{-1}{(-\overline{x})}\right),
      \nabla\left(\frac{-1}{\underline{x}}\right)
\right>
\]
which is undefined if $0 \in x$. Lambov implements this by checking for
zero, followed by division of $-1$ by the argument and swapping the
two components.   

\section{Absolute Value}

\[\abs{x} = 
\left[max(\underline{x},-\overline{x},0),
      max(-\underline{x},\overline{x})\right] =
\left<max(0,\underline{x},(-\overline{x})),
     -min(\underline{x},(-\overline{x}))\right>
\]

\section{Square}
\[x^2 = \abs{x}\abs{x}\]
using multiplication by positive numbers, mentioned above.

\section{Square Root}

\[\sqrt{x} = \left[\sqrt{\underline{x}},\sqrt{\overline{x}}\right]\]
which is defined if $0 \le \underline{x}$

Lambov notes that this formula has a rounding issue. He notes that
since
\[\Delta(r) \le -\nabla(-\epsilon - \nabla(r))\]
he uses the formula
\[\sqrt{x} \subseteq
\left\{
\begin{array}{l}
\left<\nabla\left(\sqrt{\underline{x}}\right),
     -\nabla\left(\sqrt{-(-\overline{x})}\right)\right>,
{\rm\ if\ }\nabla\left(\nabla\left(\sqrt{-(-\overline{x})}\right)\right)^2
= -(-\overline{x})\\
\left<\nabla\left(\sqrt{\underline{x}}\right),
      \nabla\left(\nabla\left(-\epsilon-\sqrt{-(-\overline{x})}\right)
      \right)\right>,
{\rm\ otherwise}
\end{array}
\right.
\]
where $\epsilon$ is the smallest representable positive number.

The first branch of this formula is only satisfied if the result of
$\sqrt{-(-\overline{x})}$ is exactly representable, in which case
\[\nabla\left(\sqrt{-(-\overline{x})}\right) =
  \nabla\left(\sqrt{-(-\overline{x})}\right)
\]
otherwise the second branch of the formula adjusts the high bound
to the next representable number. If tight bounds are not required
the second branch is always sufficient.

If the argument is entirely negative, the implementation will raise
an exception. If it contains a negative part, the implementation will
crop it to only its non-negative part to allow that computations
such as $\sqrt{0}$ ca be carried out in exact real arithmetic.

\chapter{Integration}

An {\sl elementary function}\cite{Bro98b}
\index{elementary function}
of a variable $x$ is a function that can
be obtained from the rational functions in $x$ by repeatedly adjoining
a finite number of nested logarithms, exponentials, and algebraic
numbers or functions. Since $\sqrt{-1}$ is elementary, the
trigonometric functions and their inverses are also elementary (when
they are rewritten using complex exponentials and logarithms) as well
as all the ``usual'' functions of calculus. For example,

\begin{equation}\label{Int1}
\sin(x+\tan(x^3-\sqrt{x^3-x+1}))
\end{equation}
is elementary when rewritten as
\[
\frac{\sqrt{-1}}{2}(e^{t-x\sqrt{-1}}-e^{x\sqrt{-1}-t})
{\rm\ where\ }
t=\frac{1-e^{2\sqrt{-1}(x^3-\sqrt{x^3-x+1})}}
{1+e^{2\sqrt{-1}(x^3-\sqrt{x^3-x+1})}}
\]
This tutorial describes recent algorithmic solutions to the {\sl
problem of integration in finite terms}: 
\index{integration in finite terms}
to decide in a finite number
of steps whether a given elementary funcction has an elementary
indefinite integral, and to compute it explicitly if it exists. While
this problem was studied extensively by Abel and Liouville during the
last century, the difficulties posed by algebraic functions caused
Hardy (1916) to state that ``there is reason to suppose that no such
method can be given''. This conjecture was eventually disproved by
Risch (1970), who described an algorithm for this problem in a series
of reports \cite{Ostr1845,Risc68,Risc69a,Risc69b,Risc70}. 
In the past 30 years, this procedure
has been repeatedly improved, extended and refined, yielding practical
algorithms that are now becoming standard and are implemented in most
of the major computer algebra systems. In this tutorial, we outline
the above algorithms for various classes of elementary functions,
starting with rational functions and progressively increasing the
class of functions up to general elementary functions. Proofs of
correctness of the algorithms presented here can be found in several
of the references, and are generally too long and too detailed to be
described in this tutorial.

{\bf Notations}: we write $x$ for the variable of integration, and $\prime$
for the derivation $d/dx$. $\mathbb{Z}$,$\mathbb{Q}$,$\mathbb{R}$,and
$\mathbb{C}$ denote respectively the integers, rational, real and
complex numbers. All fields are commutative and, except when mentioned
explicitly otherwise, have characteristic 0. If $K$ is a field, then
$\overline{K}$ denotes its algebraic closure. For a polynomial $p$, 
pp($p$) denotes the primitive
part of $p$, {\sl i. e.} $p$ divided by the gcd of its coefficients.
\section{Rational Functions}
By a {\sl rational function}, we mean a quotient of polynomials in the
integration variable $x$. This means that other functions can appear
in the integrand, provided they do not involve $x$, hence that the
coefficients of our polynomials in $x$ lie in an arbitrary field $K$
satisfying: $\forall{a} \in K,\ a^{\prime}=0$.

\subsection{The full partial-fraction algorithm}
This method, which dates back to Newton, Leibniz, and Bernoulli,
should not be used in practice, yet it remains the method found in
most calculus tests and is often taught. Its major drawback is the
factorization of the denominator of the integrand over the real or
complex numbers. We outline it because it provides the theoretical
foundations for all the subsequent algorithms. Let 
$f \in \mathbb{R}(x)$ be our integrand, and write 
$f=P+A/D$ where $P, A, D \in \mathbb{R}[x]$, gcd$(A,D)=1$, and
deg$(A) < $deg$(D)$. Let
\[
D=c\prod_{i=1}^n(x-a_i)^{e_i}\prod_{j=1}^m(x^2+b_jx+c_j)^{f_j}
\]
be the irreducible factorization of $D$ over $\mathbb{R}$, where $c$,
the $a_i$'s, $b_j$'s and $c_j$'s are in $\mathbb{R}$ and the $e_i$'s
and $f_j$'s are positive integers. Computing the partial fraction
decomposition of $f$, we get
\[
f=P+\sum_{i=1}^n\sum_{k=1}^{e_i}\frac{A_{ik}}{(x-a_i)^k}
+\sum_{j=1}^m\sum_{k=1}^{f_i}\frac{B_{jk}x+C_{jk}}{(x^2+b_jx+c_j)^k}
\]
where the $A_{ik}$'s, $B_{jk}$'s, and $C_{jk}$'s are in
$\mathbb{R}$. Hence,
\[
\int{f}=\int{P}+\sum_{i=1}^n\sum_{k=1}^{e_i}\int{\frac{A_{ik}}{(x-a_i)^k}}
+\sum_{j=1}^m\sum_{k=1}^{f_i}\int{\frac{B_{jk}x+C_{jk}}{(x^2+b_jx+c_j)^k}}
\]
Computing $\int{P}$ poses no problem (it will for any other class of
functions), and for the other terms we have
\begin{equation}\label{Int2}
\int{\frac{A_{ik}}{(x-a_i)^k}}=\left\{
\begin{array}{lc}
A_{ik}(x-a_i)^{1-k}/(1-k)&{\rm if\ } k > 1\\
A_{i1}\log(x-a_i)&{\rm if\ } k = 1\\
\end{array}
\right.
\end{equation}
and, noting that $b_j^2-4c_j < 0$ since $x^2+b_jx+c_j$ is irreducible
in $\mathbb{R}$[x].
\[
\int\frac{B_{j1}x+C_{j1}}{(x^2+b_jx+c_j)}=
\frac{B_{j1}}{2}\log(x^2+b_jx+c_j)
+\frac{2C_{j1}-b_jB_{j1}}{\sqrt{4c_j-b_j^2}}
arctan\left(\frac{2x+b_j}{\sqrt{4c_j-b_j^2}}\right)
\]
and for $k > 1$,
\[
\begin{array}{lcl}
\displaystyle
\int{\frac{B_{jk}x+C_{jk}}{(x^2+b_jx+c_j)^k}}&=&
\displaystyle\frac{(2C_{jk}-b_jB_{jk})x+b_jC_{jk}-2c_jB_{jk}}
{(k-1)(4c_j-b_j^2)(x^2+b_jx+c_j)^{k-1}}\\
&&\displaystyle+\int{\frac{(2k-3)(2C_{jk}-b_jB_{jk})}
{(k-1)(4c_j-b_j^2)(x^2+b_jx+c_j)^{k-1}}}\\
\end{array}
\]
This last formula is then used recursively until $k=1$.

An alternative is to factor $D$ linearly over $\mathbb{C}$:
$D=\prod_{i=1}^q(x-\alpha_i)^{e_i}$, and then use \ref{Int2} on each term of
\begin{equation}\label{Int3}
f=P+\sum_{i=1}^q\sum_{j=1}^{e_i}\frac{A_{ij}}{(x-\alpha_i)^j}
\end{equation}
Note that this alternative is applicable to coefficients in any field
$K$, if we factor $D$ linearly over its algebraic closure
$\overline{K}$, and is equivalent to expanding $f$ into its Laurent
series at all its finite poles, since that series at 
$x=\alpha_i \in \overline{K}$ is
\[
f=\frac{A_{ie_i}}{(x-\alpha_i)^{e_i}}
+\cdots
+\frac{A_{i2}}{(x-\alpha_i)^2}
+\frac{A_{i1}}{(x-\alpha_i)}
+\cdots
\]
where the $A_{ij}$'s are the same as those in \ref{Int3}. Thus, this approach
can be seen as expanding the integrand into series around all the
poles (including $\infty$), then integrating the series termwise, and
then interpolating for the answer, by summing all the polar terms,
obtaining the integral of \ref{Int3}. In addition, this alternative shows
that any rational function $f \in K(x)$ has an elementary integral of
the form
\begin{equation}\label{Int4}
\int{f}=v+c_1\log(u_1)+\cdots+c_m\log(u_m)
\end{equation}
where $v,u_1,\ldots,u_m \in \overline{K}(x)$ are the rational
functions, and $c_1,\ldots,c_m \in \overline{K}$ are constants. The
original Risch algorithm is essentially a generalization of this
approach that searches for integrals of arbitrary elementary functions
in a form similar to \ref{Int4}.

\subsection{The Hermite reduction}
The major computational inconvenience of the full partial fraction
approach is the need to factor polynomials over $\mathbb{R}$,
$\mathbb{C}$, or $\overline{K}$, thereby introducing algebraic numbers
even if the integrand and its integral are both in $\mathbb{Q}(x)$. On
the other hand, introducing algebraic numbers may be necessary, for
example it is proven in \cite{Risc69a} that any field containing an
integral of $1/(x^2+2)$ must also contain $\sqrt{2}$. Modern research
has yielded so-called ``rational'' algorithms that
\begin{itemize}
\item compute as much of the integral as possible with all
calculations being done in $K(x)$, and
\item compute the minimal algebraic extension of $K$ necessary to
express the integral
\end{itemize}
The first rational algorithms for integration date back to the
$19^{{\rm th}}$ century, when both Hermite \cite{Herm1872} and
Ostrogradsky \cite{Ostr1845} invented methods for 
computing the $v$ of \ref{Int4}
entirely within $K(x)$. We describe here only Hermite's method, since
it is the one that has been generalized to arbitrary elementary
functions. The basic idea is that if an irreducible $p \in K[x]$
appears with multiplicity $k > 1$ in the factorization of the
denominator of the integrand, then \ref{Int2} implies that it appears with
multiplicity $k-1$ in the denominator of the integral. Furthermore, it
is possible to compute the product of all such irreducibles for each
$k$ without factoring the denominator into irreducibles by computing
its {\sl squarefree factorization}, {\sl i.e} a factorization
$D=D_1D_2^2\cdots D_m^m$, where each $D_i$ is squarefree and 
gcd$(D_i,D_j)=1$ for $i \ne j$. A straightforward way to compute it is
as follows: let $R={\rm gcd}(D,D^{\prime})$, 
then $R=D_2D_2^3\cdots D_m^{m-1}$, so 
$D/R=D_1D_2\cdots D_m$ and gcd$(R,D/R)=D_2\cdots D_m$, which implies
finally that
\[
D_1=\frac{D/R}{{\rm gcd}(R,D/R)}
\]
Computing recursively a squarefree factorization of $R$ completes the
one for $D$. Note that \cite{Yun76} presents a more efficient method for
this decomposition. Let now $f \in K(x)$ be our integrand, and write
$f=P+A/D$ where $P,A,D \in K[x]$, gcd$(A,D)=1$, and\\
${\rm deg}(A)<{\rm deg}(D)$. 
Let $D=D_1D_2^2\cdots D_m^m$ be a squarefree factorization of $D$ and
suppose that $m \ge 2$ (otherwise $D$ is already squarefree). Let then
$V=D_m$ and $U=D/V^m$. Since gcd$(UV^{\prime},V)=1$, we can use the
extended Euclidean algorithm to find $B,C \in K[x]$ such that
\[
\frac{A}{1-m}=BUV^{\prime}+CV
\]
and ${\rm deg}(B) < {\rm deg}(V)$. 
Multiplying both sides by $(1-m)/(UV^m)$ gives
\[
\frac{A}{UV^m}=\frac{(1-m)BV^{\prime}}{V^m}+\frac{(1-m)C}{UV^{m-1}}
\]
so, adding and subtracting $B^{\prime}/V^{m-1}$ to the right hand side, we
get
\[
\frac{A}{UV^m}=\left(\frac{B^{\prime}}{V^{m-1}}-
\frac{(m-1)BV^{\prime}}{V^m}\right)
+\frac{(1-m)C-UB^{\prime}}{UV^{m-1}}
\]
and integrating both sides yields
\[
\int\frac{A}{UV^m}=\frac{B}{V^{m-1}}+\int\frac{(1-m)C-UB^{\prime}}{UV^{m-1}}
\]
so the integrand is reduced to one with a smaller power of $V$ in the
denominator. This process is repeated until the denominator is
squarefree, yielding $g,h \in K(x)$ such that $f=g^{\prime}+h$ and $h$ has
a squarefree denominator.

\subsection{The Rothstein-Trager and Lazard-Rioboo-Trager algorithms}
Following the Hermite reduction, we only have to integrate fractions
of the form $f=A/D$ with ${\rm deg}(A)<{\rm deg}(D)$ and $D$ squarefree. It
follows from \ref{Int2} that
\[
\int{f}=\sum_{i=1}^n a_i\log(x-\alpha_i)
\]
where the $\alpha_i$'s are the zeros of $D$ in $\overline{K}$, and the
$a_i$'s are the residues of $f$ at the $\alpha_i$'s. The problem
is then to compute those residues without splitting $D$. Rothstein
\cite{Roth77} and Trager \cite{Trag76} independently proved that the
$\alpha_i$'s are exactly the zeros of
\begin{equation}\label{Int5}
R={\rm resultant}_x(D,A-tD^{\prime}) \in K[t]
\end{equation}
and that the splitting field of $R$ over $K$ is indeed the minimal
algebraic extension of $K$ necessary to express the integral in the
form \ref{Int4}. The integral is then given by
\begin{equation}\label{Int6}
\int\frac{A}{D}=\sum_{i=1}^m\sum_{a|R_i(a)=0}a\log(\gcd(D,A-aD^{\prime}))
\end{equation}
where $R=\prod_{i=1}^m R_i^{e_i}$ is the irreducible factorization of
$R$ over $K$. Note that this algorithm requires factoring $R$ into
irreducibles over $K$, and computing greatest common divisors in
$(K[t]/(R_i))[x]$, hence computing with algebraic numbers. Trager and
Lazard \& Rioboo \cite{Laza90} independently discovered that those
computations can be avoided, if one uses the subresultant PRS
algorithm to compute the resultant of \ref{Int5}: let 
$(R_0,R_1,\ldots R_k\ne 0,0,\ldots)$ be the subresultant PRS with
respect to $x$ of $D$ and $A-tD^{\prime}$ and $R=Q_1Q_2^2\ldots Q_m^m$ be a 
{\sl squarefree} factorization of their resultant. Then,
\[
\sum_{a|Q_i(a)=0} a\log(\gcd(D,A-aD^{\prime}))=\hbox{\hskip 5.0cm}
\]
\[
\left\{
\begin{array}{ll}
\sum_{a|Q_i(a)=0} a \log(D) & {\rm if\ }i = {\rm deg}(D)\\
\sum_{a|Q_i(a)=0} a \log({\rm pp}_x(R_{k_i})(a,x))&
{\rm where\ }{\rm deg}(R_{k_i})=i,1 \le k_i \le n\\
&{\rm if\ }i < {\rm deg}(D)
\end{array}
\right.
\]
Evaluating ${\rm pp}_x(R_{k_i})$ at $t=a$ where $a$ is a root of $Q_i$
is equivalent to reducing each coefficient with respect to $x$ of
${\rm pp}_x(R_{k_i})$ module $Q_i$, hence computing in the algebraic
extension $K[t]/(Q_i)$. Even this step can be avoided: it is in fact
sufficient to ensure that $Q_i$ and the leading coefficient with
respect to $x$ of $R_{k_i}$ do not have a nontrivial common factor,
which implies then that the remainder by $Q_i$ is nonzero, see
\cite{Muld97} for details and other alternatives for computing
${\rm pp}_x(R_{k_i})(a,x)$

\section{Algebraic Functions}
By an {\sl algebraic function}, we mean an element of a finitely
generated algebraic extension $E$ of the rational function field
$K(x)$. This includes nested radicals and implicit algebraic
functions, not all of which can be expressed by radicals. It turns out
that the algorithms we used for rational functions can be extended to
algebraic functions, but with several difficulties, the first one
being to define the proper analogues of polynomials, numerators and
denominators. Since $E$ is algebraic over $K(x)$, for any
$\alpha \in E$, there exists a polynomial $p \in K[x][y]$ such that 
$p(x,\alpha)=0$. We say that $\alpha \in E$ is {\sl integral over}
$K[x]$ if there is a polynomial $p \in K[x][y]$, {\sl monic in y},
such that $p(x,\alpha)=0$. Integral elements are analogous to
polynomials in that their value is defined for any 
$x \in \overline{K}$ (unlike non-integral elements, which must have at
least one pole in $\overline{K}$). The set
\[
{\bf O}_{K[x]} = \{\alpha \in E {\rm\ such\ that\ }\alpha
{\rm\ is\ integral\ over\ }K[x]\}
\]
is called the {\sl integral closure of} $K[x]$ {\sl in E}. It is a
ring and a finitely generated $K[x]$-module. Let $\alpha \in E^{*}$ be
any element and $p=\sum_{i=0}^m a_iy^i \in K[x][y]$ be such that
$p(x,\alpha)=0$ and $a_m \ne 0$. Then, $q(x,a_my)=0$ where
$q=y^m+\sum_{i=0}^{m-1} a_ia_m^{m-i-1}y^i$ is monic in $y$, 
so $a_my \in {\bf O}_{K[x]}$. We need a canonical representation
for algebraic functions similar to quotients of polynomials for
rational functions. Expressions as quotients of integral functions are
not unique, for example, $\sqrt{x}/x=x/\sqrt{x}$. However, $E$ is a
finite-dimensional vector space over $K(x)$, so let $n=[E:K(x)]$ and
$w=(w_1,\ldots,w_n)$ be any basis for $E$ over $K(x)$. By the above
remark, there are $a_1,\ldots,a_n \in K(x)^{*}$ such that
$a_iw_i \in {\bf O}_{K[x]}$ for each $i$. Since
$(a_1w_1,\ldots,a_nw_n)$ is also a basis for $E$ over $K(x)$, we can
assume without loss of generality that the basis $w$ is composed of
integral elements. Any $\alpha \in E$ can be written uniquely as
$\alpha = \sum_{i=1}^n f_iw_i$ for $f_1,\ldots,f_n \in K(x)$, and
putting the $f_i$'s over a monic common denominator $D \in K[x]$, we
get an expression
\[
\alpha = \frac{A_1w_1+\ldots+A_nw_n}{D}
\]
where $A_1,\ldots,A_n \in K[x]$ and $\gcd(D,A_1,\ldots,A_n)=1$. We
call $\sum_{i=1}^n A_iw_i \in {\bf O}_{K[x]}$ and
$D \in K[x]$ respectively the {\sl numerator} and {\sl denominator} of
$\alpha$ with respect to $w$. They are defined uniquely once the basis
$w$ is fixed.
\subsection{The Hermite reduction}
Now that we have numerators and denominators for algebraic functions,
we can attempt to generalize the Hermite reduction of the previous
section, so let $f \in E$ be our integrand, 
$w=(w_1,\ldots,w_n) \in {{\bf O}_{K[n]}}^{n}$ be a basis for $E$
over $K(x)$ and let $\sum_{i=1}^m A_iw_i \in {\bf O}_{K[x]}$
and $D \in K[x]$ be the numerator and denominator of $f$ with respect
to $w$, Let $D=D_1D_2^2\ldots D_m^m$ be a squarefree factorization of
$D$ and suppose that $m \ge 2$. Let then $V=D_m$ and $U=D/V^m$, and we
ask whether we can compute 
$B=\sum_{i=1}^n B_iw_i \in {\bf O}_{K[x]}$ and $h \in E$ such
that ${\rm deg}(B_i) < {\rm deg}(V)$ for each $i$,
\begin{equation}\label{Int7}
\int\frac{\sum_{i=1}^n A_iw_i}{UV^m}=\frac{B}{V^{m-1}}+\int{h}
\end{equation}
and the denominator of $h$ with respect to $w$ has no factor of order
$m$ or higher. This turns out to reduce to solving the following
linear system
\begin{equation}\label{Int8}
f_1S_1+\ldots+f_nS_n=A_1w_1+\ldots+A_nw_n
\end{equation}
for $f_1,\ldots,f_n \in K(x)$, where
\begin{equation}\label{Int9}
S_i=UV^m\left(\frac{w_i}{V^{m-1}}\right)^{\prime}\quad{\rm for\ }1\le i\le n
\end{equation}
Indeed, suppose that \ref{Int8} has a solution $f_1,\ldots,f_n \in K(x)$, and
write $f_i=T_i/Q$, where $Q,T_1,\ldots,T_n \in K[x]$ and
$\gcd(Q,T_1,\ldots,T_n)=1$. Suppose further that $\gcd(Q,V)=1$. Then,
we can use the extended Euclidean algorithm to find $A,R \in K[x]$
such that $AV+RQ=1$, and Euclidean division to find $Q_i,B_i \in K[x]$
such that ${\rm deg}(B_i)<{\rm deg}(V)$ when $B_i \ne 0$ 
and $RT_i=VQ_i+B_i$ for
each $i$. We then have
\[
\begin{array}{lcl}
h&=&\displaystyle
f-\left(\frac{\sum_{i=1}^n B_iw_i}{V^{m-1}}\right)^{\prime}\\
&&\\
&=&\displaystyle
\frac{\sum_{i=1}^nA_iw_i}{UV^m}
-\frac{\sum_{i=1}^nB_i^{\prime}w_i}{V^{m-1}}
-\sum_{i=1}^n(RT_i-VQ_i)\left(\frac{w_i}{V^{m-1}}\right)^{\prime}\\
&&\\
&=&\displaystyle
\frac{\sum_{i=1}^nA_iw_i}{UV^m}
-\frac{R\sum_{i=1}^nT_iS_i}{UV^m}
+V\sum_{i=1}^nQ_i\left(\frac{w_i}{V^{m-1}}\right)^{\prime}
-\frac{\sum_{i=1}^nB_i^{\prime}w_i}{V^{m-1}}\\
&&\\
&=&\displaystyle
\frac{(1-RQ)\sum_{i=1}^nA_iw_i}{UV^m}
+\frac{\sum_{i=1}^nQ_iw_i^{\prime}}{V^{m-2}}
-(m-1)V^{\prime}\frac{\sum_{i=1}^nQ_iw_i}{V^{m-1}}
-\frac{\sum_{i=1}^nB_i^{\prime}w_i}{V^{m-1}}\\
&&\\
&=&\displaystyle
\frac{\sum_{i=1}^nAA_iw_i}{UV^{m-1}}
-\frac{\sum_{i=1}^n((m-1)V^{\prime}Q_i+B_i^{\prime})w_i}{V^{m-1}}
+\frac{\sum_{i=1}^nQ_iw_i^{\prime}}{V^{m-2}}
\end{array}
\]
Hence, if in addition the denominator of $h$ has no factor of order
$m$ or higher, then $B=\sum_{i=1}^nB_iw_i \in {\bf O}_{K[x]}$
and $h$ solve \ref{Int7} and we have reduced the integrand. Unfortunately, it
can happen that the denominator of $h$ has a factor of order $m$ or
higher, or that \ref{Int8} has no solution in $K(x)$ whose denominator is
coprime with $V$, as the following example shows.

\noindent
{\bf Example 1} Let $E=K(x)[y]/(y^4+(x^2+x)y-x^2)$ {\sl with basis}
$w=(1,y,y^2,y^3)$ {\sl over} $K(x)$ {\sl and consider the integrand}
\[
f=\frac{y^3}{x^2}=\frac{w_4}{x^2}\in E
\]
We have $D=x^2$, so $U=1,V=x$ and $m=2$.
Then, $S_1=x^2(1/x)^{\prime}=-1$,
\[
\begin{array}{lcl}
\displaystyle S_2&=&
\displaystyle x^2\left(\frac{y}{x}\right)^{\prime}\\
&&\\
\displaystyle &=&
\displaystyle \frac{24(1-x^2)y^3+32x(1-x)y^2-(9x^4+45x^3+209x^2+63x+18)y
-18x(x^3+x^2-x-1)}{27x^4+108x^3+418x^2+108x+27}\\
&&\\
\displaystyle S_3&=&x^2\left(\frac{y^2}{x}\right)^{\prime}\\
&&\\
\displaystyle &=&
\displaystyle\frac{64x(1-x)y^3+9(x^4+2x^3-2x-1)y^2+12x(x^3+x^2-x-1)y+
48x^2(1-x^2)}{27x^4+108x^3+418x^2+108x+27}\\
&&\\
and&&\\
&&\\
\displaystyle S_4&=&x^2\left(\frac{y^3}{x}\right)^{\prime}\\
&&\\
\displaystyle &=&
\displaystyle\frac{(27x^4+81x^3+209x^2+27x)y^3+18x(x^3+x^2-x-1)y^2
+24x^2(x^2-1)y+96x^3(1-x)}
{27x^4+108x^3+418x^2+108x+27}
\end{array}
\]
so \ref{Int8} becomes
\begin{equation}\label{Int10}
M
\left(
\begin{array}{c}
f_1\\
f_2\\
f_3\\
f_4
\end{array}
\right)=
\left(
\begin{array}{c}
0\\
0\\
0\\
1
\end{array}
\right)
\end{equation}
where
\[
M=\left(
\begin{array}{cccc}
\displaystyle -1&\frac{-18x(x^3+x^2-x-1)}{F}&\frac{48x^2(1-x^2)}{F}
&\frac{96x^3(1-x)}{F}\\
\displaystyle 0&\frac{-(9x^4+45x^3+209x^2+63x+18)}{F}
&\frac{12x(x^3+x^2-x-1)}{F}&\frac{24x^2(x^2-1)}{F}\\
\displaystyle 0&\frac{32x(1-x)}{F}&\frac{9(x^4+2x^3-2x-1)}{F}
&\frac{18x(x^3+x^2-x-1)}{F}\\
\displaystyle 0&\frac{24(1-x^2)}{F}&\frac{64x(1-x)}{F}
&\frac{(27x^4+81x^3+209x^2+27x)}{F}
\end{array}
\right)
\]
and $F=27x^4+108x^3+418x^2+108x+27$. The system \ref{Int10} admits a unique
solution $f_1=f_2=0, f_3=-2$ and $f_4=(x+1)/x$, whose denominator is
not coprime with $V$, so the Hermite reduction is not applicable.

The above problem was first solved by Trager \cite{Trag84}, who proved 
that if $w$ is an {\sl integral basis, i.e.} its elements generate 
${\bf O}_{K[x]}$ over $K[x]$, then the system \ref{Int8} always has a
unique solution in $K(x)$ when $m > 1$, and that solution always has a
denominator coprime with V. Furthermore, the denominator of each
$w_i^{\prime}$ must be squarefree, implying that the denominator of $h$ is
a factor of $FUV^{m-1}$ where $F \in K[x]$ is squarefree and coprime
with $UV$. He also described an algorithm for computing an integral
basis, a necessary preprocessing for his Hermite reduction. The main
problem with that approach is that computing the integral basis,
whether by the method of \cite{Trag84} or the local alternative \cite{Hoei94},
can be in general more expansive than the rest of the reduction
process. We describe here the lazy Hermite reduction \cite{Bron98}, which
avoids the precomputation of an integral basis. It is based on the
observation that if $m > 1$ and \ref{Int8} does not have a solution allowing
us to perform the reduction, then either
\begin{itemize}
\item the $S_i$'s are linearly dependent over $K(x)$, or
\item \ref{Int8} has a unique solution in $K(x)$ whose denominator has a
nontrivial common factor with $V$, or
\item the denominator of some $w_i$ is not squarefree
\end{itemize}
In all of the above cases, we can replace our basis $w$ by a new one,
also made up of integral elements, so that that $K[x]$-module
generated by the new basis strictly contains the one generated by $w$:

\noindent
{\bf Theorem 1 (\cite{Bron98})} {\sl Suppose that $m \ge 2$ and that 
$\{S_1,\ldots,S_n\}$ as given by \ref{Int9} are linearly dependent over $K(x)$,
and let $T_1,\ldots,T_n \in K[x]$ be not all 0 and such that
$\sum_{i=1}^n T_iS_i=0$. Then,
\[
w_0=\frac{U}{V}\sum_{i=1}^n T_iw_i \in {\bf O}_{K[x]}
\]
Furthermore, if $\gcd(T_1,\ldots,T_n)=1$ then
$w_0 \notin K[x]w_1+\cdots+K[x]w_n$.}

\noindent
{\bf Theorem 2 (\cite{Bron98})} {\sl Suppose that $m \ge 2$ and that
$\{S_1,\ldots,S_n\}$ as given by \ref{Int9} are linearly independent over
$K(x)$, and let $Q,T_1,\ldots,T_n \in K[x]$ be such that
\[
\sum_{i=1}^n A_iw_i = \frac{1}{Q}\sum_{i=1}^n T_iS_i
\]
Then,
\[
w_0=\frac{U(V/\gcd(V,Q))}{\gcd(V,Q)}\sum_{i=1}^n T_iw_i \in 
{\bf O}_{K[x]}
\]
Furthermore, 
if $\gcd(Q,T_1,\ldots,T_n)=1$ and $\deg(\gcd(V,Q)) \ge 1$, then
$w_0 \notin K[x]w_1+\cdots+K[x]w_n$.}

{\bf Theorem 3 (\cite{Bron98})} {\sl Suppose that the denominator $F$ of
some $w_i$ is not squarefree, and let $F=F_1F_2^2\cdots F_k^k$ be its
squarefree factorization. Then,}
\[
w_0=F_1\cdots F_kw_i^{\prime} \in {\bf O}_{K[x]} \backslash
(K[x]w_1+\cdots+K[x]w_n).
\]

The lazy Hermite reduction proceeds by solving the system \ref{Int8} in
$K(x)$. Either the reduction will succeed, or one of the above
theorems produces an element
$w_0 \in {\bf O}_{K[x]} \backslash (K[x]w_1+\cdots+K[x]w_n).$ Let then
$\sum_{i=1}^n C_iw_i$ and $F$ be the numerator and denominator of
$w_0$ with respect to $w$. Using Hermitian row reduction, we can zero
out the last row of
\[
\left(
\begin{array}{cccc}
F  &   &      &\\
   &F  &      &\\
   &   &\ddots&\\
   &   &      &F\\
C_1&C_2&\cdots&C_n
\end{array}
\right)
\]
obtaining a matrix of the form
\[
\left(
\begin{array}{cccc}
C_{1,1} & C_{1,2} & \cdots & C_{1,n}\\
C_{2,1} & C_{2,2} & \cdots & C_{2,n}\\
\vdots  & \vdots  &        & \vdots\\
C_{n,1} & C_{n,2} & \cdots & C_{n,n}\\
0       & 0       & \cdots & 0\\
\end{array}
\right)
\]
with $C_{ij} \in K[x]$. Let $\overline{w}_i=(\sum_{j=1}^n
C_{ij}w_j)/F$
for $1 \le i \le n$. Then, 
$\overline{w}=(\overline{w}_1,\ldots,\overline{w}_n)$ is a basis for
$E$ over $K$ and
\[
K[x]\overline{w}_1+\cdots+K[x]\overline{w}_n=K[x]w_1+\cdots+K[x]w_n+K[x]w_0
\]
is a submodule of ${\bf O}_{K[x]}$, which strictly contains
$K[x]w_1+\cdots+K[x]w_n$, since it contains $w_0$. Any strictly
increasing chain of submodules of ${\bf O}_{K[x]}$ must
stabilize after a finite number of steps, which means that this
process produces a basis for which either the Hermite reduction can be
carried out, or for which $f$ has a squarefree denominator.

\noindent
{\bf Example 2} Continuing example 1 for which the Hermite reduction
failed, Theorem 2 implies that
\[
w_0=\frac{1}{x}(-2xw_3+(x+1)w_4)=(-2xy^2+(x+1)y^3)x \in {\bf O}_{K[x]}
\]
Performing a Hermitian row reduction on
\[
\left(
\begin{array}{cccc}
x& &   & \\
 &x&   & \\
 & &x  & \\
 & &   &x\\
0&0&-2x&x+1\\
\end{array}
\right)
\]
yields
\[
\left(
\begin{array}{cccc}
x& & & \\
 &x& & \\
 & &x& \\
 & & &1\\
0&0&0&0\\
\end{array}
\right)
\]
so the new basis is $\overline{w}=(1,y,y^2,y^3/x)$, and the
denominator of $f$ with respect to $\overline{w}$ is $x$, which is
squarefree. 

\subsection{Simple radical extensions}
The integration algorithm becomes easier when $E$ is a simple radical
extension of $K(x)$, {\sl i.e.} $E=K(x)[y]/(y^n-a)$ for some 
$a \in K(x)$. Write $a=A/D$ where $A,D \in K[x]$, and let
$AD^{n-1}=A_1A_2^2\cdots A_k^k$ be a squarefree factorization of 
$AD^{n-1}$. Writing $i=nq_i+r_i$, for $1 \le i \le k$, where
$0 \le r_i < n$, let $F=A_1^{q_1}\cdots A_k^{q_k}$,
$H=A_1^{r_1}\cdots A_k^{r_k}$ and $z=yD/F$. Then,
\[
z^n=\left(y\frac{D}{F}\right)^n=\frac{y^nD^n}{F^n}=\frac{AD^{n-1}}{F}
=A_1^{r_1}\cdots A_k^{r_k}=H
\]
Since $r_i < n$ for each $i$, the squarefree factorization of $H$ is
of the form $H=H_1H_2^2\cdots H_m^m$ with $m<n$. An integral basis is
then $w=(w_1,\ldots,w_n)$ where
\begin{equation}\label{Int11}
w_i=\frac{z^{i-1}}{\prod_{j=1}^m H_j^{\lfloor(i-1)j/n\rfloor}}\quad
1 \le i \le n
\end{equation}
and the Hermite reduction with respect to the above basis is always 
guaranteed to succeed. Furthermore, when using that basis, the system
\ref{Int8} becomes diagonal and its solution can be written explicitly:
writing $D_i=\prod_{j=1}^m H_j^{\lfloor ij/n\rfloor}$ we have
\[
\begin{array}{ccl}
S_i & = &\displaystyle UV^m\left(\frac{w_i}{V^{m-1}}\right)^{\prime}
=UV^m\left(\frac{z^{i-1}}{D_{i-1}V^{m-1}}\right)^{\prime}\\
&&\\
&=&\displaystyle UV^m\left(\frac{i-1}{n}\frac{H^{\prime}}{H}-
\frac{{D_{i-1}}^{\prime}}{D_{i-1}}
-(m-1)\frac{V^{\prime}}{V}\right)\left(\frac{z^{i-1}}{D_{i-1}V^{m-1}}\right)\\
&&\\
&=&\displaystyle U\left(V\left(\frac{i-1}{n}\frac{H^{\prime}}{H}
-\frac{{D_{i-1}}^{\prime}}{D_{i-1}}\right)-(m-1)V^{\prime}\right)w_i
\end{array}
\]
so the unique solution of \ref{Int8} in $K(x)$ is
\begin{equation}\label{Int12}
f_i=\frac{A_i}{U\left(V\left(\frac{i-1}{n}\frac{H^{\prime}}{H}-
\frac{{D_{i-1}}^{\prime}}{D_{i-1}}\right)-(m-1)V^{\prime}\right)}
\quad{\rm for\ }1 \le i \le n
\end{equation}
and it can be shown that the denominator of each $f_i$ is coprime with 
$V$ when $m \ge 2$.

\noindent
{\bf Example 3} {\sl Consider
\[
\int\frac{(2x^8+1)\sqrt{(x^8+1)}}{x^{17}+2x^9+x}~dx
\]
The integrand is
\[
f=\frac{(2x^8+1)y}{x^{17}+2x^9+x} \in E
=\mathbb{Q}(x)[y]/(y^2-x^8-1)
\]
so $H=x^8+1$ which is squarefree, implying that the integral basis
\ref{Int11} is $(w_1,w_2)=(1,y)$. The squarefree factorization of
$x^{17}+2x^9+x$ is $x(x^8+1)^2$ so $U=x$, $V=x^8+1$, $m=2$, and the
solution \ref{Int12} of \ref{Int8} is
\[
f_1=0,\quad
f_2=\frac{2x^8+1}{x\left((x^8+1)\frac{1}{2}\frac{8x^7}{x^8+1}
-8x^7\right)}=-\frac{(2x^8+1)/4}{x^8}
\]
We have $Q=x^8$, so $V-Q=1$, $A=1$, $R=-1$ and $RQf_2=V/2-1/4$,
implying that
\[
B=-\frac{y}{4}\quad {\rm and}\quad h=f-\left(\frac{B}{V}\right)^{\prime}
=\frac{y}{x(x^8+1)}
\]
solve \ref{Int7}, i.e.
\[
\int\frac{(2x^8+1)\sqrt{(x^8+1)}}{x^{17}+2x^9+x}~dx=
-\frac{\sqrt{x^8+1}}{4(x^8+1)}
+\int\frac{\sqrt{x^8+1}}{x(x^8+1)}~dx
\]
and the remaining integrand has a squarefree denominator.}

\subsection{Liouville's Theorem}
Up to this point, the algorithms we have presented never fail, yet it
can happen that an algebraic function does not have an elementary
integral, for example
\[
\int{\frac{x~dx}{\sqrt{1-x^3}}}
\]
which is not an elementary function of $x$. So we need a way to
recognize such functions before completing the integration
algorithm. Liouville was the first to state and prove a precise
theorem from Laplace's observation that we can restrict the elementary
integration problem by allowing only new logarithms to appear linearly
in the integral, all the other terms appearing in the integral being
already in the integrand.

{\bf Theorem 4 (Liouville \cite{Liou1833a,Liou1833b})} {\sl
Let $E$ be an algebraic extension of the rational function field
$K(x)$, and $f \in E$. If $f$ has an elementary integral, then there
exist $v \in E$, constants $c_1,\ldots,c_n \in \overline{K}$ and
$u_1,\ldots,u_k \in E(c_1,\ldots,c_k)^{*}$ such that}
\begin{equation}\label{Int13}
f=v^{\prime}+c_1\frac{u_1^{\prime}}{u_1}+\cdots+c_k\frac{u_k^{\prime}}{u_k}
\end{equation}
The above is a restriction to algebraic functions of the strong
Liouville Theorem, whose proof can be found in \cite{Bron97,Risc69b}. 
An elegant
and elementary algebraic proof of a slightly weaker version can be
found in \cite{Rose72}. As a consequence, we can look for an integral of
the form \ref{Int4}, Liouville's Theorem guaranteeing that there is no
elementary integral if we cannot find one in that form. Note that the
above theorem does not say that every integral must have the above
form, and in fact that form is not always the most convenient one, for
example, 
\[
\int{\frac{dx}{1+x^2}}=arctan(x)=\frac{\sqrt{-1}}{2}
\log\left(\frac{\sqrt{-1}+x}{\sqrt{-1}-x}\right)
\]

\subsection{The integral part}
Following the Hermite reduction, we can assume that we have a basis 
$w=(w_1,\ldots,w_n)$ of $E$ over $K(x)$ made of integral elements such
that our integrand is of the form $f=\sum_{i=1}^n A_iw_i/D$ where 
$D \in K[x]$ is squarefree. Given Liouville's Theorem, we now have to
solve equation \ref{Int13} for $v$, $u_1,\ldots,u_k$ and the constants 
$c_1,\ldots,c_k$. Since $D$ is squarefree, it can be shown that 
$v \in {\bf O}_{K[x]}$ for any solution, and in fact $v$
corresponds to the polynomial part of the integral of rational
functions. It is however more difficult to compute than the integral
of polynomials, so Trager \cite{Trag84} gave a change of variable that
guarantees that either $v^{\prime}=0$ or $f$ has no elementary integral. In
order to describe it, we need to define the analogue for algebraic
functions of having a nontrivial polynomial part: we say that 
$\alpha \in E$ is {\sl integral at infinity} if there is a polynomial
$p=\sum_{i=1}^m a_iy^i \in K[x][y]$ such that $p(x,\alpha)=0$ and
${\rm deg}(a_m) \ge {\rm deg}(a_i)$ for each $i$. Note that a rational function
$A/D \in K(x)$ is integral at infinity if and only if 
${\rm deg}(A) \le {\rm deg}(D)$ 
since it is a zero of $Dy-A$. When $\alpha-E$ is
not integral at infinity, we say that it has a {\sl pole at
infinity}. Let
\[
{\bf O}_\infty = \{\alpha \in E {\rm\ such\ that\ }\alpha
{\rm\ is\ integral\ at\ infinity}\}
\]
A set $(b_1,\ldots,b_n) \in E^n$ is called {\sl normal at infinity} if
there are $r_1,\ldots,r_n \in K(x)$ such that every 
$\alpha \in {\bf O}_\infty$ can be written as
$\alpha = \sum_{i=1}^n B_ir_ib_i/C$ where $C,B_1,\ldots,B_n \in K[x]$
and ${\rm deg}(C) \ge {\rm deg}(B_i)$ for each $i$. 
We say that the differential
$\alpha{}dx$ is integral at infinity if 
$\alpha x^{1+1/r} \in {\bf O}_\infty$ where $r$ is the smallest
ramification index at infinity. Trager \cite{Trag84} described an
algorithm that converts an arbitrary integral basis $w_1,\ldots,w_n$
into one that is also normal at infinity, so the first part of his
integration algorithm is as follows:
\begin{enumerate}
\item Pick any basis $b=(b_1,\ldots,b_n)$ of $E$ over $K(x)$ that is
composed of integral elements.
\item Pick an integer $N \in \mathbb{Z}$ that is not zero of the
denominator of $f$ with respect to $b$, nor of the discriminant of $E$
over $K(x)$, and perform the change of variable $x=N+1/z$,
$dx=-dz/z^2$ on the integrand.
\item Compute an integral basis $w$ for $E$ over $K(z)$ and make it
normal at infinity
\item Perform the Hermite reduction on $f$ using $w$, this yields 
$g,h \in E$ such that $\int{f~dz}=g+\int{h~dz}$ and $h$ has a
squarefree denominator with respect to $w$.
\item If $hz^2$ has a pole at infinity, then $\int{f~dz}$ and
$\int{h~dz}$ are not elementary functions
\item Otherwise, $\int{h~dz}$ is elementary if and only if there are
constants $c_1,\ldots,c_k \in \overline{K}$ and 
$u_1,\ldots,u_k \in E(c_1,\ldots,c_k)^{*}$ such that
\end{enumerate}
\begin{equation}\label{Int14}
h=\frac{c_1}{u_1}\frac{du_1}{dz}+\cdots+\frac{c_k}{u_k}\frac{du_k}{dz}
\end{equation}
The condition that $N$ is not a zero of the denominator of $f$ with
respect to $b$ implies that the $fdz$ is integral at infinity after
the change of variable, and Trager proved that if $hdz$ is not
integral at infinity after the Hermite reduction, then $\int{h~dz}$
and $\int{f~dz}$ are not elementary functions. The condition that $N$
is not a zero of the discriminant of $E$ over $K(x)$ implies that the
ramification indices at infinity are all equal to 1 after the change
of variable, hence that $h~dz$ is integral at infinity if and only if
$hz^2 \in {\bf O}_\infty$. That second condition on $N$ can be
disregarded, in which case we must replace $hz^2$ in step 5 by 
$hz^{1+1/r}$ where $r$ is the smallest ramification index at
infinity. Note that $hz^2 \in {\bf O}_\infty$ implies that 
$hz^{1+1/r} \in {\bf O}_\infty$, but not conversely. Finally, we
remark that for simple radical extensions, the integral basis \ref{Int11} is
already normal at infinity.

Alternatively, we can use lazy Hermite reduction in the above
algorithm: in step 3, we pick any basis made of integral elements,
then perform the lazy Hermite reduction in step 4. If $h \in K(z)$
after the Hermite reduction, then we can complete the integral without
computing an integral basis. Otherwise, we compute an integral basis
and make it normal at infinity between steps 4 and 5. This lazy
variant can compute $\int{f~dx}$ whenever it is an element of $E$
without computing an integral basis.

\subsection{The logarithmic part}
Following the previous sections, we are left with solving equation
\ref{Int14} for the constants $c_1,\ldots,c_k$ and for $u_1,\ldots,u_k$. We
must make at this point the following additional assumptions:
\begin{itemize}
\item we have an integral primitive element for $E$ over $K(z)$, {\sl
i.e.} $y \in {\bf O}_{K[z]}$ such that $E=K(z)(y)$,
\item $[E : K(z)]=[E : \overline{K}(z)]$, {\sl i.e.} the minimal
polynomial for $y$ over $K[z]$ is absolutely reducible, and
\item we have an integral basis $w=(w_1,\ldots,w_n)$ for $E$ over
$K(z)$, and $w$ is normal at infinity
\end{itemize}
A primitive element can be computed by considering linear combinations
of the generators of $E$ over $K(x)$ with random coefficients in
$K(x)$, and Trager \cite{Trag84} describes an absolute factorization
algorithm, so the above assumptions can be ensured, although those
steps can be computationally very expensive, except in the case of
simple radical extensions. Before describing the second part of
Trager's integration algorithm, we need to define some concepts from
the theory of algebraic curves. Given a finite algebraic extension 
$E=K(z)(y)$ of $K(z)$, a {\sl place} $P$ of $E$ is a proper local
subring of $E$ containing $K$, and a {\sl divisor} is a formal sum
$\sum{n_PP}$ with finite support, where the $n_P$'s are integers and
the $P$'s are places. Let $P$ be a place, then its maximal ideal
$\mu_P$ is principal, so let $p\in E$ be a generator of $\mu_P$. The
{\sl order at} $P$ is the function 
$\nu_P : E^{*} \rightarrow \mathbb{Z}$ which maps $f \in E^{*}$ to the
largest $k \in \mathbb{Z}$ such that $f \in p^kP$. Given 
$f \in E^{*}$, the {\sl divisor of} $f$ is $(f) = \sum{\nu_P(f)P}$
where the sum is taken over all the places. It has finite support
since $\nu_P(f) \ne 0$ if and only if $P$ is a pole or zero of
$f$. Finally, we say that a divisor $\delta = \sum{n_PP}$ is
{\sl principal} if $\delta=(f)$ for some $f \in E^{*}$. Note that if 
$\delta$ is principal, the $\sum{n_P}=0$, but the converse is not
generally true, except if $E=K(z)$. Trager's algorithm proceeds
essentially by constructing candidate divisors for the $u_i$'s of
\ref{Int14}: 
\begin{itemize}
\item Let $\sum_{i=1}^n A_iw_i$ be the numerator of $h$ with respect
to $w$, and $D$ be its (squarefree) denominator
\item Write $\sum_{i=1}^n A_iw_i=G/H$, where $G \in K[z,y]$ and 
$H \in K[z]$
\item Let $f \in K[z,y]$ be the (monic) minimum polynomial for $y$
over $K(z)$, $t$ be a new indeterminante and compute
\[
R(t)={\rm resultant_z\ }\left({\rm\ pp_t }\left({\rm\ resultant_y\ }\left(
G-tH\frac{dD}{dz},F\right)\right),D\right) \in K[t]
\]
\item Let $\alpha_1,\ldots,\alpha_s \in \overline{K}$ be the distinct
nonzero roots of $R$, $(q_1,\ldots,q_k)$ be a basis for the vector
space that they generate over $\mathbb{Q}$, write
$\alpha_i=r_{i1}q_1+\cdots+r_{ik}q_k$ for each $i$, where 
$r_{ij} \in \mathbb{Q}$ and let $m > 0$ be a common denominator for
all the $r_{ij}$'s
\item For $1 \le j \le k$, let 
$\delta_j=\sum_{i=1}^s mr_{ij}\sum_l r_lP_l$ where $r_l$ is the
ramification index of $P_l$ and $P_l$ runs over all the places at
which $h~dz$ has residue $r_i\alpha_i$
\item If there are nonzero integers $n_1,\ldots,n_k$ such that 
$n_j\delta_j$ is principal for each $j$, then let
\[
u=h-\frac{1}{m}\sum_{j=1}^k\frac{q_j}{n_ju_j}\frac{du_j}{dz}
\]
where $u_j \in E(\alpha_1,\ldots,\alpha_s)^{*}$ is such that 
$n_j\delta_j=(u_j)$. If $u=0$, then 
$\int{h~dz}=\sum_{j=1}^k q_j\log(u_j)/(mn_j)$, otherwise if either 
$u \ne 0$ or there is no such integer $n_j$ for at least one $j$,
then $h~dz$ has no elementary integral.
\end{itemize}
Note that this algorithm expresses the integral, when it is
elementary, with the smallest possible number of logarithms. Steps 3
to 6 requires computing in the splitting field $K_0$ of $R$ over $K$,
but it can be proven that, as in the case of rational functions, $K_0$
is the minimal algebraic extension of $K$ necessary to express the
integral in the form \ref{Int4}. Trager \cite{Trag84} 
describes a representation
of divisors as fractional ideals and gives algorithms for the
arithmetic of divisors and for testing whether a given divisor is
principal. In order to determine whether there exists an integer $N$
such that $N\delta$ is principal, we need to reduce the algebraic
extension to one over a finite field $\mathbb{F}_{p^q}$ for some
``good'' prime $p \in \mathbb{Z}$. Over $\mathbb{F}_{p^q}$, it is
known that for every divisor $\delta=\sum{n_PP}$ such that
$\sum{n_P}=0$, $M\delta$ is principal for some integer
$1 \le M \le (1+\sqrt{p^q})^{2g}$, where $g$ is the genus of the curve
\cite{Weil71}, so we compute such an $M$ by testing $M=1,2,3,\ldots$ until
we find it. It can then be shown that for almost all primes $p$, if
$M\delta$ is not principal in characteristic 0, the $N\delta$ is not
principal for any integer $N \ne 0$. Since we can test whether the
prime $p$ is ``good'' by testing whether the image in
$\mathbb{F}_{p^q}$ of the discriminant of the discriminant of the
minimal polynomial for $y$ over $K[z]$ is 0, this yields a complete
algorithm. In the special case of hyperelliptic extensions, {\sl i.e.}
simple radical extensions of degree 2, Bertrand \cite{Bert95} describes a
simpler representation of divisors for which the arithmetic and
principality tests are more efficient than the general methods.

\noindent
{\bf Example 4} {\sl
Continuing example 3, we were left with the integrand
\[
\frac{\sqrt{x^8+1}}{x(x^8+1)}=\frac{w_2}{x(x^8+1)} \in E
=\mathbb{Q}(x)[y]/(y^2-x^8-1)
\]
where $(w_1,w_2)=(1,y)$ is an integral basis normal at infinity, and
the denominator $D=x(x^8+1)$ of the integrand is squarefree. Its
numerator is $w_2=y$, so the resultant of step 3 is
\[
resultant_x(pp_t(resultant_y(y-t(9x^8+1),y^2-x^8-1)),x(x^8+1))=
ct^{16}(t^2-1)
\]
where $c$ is a large nonzero integer. Its nonzero roots are $\pm 1$,
and the integrand has residue 1 at the place $P$ corresponding to the
point $(x,y)=(0,1)$ and $-1$ at the place $Q$ corresponding to the
point $(x,y)=(0,-1)$, so the divisor $\delta_1$ of step 5 is 
$\delta_1=P-Q$. It turns out that $\delta_1$, $2\delta_1$, and 
$3\delta_1$ are not principal, but that
\[
4\delta_1=\left(\frac{x^4}{1+y}\right)\quad{\rm\ and\ }\quad
\frac{w_2}{x(x^8+1)}
-\frac{1}{4}\frac{(x^4/(1+y))^{\prime}}{x^4/(1+y)}=0
\]
which implies that
\[
\int{\frac{\sqrt{x^8+1}}{x(x^8+1)}}~dx
=\frac{1}{4}\log\left(\frac{x^4}{1+\sqrt{x^8+1}}\right)
\]}

\noindent
{\bf Example 5} {\sl
Consider
\[
\int{\frac{x~dx}{\sqrt{1-x^3}}}
\]
The integrand is 
\[
f=\frac{xy}{1-x^3} \in E =
\mathbb{Q}(x)[y]/(y^2+x^3-1)
\]
where $(w_1,w_2)=(1,y)$ is an integral basis normal at infinity, and
the denominaotr $D=1-x^3$ of the integrand is squarefree. Its
numerator is $xw_2=xy$, so the resultant of step 3 is
\[
resultant_x(pp_t(resultant_y(xy+3tx^2,y^2+x^3-1)),1-x^3)=729t^6
\]
whose only root is 0. Since $f \ne 0$, we conclude from step 6 that
$\int{f~dx}$ is not an elementary function.}

\noindent
{\bf Example 6} {\sl
\[
\int{\frac{dx}{x\sqrt{1-x^3}}}
\]
The integrand is
\[
f=\frac{y}{x-x^4} \in E =
\mathbb{Q}(x)[y]/(y^2+x^3-1)
\]
where $(w_1,w_2)=(1,y)$ is an integral basis normal at infinity, and
the denominator $D=x-x^4$ of the integrand is squarefree. Its
numerator is $w_2=y$, so the resultant of step 3 is
\[
resultant_x(pp_t(resultant_y(y+t(4x^3-1),y^2+x^3-1)),x-x^4)=729t^6(t^2-1)
\]
Its nonzero roots are $\pm 1$, and the integrand has residue 1 at the
place $P$ corrseponding to the point $(x,y)=(0,1)$ and $-1$ at the
place $Q$ corresponding to the point $(x,y)=(0,-1)$ so the divisor
$\delta_1$ of step 5 is $\delta_1=P-Q$. It turns out that $\delta_1$
and $2\delta_1$ are not principal, but that
\[
3\delta_1=\left(\frac{y-1}{y+1}\right)\quad{\rm and}\quad
\frac{y}{x-x^4}-\frac{1}{3}\frac{((y-1)/(y+1))^{\prime}}{(y-1)/(y+1)}=0
\]
which implies that
\[
\int{\frac{dx}{x\sqrt{1-x^3}}}
=\frac{1}{3}\log\left(\frac{\sqrt{1-x^3}-1}{\sqrt{1-x^3}+1}\right)
\]}

\section{Elementary Functions}
Let $f$ be an arbitrary elementary function. In order to generalize
the algorithms of the previous sections, we need to build an algebraic
model in which $f$ behaves in some sense like a rational or algebraic
function. For that purpose, we need to formally define differential
fields and elementary functions.

\subsection{Differential algebra}
A {\sl differential field} $(K,')$ is a field $K$ with a given map
$a \rightarrow a^{\prime}$ from $K$ into $K$, satisfying
$(a+b)^{\prime}=a^{\prime}+b^{\prime}$ and 
$(ab)^{\prime}=a^{\prime}b+ab^{\prime}$. Such a map is called a
{\sl derivation} on $K$. An element $a \in K$ which satisfies 
$a^{\prime}=0$ is called a {\sl constant}, and the set 
Const($K$)=$\{a \in K {\rm\ such\ that\ }a^{\prime}=0\}$ of all the
constants of $K$ is called a subfield of $K$.

A differential field $(E,')$ is a {\sl differential equation} of
$(K,')$ if $K \subseteq E$ and the derivation on $E$ extends the one
on $K$. In that case, an element $t \in E$ is a {\sl monomial} over
$K$ if $t$ is transcendental over $K$ and $t' \in K[t]$, which implies
that both $K[t]$ and $K(t)$ are closed under $^{\prime}$. An element $t \in E$
is {\sl elementary over} $K$ if either
\begin{itemize}
\item $t'=b'/b$ for some $b \in K^{*}$, in which case we say that $t$
is a {\sl logarithm} over $K$, and write $t=log(b)$, or
\item $t'=b't$ for some $b \in K^{*}$, in which case we say that $t$
is an {\sl exponential} over $K$, and write $t=e^b$, or
\item $t$ is algebraic over $K$
\end{itemize}

A differential extension $(E,')$ of $(K,')$ is {\sl elementary over}
$K$, if there exist $t_1,\ldots,t_m$ in $E$ such that
$E=K(t_1,\ldots,t_m)$ and each $t_i$ is elementary over
$K(t_1,\ldots,t_{i-1})$. We say that $f \in K$ {\sl has an elementary
integral} over $K$ if there exists an elementary extension $(F,')$ of
$(K,')$ and $g \in F$ such that $g'=f$. An {\sl elementary function}
of the variable $x$ is an element of an elementary extension of the
rational function field $(C(x),d/dx)$, where $C={\rm Const}(C(x))$.

Elementary extensions are useful for modeling any function as a
rational or algebraic function of one main variable over the other
terms present in the function: given an elementary integrand
$f(x)~dx$, the integration algorithm first constructs a field $C$
containing all the constants appearing in $f$, then the rational
function field $(C(x),d/dx)$, then an elementary tower 
$E=C(x)(t_1,\ldots,t_k)$ containing $f$. Note that such a tower is not
unique, and in addition, ajoining a logarithm could in fact adjoin a
new constant, and an exponential could in fact be algebraic, for
example $\mathbb{Q}(x)(log(x),log(2x))=\mathbb{Q}(log(2))(x)(log(x))$
and $\mathbb{Q}(x)(e^{log(x)/2})=\mathbb{Q}(x)(\sqrt{x})$. There are
however algorithms that detect all such occurences and modify the
tower accordingly \cite{Risc79}, so we can assume that all the logarithms
and exponentials appearing in $E$ are monomials, and that 
${\rm Const}(E)=C$. Let now $k_0$ be the largest index such that
$t_{k_0}$ is transcendental over $K=C(x)(t_1,\ldots,t_{k_0-1})$ and
$t=t_{k_0}$. Then $E$ is a finitely generated algebraic extension of
$K(t)$, and in the special case $k_0=k$, $E=K(t)$. Thus, $f \in E$ can
be seen as a univariate rational or algebraic function over $K$, the
major difference with the pure rational or algebraic cases being that
$K$ is not constant with respect to the derivation. It turns out that
the algorithms of the previous section can be generalized to such
towers, new methods being required only for the polynomial (or
integral) part. We note that Liouville's Theorem remains valid when
$E$ is an arbitrary differential field, so the integration algorithms
work by attempting to solve equation \ref{Int13} as previously.

\noindent
{\bf Example 7} {\sl
The function (1) is the element $f=(t-t^{-1})\sqrt{-1}/2$ of $E=K(t)$
where $K=\mathbb{Q}(\sqrt{-1})(x)(t_1,t_2)$ with
\[
t_1=\sqrt{x^3-x+1},\quad t_2=e^{2\sqrt{-1}(x^3-t_1)},\quad{\rm and}\quad
t=e^{((1-t_2)/(1+t_2))-x\sqrt{-1}}
\]
which is transcendental over $K$. Alternatively, it can also be
written as the element $f=2\theta/(1+\theta^2)$ of $F=K(\theta)$ where 
$K==\mathbb{Q}(x)(\theta_1,\theta_2)$ with
\[
\theta_1=\sqrt{x^3-x+1},\quad\theta_2=\tan(x^3-\theta_1),\quad{\rm
and}\quad\theta=\tan\left(\frac{x+\theta_2}{2}\right)
\]
which is a transcendental monomial over $K$. It turns out that both
towers can be used in order to integrate $f$.}

The algorithms of the previous sections relied extensively on
squarefree factorization and on the concept of squarefree
polynomials. The appropriate analogue in monomial extensions is the
notion of {\sl normal} polynomials: let $t$ be a monomial over $K$, we
say that $p\in K[t]$ is {\sl normal} (with respect to ') if
$\gcd(p,p')=1$, and that $p$ is {\sl special} if $\gcd(p,p')=p$, 
{\sl i.e.} $p | p'$ in $K[t]$. For $p \in K[t]$ squarefree, let 
$p_s=\gcd(p,p')$ and $p_n=p/p_s$. Then $p=p_sp_n$, while $p_s$ is
special and $p_n$ is normal. Therefore, squarefree factorization can
be used to write any $q \in K[t]$ as a product $q=q_sq_n$, where
$\gcd(q_s,q_n)=1$, $q_s$ is special and all the squarefree factors of
$q_n$ are normal. We call $q_s$ the {\sl special part} of $q$ and
$q_n$ its {\sl normal part}.

\subsection{The Hermite reduction}
The Hermite reductions we presented for rational and algebraic
functions work in exactly the same way algebraic extensions of
monomial extensions of $K$, as long as we apply them only to the
normal part of the denominator of the integrand. Thus, if $D$ is the
denominator of the integrand, we let $S$ be the special part of $D$,
$D_1D_2^2\ldots D_m^m$ be a squarefree factorization of the {\sl
normal} part of $D$, $V=D_m$, $U=D/V^m$ and the rational and algebraic
Hermite reductions proceed normally, eventually yielding an integrand
whose denominator has a squarefree normal part.

\noindent
{\bf Example 8} {\sl
Consider
\[
\int{\frac{x-\tan(x)}{\tan(x)^2}}~dx
\]
The integrand is
\[
f=\frac{x-t}{t^2} \in K(t)\quad {\rm where\ }
K=\mathbb{Q}(x) {\rm\ and\ }t'=t^2+1
\]
Its denominator is $D=t^2$, and $\gcd(t,t')=1$ implying that $t$ is
normal, so $m=2$, $V=t$, $U=D/t^2=1$, and the extended Euclidean
algorithm yields
\[
\frac{A}{1-m}=t-x=-x(t^2+1)+(xt+1)t=-xUV'+(xt+1)V
\]
implying that
\[
\int{\frac{x-\tan(x)}{\tan(x)^2}}~dx=-\frac{x}{\tan(x)}-\int{x}~dx
\]
and the remaining integrand has a squarefree denominator.}

\noindent
{\bf Example 9} {\sl
Consider
\[
\int{\frac{\log(x)^2+2x\log(x)+x^2+(x+1)\sqrt{x+\log(x)}}
{x\log(x)^2+2x^2\log(x)+x^3}}~dx
\]
The integrand is
\[
f=\frac{t^2+2xt+x^2+(x+1)y}{xt^2+2x^2t+x^3} \in E
=K(t)[y]/(y^2-x-t)
\]
where $K=\mathbb{Q}(x)$ and $t=log(x)$. The denominator of $f$ with
respect to the basis $w=(1,y)$ is $D=xt^2+2x^2t+x^3$ whose squarefree
factorization is $x(t+x)^2$. Both $x$ and $t+x$ are normal, so $m=2$,
$V=t+x$, $U=D/V^2=x$, and the solution \ref{Int12} of \ref{Int8} is
\[
f_1=\frac{t^2+2xt+x^2}{x(-(t'+1))}
=-\frac{t^2+2xt+x^2}{x+1},
\]
\[
f_2=\frac{x+1}{x\left((t+x)\frac{1}{2}\frac{t^{\prime}+1}{t+z}-
(t'+1)\right)}=-2
\]
We have $Q=1$, so $0V+1Q=1$, $A=0$, $R=1$, $RQf_1=f_1=-V^2/(x+1)$ and
$RQf_2=f_2=0V-2$, so $B=-2y$ and
\[
h=f-\left(\frac{B}{V}\right)^{\prime}=\frac{1}{x}
\]
implying that
\[
\int{\frac{\log(x)^2+2x\log(x)+x^2+(x+1)\sqrt{x+\log(x)}}
{x\log(x)^2+2x^2\log(x)+x^2}}~dx
=\frac{2}{\sqrt{x+log(x)}}+\int{\frac{dx}{x}}
\]
and the remaining integrand has a squarefree denominator.}

\subsection{The polynomial reduction}
In the transcendental case $E=K(t)$ and when $t$ is a monomial
satisfying ${\rm deg}_t(t') \ge 2$, then it is possible to reduce the
degree of the polynomial part of the integrand until it is smaller
than ${\rm deg}_t(t')$. In the case when $t=\tan(b)$ for some $b \in K$, then
it is possible either to prove that the integral is not elementary, or
to reduce the polynomial part of the integrand to be in $K$. Let 
$f \in K(t)$ be our integrand and write $f=P+A/D$, where
$P,A,D \in K[t]$ and ${\rm deg}(A) < {\rm deg}(D)$. Write 
$P=\sum_{i=1}^e p_it^i$ and $t'=\sum_{i=0}^d c_it^i$ where
$p_0,\ldots,p_e,c_0,\ldots,c_d \in K$, $d \ge 2$, $p_e\ne 0$ and 
$c_d\ne 0$. It is easy to verify that if $e \ge d$, then
\begin{equation}\label{Int15}
P=\left(\frac{a_e}{(e-d+1)c_d}t^{e-d+1}\right)^{\prime}+\overline{P}
\end{equation}
where $\overline{P} \in K[t]$ is such that $\overline{P}=0$ or 
${\rm deg}_t(\overline{P}) < e$. Repeating the above transformation we
obtain $Q,R \in K[t]$ such that $R=0$ or ${\rm deg}_t(R) < d$ and $P=Q'+R$. 
Write then $R=\sum_{i=0}^{d-1} r_it^i$ where
$r_0,\ldots,r_{d-1} \in K$. Again, it is easy to verify that for any
{\sl special} $S \in K[t]$ with ${\rm deg}_t(S) > 0$, we have
\[
R=\frac{1}{{\rm deg}_t(S)}\frac{r_{d-1}}{c_d}\frac{S'}{S}+\overline{R}
\]
where $\overline{R} \in K[t]$ is such that $\overline{R}=0$ or
${\rm deg}_t(\overline{R}) < e-1$. Furthermore, it can be proven \cite{Bron97}
that if $R+A/D$ has an elementary integral over $K(t)$, then 
$r_{d-1}/{c_d}$ is a constant, which implies that
\[
\int{R}=\frac{1}{{\rm deg}_t(S)}\frac{r_{d-1}}{c_d}\log(S)
+\int\left(\overline{R}+\frac{A}{D}\right)
\]
so we are left with an integrand whose polynomial part has degree at
most ${\rm deg}_t(t')-2$. In this case $t=\tan(b)$ for $b \in K$, then
$t'=b't^2+b'$, so $\overline{R} \in K$.

{\bf Example 10} {\sl
Consider
\[
\int(1+x\tan(x)+\tan(x)^2)~dx
\]
The integrand is
\[
f=1+xt+t^2 \in K(t)\quad{\rm where\ }K=\mathbb{Q}(x)
{\rm\ and\ }t'=t^2+1
\]
Using \ref{Int15}, we get $\overline{P}=f-t'=f-(t^2+1)=xt$ so
\[
\int(1+x\tan(x)+\tan(x)^2)~dx=\tan(x)+\int{x\tan(x)}~dx
\]
and since $x'\ne 0$, the above criterion imples that the remaining
integral is not an elementary function.}

\subsection{The residue criterion}
Similarly to the Hermite reduction, the Rothstein-Trager and
Lazard-Rioboo-Trager algorithms are easy to generalize to the
transcendental case $E=K(t)$ for arbitrary monomials $t$: let 
$f\in K(t)$ be our integrand and write $f=P+A/D+B/S$ where
$P,A,D,B,S \in K[t]$, ${\rm deg}(A) < {\rm deg}(D)$, $S$ is special and, following
the Hermite reduction, $D$ is normal. Let then $z$ be a new
indeterminate, $\kappa : K[z] \rightarrow K[z]$ be give by 
$\kappa(\sum_i a_iz^i)=\sum_i a_i^{\prime}z^i$,
\[
R={\rm resultant_t}(D,A-zD') \in K[z]
\]
be the Rothstein-Trager resultant, $R=R_1R_2^2\ldots R_k^k$ be its
squarefree factorization, $Q_i=\gcd_z(R_i,\kappa(R_i))$ for each $i$,
and 
\[
g=\sum_{i=1}^k\sum_{a|Q_i(a)=0} a\log(\gcd{}_t(D,A-aD'))
\]

Note that the roots of each $Q_i$ must all be constants, and that the
arguments of the logarithms can be obtained directly from the
subresultant PRS of $D$ and $A-zD'$ as in the rational function
case. It can then be proven \cite{Bron97} that
\begin{itemize}
\item $f-g'$ is always ``simpler'' than $f$
\item the splitting field of $Q_1\cdots Q_k$ over $K$ is the minimal
algebraic extension of $K$ needed in order to express $\int f$ in the
form \ref{Int4}
\item if $f$ has an elementary integral over $K(t)$, then 
$R | \kappa(R)$ in $K[z]$ and the denominator of $f-q'$ is special
\end{itemize}
Thus, while in the pure rational function case the remaining integrand
is a polynomial, in this case the remaining integrand has a special
denominator. In that case we have additionally that if its integral is
elementary, then \ref{Int13} has a solution such that $v\in K(t)$ has a
special denominator, and each $u_i \in K(c_1,\ldots,c_k)[t]$ is
special.

\noindent
{\bf Example 11} {\sl
Consider
\[
\int{\frac{2\log(x)^2-\log(x)-x^2}{\log(x)^3-x^2\log(x)}}~dx
\]
The integrand is
\[
f=\frac{2t^2-t-x^2}{t^2-xt^2} \in K(t)\quad
{\rm where\ }K=\mathbb{Q}(x){\rm\ and\ }t=\log(x)
\]
Its denominator is $D=t^3-x^2t$, which is normal, and the resultant is 
\[
\begin{array}{ccl}
R&=&\displaystyle
resultant_t\left(t^3-x^2t,\frac{2x-3z}{x}t^2+(2xz-1)t+x(z-x)\right)\\
&&\\
&=&\displaystyle
4x^3(1-x^2)\left(z^3-xz^2-\frac{1}{4}z+\frac{x}{4}\right)
\end{array}
\]
which is squarefree in $K[z]$. We have
\[
\kappa(R)=-x^2(4(5x^2+3)z^3+8x(3x^2-2)z^2+(5x^2-3)z-2x(3x^2-2))
\]
so
\[
Q_1=\gcd{}_z(R,\kappa R)=x^2\left(z^2-\frac{1}{4}\right)
\]
and
\[
\gcd{}_t\left(t^3+x^2t,\frac{2x-3a}{x}t^2+(2xa-1)t+x(a-x)\right)=t+2ax
\]
where $a^2-1/4=0$, whence
\[
g=\sum_{a|a^2-1/4=0} a\log(t+2ax)=\frac{1}{2}\log(t+x)-\frac{1}{2}\log(t-x)
\]
Computing $f-g'$ we find
\[
\int{\frac{2\log(x)^2-\log(x)-x^2}{\log(x)^3-x^2\log(x)}}~dx
=\frac{1}{2}\log\left(\frac{\log(x)+x}{\log(x)-x}\right)
+\int{\frac{dx}{\log(x)}}
\]
and since ${\rm deg}_z(Q_1) < {\rm deg}_z(R)$, it follows that the remaining
integral is not an elementary function (it is in fact the logarithmic
integral $Li(x)$).}

In the most general case, when $E=K(t)(y)$ is algebraic over $K(t)$ and
$y$ is integral over $K[t]$, the criterion part of the above result
remains valid: let $w=(w_1,\ldots,w_n)$ be an integral basis for $E$
over $K(t)$ and write the integrand $f \in E$ as 
$f=\sum_{i=1}^n A_iw_i/D+\sum_{i=1}^n B_iw_i/S$ where $S$ is special
and, following the Hermite reduction, $D$ is normal. Write 
$\sum_{i=1}^n A_iw_i=G/H$, where $G \in K[t,y]$ and $H \in K[t]$, let
$F \in K[t,y]$ be the (monic) minimum polynomial for $y$ over $K(t)$,
$z$ be a new indeterminante and compute
\begin{equation}\label{Int16}
R(z)={\rm resultant_t}({\rm pp_z}({\rm resultant_y}(G-tHD',F)),D) \in K[t]
\end{equation}
It can then be proven \cite{Bron90c} that if $f$ has an elementary integral
over $E$, then $R|\kappa(R)$ in $K[z]$.

{\bf Example 12} {\sl
Consider
\begin{equation}\label{Int17}
\int{\frac{\log(1+e^x)^{(1/3)}}{1+\log(1+e^x)}}~dx
\end{equation}
The integrand is
\[
f=\frac{y}{t+1} \in E = K(t)[y]/(y^3-t)
\]
where $K=\mathbb{Q}(x)(t_1)$, $t_1=e^x$ and $t=\log(1+t_1)$. Its
denominator with respect to the integral basis $w=(1,y,y^2)$ is
$D=t+1$, which is normal, and the resultant is
\[
R={\rm resultant_t}({\rm pp_z}({\rm resultant_y}(y-zt_1/(1+t_1),y^3-t)),t+1)
=-\frac{t_1^3}{(1+t_1)^3}z^3-1
\]
We have
\[
\kappa(R)=-\frac{3t_1^3}{(1+t_1)^4}z^3
\]
which is coprime with $R$ in $K[z]$, implying that the integral \ref{Int17}
is not an elementary function.
}

\subsection{The transcendental logarithmic case}
Suppose now that $t=\log(b)$ for some $b \in K^{*}$, and that
$E=K(t)$. Then, every special polynomial must be in $K$, so, following
the residue criterion, we must look for a solution $v \in K[t]$,
$u_1,\ldots,u_k \in K(c_1,\ldots,c_n)^{*}$ of \ref{Int13}. Furthermore, the
integrand $f$ is also in $K[t]$, so write
$f=\sum_{i=0}^d f_it^i$ where $f_0,\ldots,f_d \in K$ and $f_d \ne 0$. We
must have ${\rm deg}{}_t(v) \le d+1$, so writing $v=\sum_{i=0}^{d+1} v_it^i$,
we get 
\[
\int f_dt^d+\cdots+f_1t+f_0=v_{d+1}t^{d+1}+\cdots+v_1t+v_0
+\sum_{i=1}^k c_i\log(u_i)
\]
If $d=0$, then the above is simply an integration problem for 
$f_0 \in K$, which can be solved recursively. Otherwise,
differentiating both sides and equating the coefficients of $t^d$, we
get ${v_{d+1}}'=0$ and
\begin{equation}\label{Int18}
f_d=v_d'+(d+1)v_{d+1}\frac{b'}{b}
\end{equation}
Since $f_d \in K$, we can recursively apply the integration algorithm
to $f_d$, either proving that \ref{Int18} has no solution, in which case $f$
has no elementary integral, or obtaining the constant $v_{d+1}$, and
$v_d$ up to an additive constant (in fact, we apply recursively a
specialized version of the integration algorithm to equations of the
form \ref{Int18}, see \cite{Bron97} for details). Write then
$v_d=\overline{v_d}+c_d$ where $\overline{v_d} \in K$ is known and 
$c_d \in {\rm Const}(K)$ is undetermined. Equating the coefficients of
$t^{d-1}$ yields
\[
f_{d-1}-d\overline{v_d}\frac{b'}{b}={v_{d-1}}'+dc_d\frac{b'}{b}
\]
which is an equation of the form \ref{Int18}, so we again recursively compute
$c_d$ and $v_{d-1}$ up to an additive constant. We repeat this process
until either one of the recursive integrations fails, in which case $f$
has no elementary integral, or we reduce our integrand to an element
of $K$, which is then integrated recursively. The algorithm of this
section can also be applied to real arc-tangent extensions, {\sl i.e.}
$K(t)$ where $t$ is a monomial satisfying $t'=b'/(1+b^2)$ for some 
$b \in K$.

\subsection{The transcendental exponential case}
Suppose now that $t=e^b$ for some $b \in K$, and that $E =
K(t)$. Then, every nonzero special polynomial must be of the form
$at^m$ for $a \in K^{*}$ and $m \in \mathbb{N}$. Since
\[
\frac{(at^m)'}{at^m}=\frac{a'}{a}+m\frac{t'}{t}=\frac{a'}{a}+mb'
\]
we must then look for a solution $v\in K[t,t^{-1}]$,
$u_1,\ldots,u_k \in K(c_1,\ldots,c_n)^{*}$ of \ref{Int13}. Furthermore, the
integrand $f$ is also in $K[t,t^{-1}]$, so write
$f=\sum_{i=e}^d f_it^i$ where $f_e,\ldots,f_d \in K$ and 
$e,d\in \mathbb{Z}$. Since $(at^{m})'=(a'+mb')t^m$ for any 
$m\in \mathbb{Z}$, we must have $v=Mb+\sum_{i=e}^d v_it^i$ for some
integer $M$, hence
\[
\int\sum_{i=e}^d f_it^i=Mb+\sum_{i=e}^d v_it^i+\sum_{i=1}^k c_i\log(u_i)
\]
Differentiating both sides and equating the coefficients of each power
to $t^d$, we get
\[
f_0=(v_0+Mb)'+\sum_{i=1}^k c_i\frac{u_i^{\prime}}{u_i}
\]
which is simply an integration problem for $f_0 \in K$, and
\[
f_i=v_i^{\prime}+ib'v_i\quad{\rm for\ }e \le i \le d, i \ne 0
\]

The above problem is called a {\sl Risch differential equation over K}. 
Although solving it seems more complicated than solving $g'=f$, it
is actually simpler than an integration problem because we look for
the solutions $v_i$ in $K$ only rather than in an extension of
$K$. Bronstein \cite{Bron90c,Bron91a,Bron97} and Risch
\cite{Risc68,Risc69a,Risc69b} describe algorithms for solving this type
of equation when $K$ is an elementary extension of the rational
function field.

\subsection{The transcendental tangent case}
Suppose now that $t=\tan(b)$ for some $b \in K$, {\sl i.e.}
$t'=b'(1+t^2)$, that $\sqrt{-1} \notin K$ and that $E=K(t)$. Then,
every nonzero special polynomial must be of the form $a(t^2+1)^m$ for
$a \in K^{*}$ and $m \in \mathbb{N}$. Since
\[
\frac{(a(t^2+1)^m)'}{a(t^2+1)^m}
=\frac{a'}{a}+m\frac{(t^2+1)'}{t^2+1}
=\frac{a'}{a}+2mb't
\]
we must look for $v=V/(t^2+1)^m$ where $V\in K[t]$, 
$m_1,\ldots,m_k \in \mathbb{N}$, constants 
$c_1,\ldots,c_k \in \overline{K}$ and
$u_1,\ldots,u_k \in K(c_1,\ldots,c_k)^{*}$ such that
\[
f=v'+2b't\sum_{i=1}^k c_im_i + \sum_{i=1}^k c_i\frac{u_i^{\prime}}{u_i}
\]
Furthermore, the integrand $f \in K(t)$ following the residue
criterion must be of the form $f=A/(t^2+1)^M$ where $A \in K[t]$ and
$M \ge 0$. If $M > 0$, it can be shown that $m=M$ and that
\begin{equation}\label{Int19}
\left(
\begin{array}{c}
c'\\
d'
\end{array}
\right)+
\left(
\begin{array}{cc}
0&-2mb'\\
2mb'&0\\
\end{array}
\right)
\left(
\begin{array}{c}
c\\
d
\end{array}
\right)=
\left(
\begin{array}{c}
a\\
b
\end{array}
\right)
\end{equation}
where $at+b$ and $ct+d$ are the remainders module $t^2+1$ of $A$ and
$V$ respectively. The above is a coupled differential system, which
can be solved by methods similar to the ones used for Risch
differential equations \cite{Bron97}. If it has no solution, then the
integral is not elementary, otherwise we reduce the integrand to 
$h \in K[t]$, at which point the polynomial reduction either proves
that its integral is not elementary, or reduce the integrand to an
element of $K$, which is integrated recursively.

\noindent
{\bf Example 13} {\sl
Consider
\[
\int{\frac{sin(x)}{x}}~dx
\]
The integrand is
\[
f=\frac{2t/x}{t^2+1} \in K(t)\quad{\rm where\ }K=\mathbb{Q}(x)
{\rm\ and\ }t=\tan\left(\frac{x}{2}\right)
\]
Its denominator is $D=t^2+1$, which is special, and the system \ref{Int19}
becomes 
\[
\left(
\begin{array}{c}
c'\\
d'
\end{array}
\right)+
\left(
\begin{array}{cc}
0&-1\\
1&0\\
\end{array}
\right)
\left(
\begin{array}{c}
c\\
d
\end{array}
\right)=
\left(
\begin{array}{c}
2/x\\
0
\end{array}
\right)
\]
which has no solution in $\mathbb{Q}(x)$, implying that the integral
is not an elementary function.}

\subsection{The algebraic logarithmic case}
The transcendental logarithmic case method also generalizes to the
case when $E=K(t)(y)$ is algebraic over $K(t)$, $t=log(b)$ for 
$b \in K^{*}$ and $y$ is integral over $K[t]$: following the residue
criterion, we can assume that $R | \kappa(R)$ where $R$ is given by
\ref{Int16}, hence that all its roots in $\overline{K}$ are constants. The
polynomial part of the integrand is replace by a family of at most
$[E : K(t)]$ Puiseux expansions at infinity, each of the form
\begin{equation}\label{Int20}
a_{-m}\theta^{-m}+\cdots+a_{-1}\theta^{-1}+\sum_{i \ge 0} a_i\theta^i
\end{equation}
where $\theta^r=t^{-1}$ for some positive integer $r$. Applying the
integration algorithm recursively to $a_r \in \overline{K}$, we can
test whether there exist $\rho \in {\rm Const}(\overline{K})$ and 
$v \in \overline{K}$ such that
\[
a_r=v'+\rho\frac{b'}{b}
\]
If there are no such $v$ and $c$ for at least one of the series, then
the integral is not elementary, otherwise $\rho$ is uniquely
determined by $a_r$, so let $\rho_1,\ldots,\rho_q$ where 
$q \le [E : K(t)]$ be the distinct constants we obtain,
$\alpha_1,\ldots,\alpha_s \in \overline{K}$ be the distinct nonzero
roots of $R$, and $(q_1,\ldots,q_k)$ be a basis for the vector space
generated by the $\rho_i$'s and $\alpha_i$'s over $\mathbb{Q}$. Write
$\alpha_i=r_{i1}q_1+\cdots+r_{ik}q_k$ and 
$\rho_i=s_{i1}q_1+\cdots+s_{ik}q_k$ for each $i$, where 
$r_{ij},s_{ij} \in \mathbb{Q}$ and let $m > 0$ be a common denominator
for all the $r_{ij}$'s and $s_{ij}$'s. For $1 \le j \le k$, let
\[
\delta_j=\sum_{i=1}^s mr_{ij} \sum_l r_lP_l 
-\sum_{i=1}^q ms_{ij} \sum_l s_lQ_l
\]
where $r_l$ is the ramification index of $P_l$, $s_l$ is the
ramification index of $Q_l$, $P_l$ runs over all the finite places at
which $h~dz$ has residue $r_l\alpha_i$ and $Q_l$ runs over all the
infinite places at which $\rho=\rho_i$. As in the pure algebraic case,
if there is a $j$ for which $N\delta_j$ is not principal for any
nonzero integer $N$, then the integral is not elementary, otherwise,
let $n_1,\ldots,n_k$ be nonzero integers such that $n_j\delta_j$ is
principal for each $j$, and
\[
h=f-\frac{1}{m}\sum_{j=1}^k\frac{q_j}{n_j}\frac{u_j^{\prime}}{u_j}
\]
where $f$ is the integrand and 
$u_j \in E(\alpha_1,\ldots,\alpha_s,\rho_1,\ldots,\rho_q)^{*}$ is such
that $n_j\delta_j=(u_j)$. If the integral of $h$ is elementary, then
\ref{Int13} must have a solution with $v \in {\bf O}_{K[x]}$ and
$u_1,\ldots,u_k \in \overline{K}$ so we must solve
\begin{equation}\label{Int21}
h=\frac{\sum_{i=1}^n A_iw_i}{D}
=\sum_{i=1}^n v_i^{\prime}w_i+\sum_{i=1}^n v_iw_i^{\prime}
+\sum_{i=1}^k c_i\frac{u_i^{\prime}}{u_i}
\end{equation}
for $v_1,\ldots,v_n \in K[t]$, constants 
$c_1,\ldots,c_n \in \overline{K}$ and
$u_1,\ldots,u_k \in \overline{K}^{*}$ where 
$w=(w_1,\ldots,w_n)$ is an integral basis for $E$ over $K(t)$.

If $E$ is a simple radical extension of $K(t)$, and we use the basis
\ref{Int11} and the notation of that section, then $w_1=1$ and
\begin{equation}\label{Int22}
w_i^{\prime}=\left(\frac{i-1}{n}\frac{H'}{H}-
\frac{D_{i-1}^{\prime}}{D_{i-1}}\right)w_i
\quad{\rm for\ }1 \le i \le n
\end{equation}
This implies that \ref{Int21} becomes
\begin{equation}\label{Int23}
\frac{A_1}{D}=v_1^{\prime}+\sum_{i=1}^k c_i\frac{u_i^{\prime}}{u_i}
\end{equation}
which is simply an integration problem for $A_1/D \in K(t)$, and
\begin{equation}\label{Int24}
\frac{A_i}{D}=v_i^{\prime}+\left(\frac{i-1}{n}\frac{H'}{H}
-\frac{D_{i-1}^{\prime}}{D_{i-1}}\right)v_i\quad{\rm for\ }1 < i \le n
\end{equation}
which are Risch differential equations over $K(t)$

\noindent
{\bf Example 14} {\sl
Consider
\[
\int{\frac{(x^2+2x+1)\sqrt{x+\log(x)}+(3x+1)\log(x)+3x^2+x}
{(x\log(x)+x^2)\sqrt{x+\log(x)}+x^2\log(x)+x^3}}~dx
\]
The integrand is
\[
f=\frac{((3x+1)t-x^3+x^2)y-(2x^2-x-1)t-2x^3+x^2+x}
{xt^2-(x^3-2x^2)t-x^4+x^3} \in E = K(t)[y]/(F)
\]
where $F=y^2-x-t$, $K=\mathbb{Q}(x)$ and $t=\log(x)$. Its denominator
with respect to the integral basis $w=(1,y)$ is
$D=xt^2-(x^3-2x^2)t-x^4+x^3$, which is normal, and the resultant is
\[
\begin{array}{ccl}
R&=&{\rm resultant_t}({\rm pp_z}({\rm resultant_y}(((3x+1)t-x^3+x^2)y\\
&&\\
&&\hbox{\hskip 2.0cm}
-(2x^2-x-1)t-2x^3+x^2+x-zD^{\prime},F)),D)\\
&&\\
&=&x^{12}(2x+1)^2(x+1)^2(x-1)^2z^3(z-2)\\
\end{array}
\]
We have
\[
\kappa(R)=\frac{36x^3+16x^2-28x-12}{x(2x+1)(x+1)(x-1)}R
\]
so $R | \kappa(R)$ in $K[z]$. Its only nonzero root is 2, and the
integrand has residue 2 at the place $P$ corresponding to the point
$(t,y)=(x^2-x,-x)$. There is only one place $Q$ at infinity of
ramification index 2, and the coefficient of $t^{-1}$ in the Puiseux
expansion of $f$ at $Q$ is
\[
a_2=1-2x+\frac{1}{x}=(x-x^2)'+\frac{x'}{x}
\]
which implies that the corresponding $\rho$ is 1. Therefore, the
divisor for the logand is $\delta=2P-2Q$. It turns out that 
$\delta=(u)$ where $u=(x+y)^2 \in E^{*}$, so the new integrand is
\[
h=f-\frac{u'}{u}=f-2\frac{(x+y)'}{x+y}=\frac{(x+1)y}{xt+x^2}
\]
We have $y^2=t+x$, which is squarefree, so \ref{Int23} becomes
\[
0=v_1^{\prime}+\sum_{i=1}^k c_i\frac{u_i^{\prime}}{u_i}
\]
whose solution is $v_1=k=0$ and \ref{Int24} becomes
\[
\frac{x+1}{xt+x^2}=v_2^{\prime}+\frac{x+1}{2xt+2x^2}v_2
\]
whose solution is $v_2=2$, implying that $h=2y'$, hence that
\[
\begin{array}{l}
\displaystyle
\int{\frac{(x^2+2x+1)\sqrt{x+\log(x)}+(3x+1)\log(x)+3x^2+x}
{(x\log(x)+x^2)\sqrt{x+\log(x)}+x^2\log(x)+x^3}}~dx =\\
\\
\displaystyle
\hbox{\hskip 4.0cm}2\sqrt{x+\log(x)}+2\log\left(x+\sqrt{x+\log(x)}\right)
\end{array}
\]}
In the general case when $E$ is not a radical extension of $K(t)$, 
\ref{Int21} is solved by bounding ${\rm deg}_t(v_i)$ and comparing the Puiseux
expansions at infinity of $\sum_{i=1}^n v_iw_i$ with those of the form
\ref{Int20} of $h$, see \cite{Bron90c,Risc68} for details.

\subsection{The algebraic exponential case}
The transcendental exponential case method also generalizes to the
case when $E=K(t)(y)$ is algebraic over $K(t)$, $t=e^b$ for $b \in K$
and $y$ is integral over $K[t]$: following the residue criterion, we
can assume that $R|\kappa(R)$ where $R$ is given by \ref{Int16}, hence that
all its roots in $\overline{K}$ are constants. The denominator of the
integrand must be of the form $D=t^mU$ where $\gcd(U,t)=1$, $U$ is
squarefree and $m \ge 0$.

If $m > 0$, $E$ is a simple radical extension of $K(t)$, and we use the
basis \ref{Int11}, then it is possible to reduce the power of $t$ appearing
in $D$ by a process similar to the Hermite reduction: writing the
integrand $f=\sum_{i=1}^n A_iw_i/(t^mU)$, we ask whether we can
compute $b_1,\ldots,b_n \in K$ and $C_1,\ldots,C_n \in K[t]$ such that
\[
\int\frac{\sum_{i=1}^n A_iw_i}{t^mU}
=\frac{\sum_{i=1}^n b_iw_i}{t^m}
+\int{\frac{\sum_{i=1}^n C_iw_i}{t^{m-1}U}}
\]
Differentiating both sides and multiplying through by $t^m$ we get
\[
\frac{\sum_{i=1}^n A_iw_i}{U}
=\sum_{i=1}^n b_i^{\prime}w_i+\sum_{i=1}^n b_iw_i^{\prime}
-mb'\sum_{i=1}^n b_iw_i+\frac{t\sum_{i=1}^n C_iw_i}{U}
\]
Using \ref{Int22} and equating the coefficients of $w_i$ on both sides, we
get
\begin{equation}\label{Int25}
\frac{A_i}{U}=b_i^{\prime}+(\omega_i-mb')b_i+\frac{tC_i}{U}
\quad{\rm for\ }1 \le i \le n
\end{equation}
where
\[
\omega_i=\frac{i-1}{n}\frac{H'}{H}-\frac{D_{i-1}^{\prime}}{D_{i-1}} \in K(t)
\]
Since $t'/t=b' \in K$, it follows that the denominator of $\omega_i$
is not divisible by $t$ in $K[t]$, hence, evaluating \ref{Int25} at $t=0$, we
get 
\begin{equation}\label{Int26}
\frac{A_i(0)}{U(0)}=b_i^{\prime}+(\omega_i(0)-mb')b_i
\quad{\rm for\ }1 \le i \le n
\end{equation}
which are Risch differential equations over $K(t)$. If any of them has
no solution in $K(t)$, then the integral is not elementary, otherwise
we repeat this process until the denominator of the integrand is
normal. We then perform the change of variable $\overline{t}=t^{-1}$,
which is also exponential over $K$ since
$\overline{t}'=-b'\overline{t}$, and repeat the above process in order
to eliminate the power of $\overline{t}$ from the denominator of the
integrand. It can be shown that after this process, any solution of
\ref{Int13} must have $v \in K$.

\noindent
{\bf Example 15} {\sl
Consider
\[
\int{\frac{3(x+e^x)^{(1/3)}+(2x^2+3x)e^x+5x^2}{x(x+e^x)^{(1/3)}}}~dx
\]
The integrand is
\[
f=\frac{((2x^2+3x)t+5x^2)y^2+3t+3x}{xt+x^2} \in E
=K(t)[y]/(y^3-t-x)
\]
where $K=\mathbb{Q}(x)$ and $t=e^x$. Its denominator with respect to
the integral basis $w=(1,y,y^2)$ is $D=xt+x^2$, which is normal, and the
resultant is
\[
\begin{array}{l}
R={\rm resultant_t}({\rm pp_z}({\rm resultant_y}
(((2x^2+3x)t+5x^2)y^2+3t+3x-zD',\\
\hbox{\hskip 5.0cm}y^3-t-x)),D)=x^8(1-x)^3z^3
\end{array}
\]
We have
\[
\kappa(R)=\frac{11x-8}{x(x-1)}R
\]
so $R|\kappa(R)$ in $K[z]$, its only root being 0. Since $D$ is not
divisible by $t$, let $\overline{t}=t^{-1}$ and $z=\overline{t}y$. We
have $\overline{t}'=-\overline{t}$ and 
$z^3-\overline{t}^2-x\overline{t}^3=0$, so the integral basis \ref{Int11} is
\[
\overline{w}=(\overline{w}_1,\overline{w}_2,\overline{w}_3)
=\left(1,z,\frac{z^2}{\overline{t}}\right)
\]
Writing $f$ in terms of that basis gives
\[
f=\frac{3x\overline{t}^2+3\overline{t}
+(5x^2\overline{t}+2x^2+3x)\overline{w}_3}
{x^2\overline{t}^2+x\overline{t}}
\]
whose denominator $\overline{D}=\overline{t}(x+x^2\overline{t})$ is
divisible by $\overline{t}$. We have
$H=\overline{t}^2(1+x\overline{t})$ so $D_0=D_1=1$ and
$D_2=\overline{t}$, implying that
\[
\omega_1=0, \omega_2=\frac{(1-3x)\overline{t}-2}{3x\overline{t}+3},
{\rm\ and\ } \omega_3=\frac{(2-3x)\overline{t}-1}{3x\overline{t}+3}
\]
Therefore the equations \ref{Int26} become
\[
0=b_1^{\prime}+b_1,0=b_2^{\prime}+\frac{1}{3}b_2,{\rm\ and\ }
2x+3=b_3^{\prime}+\frac{2}{3}b_3
\]
whose solutions are $b_1=b_2=0$ and $b_3=3x$, implying that the new
integrand is
\[
h=f-\left(\frac{3x\overline{w}_3}{\overline{t}}\right)^{\prime}=\frac{3}{x}
\]
hence that
\[
\int{\frac{3(x+e^x)^{(1/3)}+(2x^2+3x)e^x+5x^2}{x(x+e^x)^{(1/3)}}}~dx
=3x(x+e^x)^{(2/3)}+3\int{\frac{dx}{x}}
\]
}

In the general case when $E$ is not a radical extension of $K(t)$,
following the Hermite reduction, any solution of \ref{Int13} must have
$v=\sum_{i=1}^n v_iw_i/t^m$ where $v_1,\ldots,v_m \in K[t]$. We can
compute $v$ by bounding ${\rm deg}_t(v_i)$ and comparing the Puiseux
expansions at $t=0$ and at infinity of $\sum_{i=1}^n v_iw_i/t^m$ with
those of the form \ref{Int20} of the integrand, 
see \cite{Bron90c,Risc68} for details.

Once we are reduced to solving \ref{Int13} for $v \in K$, constants
$c_1,\ldots,c_k \in \overline{K}$ and 
$u_1,\ldots,u_k \in E(c_1,\ldots,c_k)^{*}$, constants
$\rho_1,\ldots,\rho_s \in \overline{K}$ can be determined at all the
places above $t=0$ and at infinity in a manner similar to the
algebraic logarithmic case, at which point the algorithm proceeds by
constructing the divisors $\delta_j$ and the $u_j$'s as in that
case. Again, the details are quite technical and can be found in 
\cite{Bron90c,Risc68,Risc69a}.

\chapter{Singular Value Decomposition}
\section{Singular Value Decomposition Tutorial}

When you browse standard web sources like Wikipedia to learn about 
Singular Value Decomposition \cite{Puff09} 
or SVD you find many equations, but 
not an intuitive explanation of what it is or how it works. SVD 
is a way of factoring matrices into a series of linear approximations 
that expose the underlying structure of the matrix. Two important 
properties are that the linear factoring is exact and optimal. Exact 
means that the series of linear factors, added together, exactly 
equal the original matrix. Optimal means that, for the standard 
means of measuring matrix similarity (the Frobenius norm), these 
factors give the best possible linear approximation at each step 
in the series.

SVD is extraordinarily useful and has many applications such as 
data analysis, signal processing, pattern recognition, image 
compression, weather prediction, and Latent Sematic Analysis or 
LSA (also referred to as Latent Semantic Indexing). Why is SVD 
so useful and how does it work?

As a simple example, let's look at golf scores. Suppose Phil, 
Tiger, and Vijay play together for 9 holes and they each make 
par on every hole. Their scorecard, which can also be viewed as 
a (hole x player) matrix might look like this.

\begin{tabular}{|c|c|c|c|c|}
\hline
Hole & Par & Phil &  Tiger & Vijay\\
\hline
1  & 4  & 4  & 4  & 4\\
2  & 5  & 5  & 5  & 5\\
3  & 3  & 3  & 3  & 3\\
4  & 4  & 4  & 4  & 4\\
5  & 4  & 4  & 4  & 4\\
6  & 4  & 4  & 4  & 4\\
7  & 4  & 4  & 4  & 4\\
8  & 3  & 3  & 3  & 3\\
9  & 5  & 5  & 5  & 5\\
\hline
\end{tabular}

Let's look at the problem of trying to predict what score each 
player will make on a given hole. One idea is give each hole a 
HoleDifficulty factor, and each player a PlayerAbility factor. 
The actual score is predicted by multiplying these two factors 
together.

PredictedScore = HoleDifficulty * PlayerAbility

For the first attempt, let's make the HoleDifficulty be the par 
score for the hole, and let's make the player ability equal to 1. 
So on the first hole, which is par 4, we would expect a player 
of ability 1 to get a score of 4.

PredictedScore = HoleDifficulty * PlayerAbility = 4 * 1 = 4

For our entire scorecard or matrix, all we have to do is multiply 
the PlayerAbility (assumed to be 1 for all players) by the 
HoleDifficulty (ranges from par 3 to par 5) and we can exactly 
predict all the scores in our example.

In fact, this is the one dimensional (1-D) SVD factorization of 
the scorecard. We can represent our scorecard or matrix as the 
product of two vectors, the HoleDifficulty vector and the 
PlayerAbility vector. To predict any score, simply multiply the 
appropriate HoleDifficulty factor by the appropriate PlayerAbility 
factor. Following normal vector multiplication rules, we can 

generate the matrix of scores by multiplying the HoleDifficulty 
vector by the PlayerAbility vector, according to the following 
equation.
 
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
4  & 4  & 4\\
5  & 5  & 5\\
3  & 3  & 3\\
4  & 4  & 4\\
4  & 4  & 4\\
4  & 4  & 4\\
4  & 4  & 4\\
3  & 3  & 3\\
5  & 5  & 5\\
\hline
\end{tabular}
 =  
\begin{tabular}{|c|}
\hline
4\\
5\\
3\\
4\\
4\\
4\\
4\\
3\\
5\\
\hline
\end{tabular}
 *  
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
1  & 1  & 1\\
\hline
\end{tabular}

which is HoleDifficulty * PlayerAbility

Mathematicians like to keep everything orderly, so the convention 
is that all vectors should be scaled so they have length 1. For 
example, the PlayerAbility vector is modified so that the sum of 
the squares of its elements add to 1, instead of the current 
$12 + 12 + 12 = 3$. To do this, we have to divide each element by 
the square root of 3, so that when we square it, it becomes 
and the three elements add to 1. Similarly, we have to divide 
each HoleDifficulty element by the square root of 148. The square 
root of 3 times the square root of 148 is our scaling factor 21.07. 
The complete 1-D SVD factorization (to 2 decimal places) is:
 
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
4  & 4  & 4\\
5  & 5  & 5\\
3  & 3  & 3\\
4  & 4  & 4\\
4  & 4  & 4\\
4  & 4  & 4\\
4  & 4  & 4\\
3  & 3  & 3\\
5  & 5  & 5\\
\hline
\end{tabular}
 =  
\begin{tabular}{|c|}
\hline
0.33\\
0.41\\
0.25\\
0.33\\
0.33\\
0.33\\
0.33\\
0.25\\
0.41\\
\hline
\end{tabular}
 *  
\begin{tabular}{|c|}
\hline
21.07\\
\hline
\end{tabular}
 *  
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
0.58  & 0.58  & 0.58\\
\hline
\end{tabular}

which is HoleDifficulty * ScaleFactor * PlayerAbility

Our HoleDifficulty vector, that starts with 0.33, is called the 
Left Singular Vector. The ScaleFactor is the Singular Value, and 
our PlayerAbility vector, that starts with 0.58 is the Right 
Singular Vector. If we represent these 3 parts exactly, and multiply 
them together, we get the exact original scores. This means our 
matrix is a rank 1 matrix, another way of saying it has a simple 
and predictable pattern.

More complicated matrices cannot be completely predicted just by 
using one set of factors as we have done. In that case, we have to 
introduce a second set of factors to refine our predictions. To do 
that, we subtract our predicted scores from the actual scores, 
getting the residual scores. Then we find a second set of 
HoleDifficulty2 and PlayerAbility2 numbers that best predict the 
residual scores.

Rather than guessing HoleDifficulty and PlayerAbility factors and 
subtracting predicted scores, there exist powerful algorithms than 
can calculate SVD factorizations for you. Let's look at the actual 
scores from the first 9 holes of the 2007 Players Championship as 
played by Phil, Tiger, and Vijay.

\begin{tabular}{|c|c|c|c|c|}
\hline
Hole  & Par  & Phil  & Tiger  & Vijay\\
\hline
1  & 4  & 4  & 4  & 5\\
2  & 5  & 4  & 5  & 5\\
3  & 3  & 3  & 3  & 2\\
4  & 4  & 4  & 5  & 4\\
5  & 4  & 4  & 4  & 4\\
6  & 4  & 3  & 5  & 4\\
7  & 4  & 4  & 4  & 3\\
8  & 3  & 2  & 4  & 4\\
9  & 5  & 5  & 5  & 5\\
\hline
\end{tabular}

The 1-D SVD factorization of the scores is shown below. To make 
this example easier to understand, I have incorporated the ScaleFactor 
into the PlayerAbility and HoleDifficulty vectors so we can ignore 
the ScaleFactor for this example.
 
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
3.95  & 4.64  & 4.34\\
4.27  & 5.02  & 4.69\\
2.42  & 2.85  & 2.66\\
3.97  & 4.67  & 4.36\\
3.64  & 4.28  & 4.00\\
3.69  & 4.33  & 4.05\\
3.33  & 3.92  & 3.66\\
3.08  & 3.63  & 3.39\\
4.55  & 5.35  & 5.00\\
\hline
\end{tabular}
 =  
\begin{tabular}{|c|}
\hline
4.34\\
4.69\\
2.66\\
4.36\\
4.00\\
4.05\\
3.66\\
3.39\\
5.00\\
\hline
\end{tabular}
 *  
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
0.91  & 1.07  & 1.00\\
\hline
\end{tabular}

which is HoleDifficulty * PlayerAbility

Notice that the HoleDifficulty factor is almost the average of that 
hole for the 3 players. For example hole 5, where everyone scored 4, 
does have a factor of 4.00. However hole 6, where the average score 
is also 4, has a factor of 4.05 instead of 4.00. Similarly, the 
PlayerAbility is almost the percentage of par that the player 
achieved, For example Tiger shot 39 with par being 36, and 
$39/36 = 1.08$ which is almost his PlayerAbility factor (for these 
9 holes) of 1.07.

Why don't the hole averages and par percentages exactly match the 
1-D SVD factors? The answer is that SVD further refines those 
numbers in a cycle. For example, we can start by assuming 
HoleDifficulty is the hole average and then ask what PlayerAbility 
best matches the scores, given those HoleDifficulty numbers? Once 
we have that answer we can go back and ask what HoleDifficulty best 
matches the scores given those PlayerAbility numbers? We keep 
iterating this way until we converge to a set of factors that best 
predict the score. SVD shortcuts this process and immediately give 
us the factors that we would have converged to if we carried out 
the process.

One very useful property of SVD is that it always finds the optimal 
set of factors that best predict the scores, according to the 
standard matrix similarity measure (the Frobenius norm). That is, 
if we use SVD to find the factors of a matrix, those are the best 
factors that can be found. This optimality property means that we 
don't have to wonder if a different set of numbers might predict 
scores better.

Now let's look at the difference between the actual scores and our 
1-D approximation. A plus difference means that the actual score is 
higher than the predicted score, a minus difference means the actual 
score is lower than the prediction. For example, on the first hole 
Tiger got a 4 and the predicted score was 4.64 so we get 
$4 - 4.64 = -0.64$. In other words, we must add -0.64 to our prediction 
to get the actual score.

Once these differences have been found, we can do the same thing 
again and predict these differences using the formula 
HoleDifficulty2 * PlayerAbility2. Since these factors are trying 
to predict the differences, they are the 2-D factors and we have 
put a 2 after their names (ex. HoleDifficulty2) to show they are 
the second set of factors.
 
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
0.05  & -0.64  & 0.66\\
-0.28  & -0.02  & 0.31\\
0.58  & 0.15  & -0.66\\
0.03  & 0.33  & -0.36\\
0.36  & -0.28  & 0.00\\
-0.69  & 0.67  & -0.05\\
0.67  & 0.08  & -0.66\\
-1.08  & 0.37  & 0.61\\
0.45  & -0.35  & 0.00\\
\hline
\end{tabular}
 =  
\begin{tabular}{|c|}
\hline
-0.18\\
-0.38\\
0.80\\
0.15\\
0.35\\
-0.67\\
0.89\\
-1.29\\
0.44\\
\hline
\end{tabular}
 *  
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
0.82  & -0.20  & -0.53\\
\hline
\end{tabular}

which is HoleDifficulty(2) * PlayerAbility(2)

There are some interesting observations we can make about these 
factors. Notice that hole 8 has the most significant HoleDifficulty2 
factor (1.29). That means that it is the hardest hole to predict. 
Indeed, it was the only hole on which none of the 3 players made 
par. It was especially hard to predict because it was the most 
difficult hole relative to par 
$(HoleDifficulty - par) = (3.39 - 3) = 0.39$, and yet Phil birdied 
it making his score more than a stroke below his predicted score 
(he scored 2 versus his predicted score of 3.08). Other holes that 
were hard to predict were holes 3 (0.80) and 7 (0.89) because Vijay 
beat Phil on those holes even though, in general, Phil was playing 
better.

The full SVD for this example matrix (9 holes by 3 players) has 3 
sets of factors. In general, a m x n matrix where m >= n can have 
at most n factors, so our $9 x 3$ matrix cannot have more than 3 sets 
of factors. Here is the full SVD factorization (to two decimal places).
 
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
4  & 4  & 5\\
4  & 5  & 5\\
3  & 3  & 2\\
4  & 5  & 4\\
4  & 4  & 4\\
3  & 5  & 4\\
4  & 4  & 3\\
2  & 4  & 4\\
5  & 5  & 5\\
\hline
\end{tabular}
 =  
\begin{tabular}{|c|c|c|}
\hline
4.34  & -0.18  & -0.90\\
4.69  & -0.38  & -0.15\\
2.66  & 0.80  & 0.40\\
4.36  & 0.15  & 0.47\\
4.00  & 0.35  & -0.29\\
4.05  & -0.67  & 0.68\\
3.66  & 0.89  & 0.33\\
3.39  & -1.29  & 0.14\\
5.00  & 0.44  & -0.36\\
\hline
\end{tabular}
 *  
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
0.91  & 1.07  & 1.00\\
0.82  & -0.20  & -0.53\\
-0.21  & 0.76  & -0.62\\
\hline
\end{tabular}

which is HoleDifficulty(1-3) * PlayerAbility(1-3)

By SVD convention, the HoleDifficulty and PlayerAbility vectors 
should all have length 1, so the conventional SVD factorization 
is:
 
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
4  & 4  & 5\\
4  & 5  & 5\\
3  & 3  & 2\\
4  & 5  & 4\\
4  & 4  & 4\\
3  & 5  & 4\\
4  & 4  & 3\\
2  & 4  & 4\\
5  & 5  & 5\\
\hline
\end{tabular}
 =  
\begin{tabular}{|c|c|c|}
\hline
0.35  & 0.09  & -0.64\\
0.38  & 0.19  & -0.10\\
0.22  & -0.40  & 0.28\\
0.36  & -0.08  & 0.33\\
0.33  & -0.18  & -0.20\\
0.33  & 0.33  & 0.48\\
0.30  & -0.44  & 0.23\\
0.28  & 0.64  & 0.10\\
0.41  & -0.22  & -0.25\\
\hline
\end{tabular}
 *  
\begin{tabular}{|c|c|c|}
\hline
21.07  & 0  & 0\\
0  & 2.01  & 0\\
0  & 0  & 1.42\\
\hline
\end{tabular}
 * 
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
0.53  & 0.62  & 0.58\\
-0.82  & 0.20  & 0.53\\
-0.21  & 0.76  & -0.62\\
\hline
\end{tabular}

which is HoleDifficulty(1-3)* ScaleFactor(1-3) * PlayerAbility(1-3)

We hope that you have some idea of what SVD is and how it can be 
used. The next section covers applying SVD to Latent Sematic 
Analysis or LSA. Although the domain is different, the concepts 
are the same. We are trying to predict patterns of how words occur 
in documents instead of trying to predict patterns of how players 
score on holes.
\chapter{Quaternions}
from \cite{Altm05}:
\begin{quotation}
Quaternions are inextricably linked to rotations.
Rotations, however, are an accident of three-dimensional space.
In spaces of any other dimensions, the fundamental operations are
reflections (mirrors). The quaternion algebra is, in fact, merely a
sub-algebra of the Clifford algebra of order three. If the quaternion
algebra might be labelled the algebra of rotations, then the Clifford
algebra is the algebra of mirrors and it is thus vastly more general
than quaternion algebra. 
\end{quotation}
\begin{center}
\bigskip
\large Peter Guthrie Tait, Robert S. Sutor, Timothy Daly

\end{center}
\section*{Preface}
\addcontentsline{toc}{section}{Preface}
The Theory of Quaternions is due to Sir William Rowan Hamilton,
Royal Astronomer of Ireland, who presented his first paper on the
subject to the Royal Irish Academy in 1843. His Lectures on
Quaternions were published in 1853, and his Elements, in 1866,
shortly after his death. The Elements of Quaternions by Tait \cite{Tait1890} 
is the accepted text-book for advanced students.

Large portions of this file are derived from a public domain version
of Tait's book combined with the algebra available in Axiom.
The purpose is to develop a tutorial introduction to the Axiom
domain and its uses.
\newpage

\section{Quaternions}

\section{Vectors, and their Composition}

{\bf 1}. For at least two centuries the geometrical representation 
of the negative and imaginary algebraic quantities, $-1$ and $\sqrt{-1}$
has been a favourite subject of speculation with mathematicians. 
The essence of almost all of the proposed processes consists in 
employing such expressions to indicate the DIRECTION, not the 
{\sl length}, of lines. 

{\bf 2}. Thus it was long ago seen that if positive quantities were 
measured off in one direction along a fixed line, a useful and lawful 
convention enabled us to express negative quantities of the same 
kind by simply laying them off on the same line in the opposite 
direction. This convention is an essential part of the Cartesian 
method, and is constantly employed in Analytical Geometry and 
Applied Mathematics. 

{\bf 3}. Wallis, towards the end of the seventeenth century, proposed 
to represent the impossible roots of a quadratic equation by going 
{\sl out} of the line on which, if real, they would have been laid off. 
This construction is equivalent to the consideration of $\sqrt{-1}$ as a 
directed unit-line perpendicular to that on which real quantities 
are measured. 

{\bf 4}. In the usual notation of Analytical Geometry of two 
dimensions, when rectangular axes are employed, this amounts 
to reckoning each unit of length along $Oy$ as $+\sqrt{-1}$, and on 
$Oy^{\prime}$ as $-\sqrt{-1}$ ; while on $Ox$ each unit is $+1$, and on 
$Ox$ it is $-1$. 

If we look at these four lines in circular order, i.e. in the order of 
positive rotation (that of the northern hemisphere of the earth 
about its axis, or {\sl opposite} to that of the hands of a watch), they 
give 
$$ 1, \sqrt{-1}, -1, -\sqrt{-1}$$

\boxer{4.6in}{
\vskip 0.1cm
In Axiom the same elements would be written as complex numbers
which are constructed using the function {\bf complex}:
\spadcommand{complex(1,0)}
$$1$$
\returnType{Type: Complex Integer}
}
\boxer{4.6in}{
\vskip 0.1cm
\spadcommand{complex(0,1)}
$$\%i$$
\returnType{Type: Complex Integer}
}
\boxer{4.6in}{
\vskip 0.1cm
\spadcommand{complex(-1,0)}
$$-1$$
\returnType{Type: Complex Integer}
}
\boxer{4.6in}{
\vskip 0.1cm
\spadcommand{complex(0,-1)}
$$-i$$
\returnType{Type: Complex Integer}
}
\boxer{4.6in}{
\vskip 0.1cm
Note that \%i is of type Complex(Integer), that is, the imaginary
part of a complex number. The apparently equivalent expression
\spadcommand{sqrt(-1)}
$$\sqrt{-1}$$
\returnType{Type: AlgebraicNumber}
has the type AlgebraicNumber which means that it is the root of
a polynomial with rational coefficients.\\
}

In this series each expression is derived from that which precedes 
it by multiplication by the factor $\sqrt{-1}$. Hence we may consider 
$\sqrt{-1}$ as an operator, analogous to a handle perpendicular to the 
plane of $xy$, whose effect on any line in that plane is to make it 
rotate (positively) about the origin through an angle of $90^{\circ}$. 

\boxer{4.6in}{
\vskip 0.1cm
In Axiom 
\spadcommand{\%i*\%i}
$$-1$$
\returnType{Type: Complex Integer}
}

{\bf 5}. In such a system, (which seems to have been first developed, 
in 1805, by Bu\'ee) a point in the plane of reference is defined by a 
single imaginary expression. Thus $a + b\sqrt{-1}$ may be considered 
as a single quantity, denoting the point, $P$, whose coordinates are 
$a$ and $b$. Or, it may be used as an expression for the line $OP$ 
joining that point with the origin. In the latter sense, the expression 
$a + b\sqrt{-1}$ implicitly contains the {\sl direction}, as well as the 
{\sl length}, of this line ; since, as we see at once, the direction is 
inclined at an angle $\tan^{-1}(b/a)$ to the axis of $x$, 
and the length is $\sqrt{a^2+b^2}$. Thus, say we have 
$$OP = a + b\sqrt{-1}$$
the line $OP$ considered as that by which we pass from one 
extremity, $O$, to the other, $P$. In this sense it is called a VECTOR. 
Considering, in the plane, any other vector, 
$$OQ = a^{\prime}+b^{\prime}\sqrt{-1}$$

\boxer{4.6in}{
\vskip 0.1cm
In order to created superscripted variables we use the superscript
function from the SYMBOL domain. So we can create $a^{\prime}$ as ``ap''
(that is, ``a-prime'') and $b^{\prime}$ as ``bp'' (``b-prime'') thus
(also note that the underscore character is Axiom's escape character
which removes any special meaning of the next character, in this case,
the quote character):
\spadcommand{ap:=superscript(a,[\_'])}
$$a^{\prime}$$
\returnType{Type: Symbol}
}
\boxer{4.6in}{
\vskip 0.1cm
\spadcommand{bp:=superscript(b,[\_'])}
$$b^{\prime}$$
\returnType{Type: Symbol}
}
\boxer{4.6in}{
\vskip 0.1cm
at this point we can type
\spadcommand{ap+bp*\%i}
$$a^{\prime}+b^{\prime}\ \%i$$
\returnType{Type: Complex Polynomial Integer}
}

the addition of these two lines obviously gives 
$$OR = a + a^{\prime} + (b + b^{\prime})\sqrt{-1}$$

\boxer{4.6in}{
\vskip 0.1cm
In Axiom the computation looks like:
\spadcommand{op:=complex(a,b)}
$$a + b\ \%i$$
\returnType{Type: Complex Polynomial Integer}
}
\boxer{4.6in}{
\vskip 0.1cm
\spadcommand{oq:=complex(ap,bp)}
$$a^{\prime} + b^{\prime}\ \%i$$
\returnType{Type: Complex Polynomial Integer}
}
\boxer{4.6in}{
\vskip 0.1cm
\spadcommand{op + oq}
$$a + a^{\prime} + (b + b^{\prime})\%i$$
\returnType{Type: Complex Polynomial Integer}
}

and we see that the sum is the diagonal of the parallelogram on 
$OP$, $OQ$. This is the law of the composition of simultaneous 
velocities; and it contains, of course, the law of subtraction of one 
directed line from another. 

{\bf 6}. Operating on the first of these symbols by the factor $\sqrt{-1}$,
it becomes $- b + a\sqrt{-1}$; and now, of course, denotes the point 
whose $x$ and $y$ coordinates are $- b$ and $a$; or the line joining this 
point with the origin. The length is still $\sqrt{a^2+b^2}$, but the angle 
the line makes with the axis of $x$ is $\tan^{-1}(- a/b)$; which is 
evidently greater by $\pi/2$ than before the operation. 

\boxer{4.6in}{
\vskip 0.1cm
\spadcommand{op*complex(0,1)}
$$-b+a\ i$$
\returnType{Type: Complex Polynomial Integer}
}

{\bf 7}. De Moivre's Theorem tends to lead us still further in the 
same direction. In fact, it is easy to see that if we use, instead 
of $\sqrt{-1}$, the more general factor $\cos \alpha + \sqrt{-1} \sin \alpha$, 
its effect on 
any line is to turn it through the (positive) angle $\alpha$. in the plane 
of $x$, $y$. [Of course the former factor, $\sqrt{-1}$, is merely the 
particular case of this, when $\alpha=\frac{\pi}{2}$].

Thus 
$$
\begin{array}{ll}
  &(\cos \alpha + \sqrt{-1} \sin \alpha) (a + b \sqrt{-1})\\
= & a \cos \alpha - b \sin \alpha + \sqrt{-1} (a \sin \alpha + b \cos \alpha)
\end{array}
$$

by direct multiplication. The reader will at once see that the new 
form indicates that a rotation through an angle $\alpha$ has taken place, 
if he compares it with the common formulae for turning the coordinate 
axes through a given angle. Or, in a less simple manner, thus 

$$
\begin{array}{rcl}
Length & = & \sqrt{(a \cos \alpha - b \sin \alpha)^2 +
                   (a \sin \alpha + b \cos \alpha)^2} \\
       & = & \sqrt{a^2 + b^2}
\end{array}
$$
as before. 

Inclination to axis of $x$
$$
\begin{array}{cl}
= & \tan^{-1}{\frac{a \sin \alpha + b \cos \alpha}
                   {a \cos \alpha - b \sin \alpha}}\\
= & \tan^{-1}{\frac{\tan \alpha + \frac{b}{a}}
                   {1 - \frac{b}{a} \tan \alpha}}\\
= & \alpha + \tan^{-1}{\frac{b}{a}}
\end{array}
$$

{\bf 8}. We see now, as it were, why it happens that 

$$(\cos \alpha + \sqrt{-1} \sin \alpha)^m = 
\cos m\alpha + \sqrt{-1} \sin m\alpha
$$ 

In fact, the first operator produces $m$ successive rotations in the 
same direction, each through the angle $\alpha$ ; the second, a single 
rotation through the angle $m\alpha$. 

{\bf 9}. It may be interesting, at this stage, to anticipate so far as to 
remark that in the theory of Quaternions the analogue of 

$$
\begin{array}{lclr}
               & \textrm{     } & \cos \theta + \sqrt{-1} \sin \theta &\\
\textrm{is}    & \textrm{     } & \cos \theta + \omega \sin \theta   &\\
\textrm{where} & \textrm{     } & \omega^2 = -1                      & \\
\end{array}
$$

Here, however, $\omega$ is not the algebraic $\sqrt{-1}$, but is 
{\sl any directed unit-line} whatever in space. 

{\bf 10}. In the present century Argand, Warren, Mourey, and 
others, extended the results of Wallis and Bu\'ee. They attempted 
to express as a line the product of two lines each represented by a 
symbol such $a+b\sqrt{-1}$. To a certain extent they succeeded, 
but all their results remained confined to two dimensions. 

The product, $\prod$, of two such lines was defined as the fourth 
proportional to unity and the two lines, thus 

$$
\begin{array}{lclr}
               & \textrm{     } & 
1 : a+b\sqrt{-1} :: a^{\prime}+b^{\prime}\sqrt{-1} : \prod\\
\textrm{or}    & \textrm{     } & 
\prod = (aa^{\prime} - bb^{\prime})+(a^{\prime}b+b^{\prime}a)\sqrt{-1}
\end{array}
$$

The length of $\prod$ is obviously the product of the lengths of the 
factor lines; and its direction makes an angle with the axis of $x$ 
which is the sum of those made by the factor lines. From this 
result the quotient of two such lines follows immediately. 

{\bf 11}. A very curious speculation, due to Servois and published 
in 1813 in Gergonne's {\sl Annales}, is one of the very few, so far as has 
been discovered, in which a well-founded guess at a possible mode 
of extension to three dimensions is contained. Endeavouring to 
extend to {\sl space} the form $a+b\sqrt{-1}$ for the plane, he is guided by 
analogy to write for a directed unit-line in space the form 

$$p \cos \alpha + q \cos \beta + r \cos \gamma$$

where $\alpha$, $\beta$, $\gamma$ 
are its inclinations to the three axes. He perceives 
easily that $p$, $q$, $r$ must be {\sl non-reals}: 
but, he asks, ``seraient-elles 
{\sl imaginaires} r\'eductibles \`a la forme g\'en\'erale $A+B\sqrt{-1}$?'' 
The $i$,$j$, $k$ of the Quaternion Calculus furnish an answer to this 
question. (See Chap. II.) But it may be remarked that, in applying the 
idea to lines in a plane, a vector $OP$ will no longer be represented 
(as in \S 5) by 
$$
\begin{array}{lclr}
                            & \textrm{   OP} & = & a + b\sqrt{-1}\\
\textrm{but by}             & \textrm{   OP} & = & pa + qb\\
\textrm{And if, similarly,} & \textrm{   OQ} & = & pa^{\prime} + qb^{\prime}\\
\end{array}
$$

the addition of these two lines gives for $OR$ (which retains its 
previous signification) 
$$OR = p(a+a^{\prime} + q(b+b^{\prime})$$

{\bf 12}. Beyond this, few attempts were made, or at least recorded, 
in earlier times, to extend the principle to space of three dimensions; 
and, though many such had been made before 1843, none, 
with the single exception of Hamilton's, have resulted in simple, 
practical methods; all, however ingenious, seeming to lead almost 
at once to processes and results of fearful complexity. 

For a lucid, complete, and most impartial statement of the 
claims of his predecessors in this field we refer to the Preface to 
Hamilton's {\sl Lectures on Quaternions}. He there shows how his long 
protracted investigations of Sets culminated in this unique system 
of tridimensional-space geometry. 

{\bf 13}. It was reserved for Hamilton to discover the use and 
properties of a class of symbols which, though all in a certain sense 
square roots of -1, may be considered as {\sl real} unit lines, tied down 
to no particular direction in space ; the expression for a vector is, 
or may be taken to be, 
$$ \rho = ix + jy + kz$$

but such vector is considered in connection with an {\sl extraspatial}
magnitude $w$, and we have thus the notion of a QUATERNION 

$$w + \rho$$

This is the fundamental notion in the singularly elegant, and 
enormously powerful, Calculus of Quaternions. 

While the schemes for using the algebraic $\sqrt{-1}$ to indicate 
direction make one direction in space expressible by real numbers, 
the remainder being imaginaries of some kind, and thus lead to 
expressions which are heterogeneous ; Hamilton s system makes all 
directions in space equally imaginary, or rather equally real, thereby 
ensuring to his Calculus the power of dealing with space 
indifferently in all directions. 

In fact, as we shall see, the Quaternion method is independent 
of axes or any supposed directions in space, and takes its reference 
lines solely from the problem it is applied to. 

{\bf 14}. But, for the purpose of elementary exposition, it is best 
to begin by assimilating it as closely as we can to the ordinary 
Cartesian methods of Geometry of Three Dimensions, with which 
the student is supposed to be, to some extent at least, acquainted. 
Such assistance, it will be found, can (as a rule) soon be dispensed 
with; and Hamilton regarded any apparent necessity for an oc 
casional recurrence to it, in higher applications, as an indication 
of imperfect development in the proper methods of the new 
Calculus. 

We commence, therefore, with some very elementary geometrical 
ideas, relating to the theory of vectors in space. It will subsequently 
appear how we are thus led to the notion of a Quaternion. 

{\bf 15}. Suppose we have two points $A$ and $B$ in {\sl space}, and suppose 
$A$ given, on how many numbers does $B$'s relative position depend ? 

If we refer to Cartesian coordinates (rectangular or not) we find 
that the data required are the excesses of $B$'s three coordinates 
over those of $A$. Hence three numbers are required. 

Or we may take polar coordinates. To define the moon's 
position with respect to the earth we must have its Geocentric 
Latitude and Longitude, or its Right Ascension and Declination, 
and, in addition, its distance or radius-vector. {\sl Three} again. 

{\bf 16}. Here it is to be carefully noticed that nothing has been 
said of the {\sl actual} coordinates of either A or B, or of the earth 
and moon, in space; it is only the {\sl relative} coordinates that are 
contemplated. 

Hence any expression, as $\overline{AB}$, denoting a line considered with 
reference to direction and currency as well as length, (whatever 
may be its actual position in space) contains implicitly {\sl three}
numbers, and all lines parallel and equal to $AB$, and concurrent 
with it, depend in the same way upon the same three. Hence, {\sl all 
lines which are equal, parallel, and concurrent, may be represented 
by a common symbol, and that symbol contains three distinct numbers}. 
In this sense a line is called a VECTOR, since by it we pass from 
the one extremity, $A$, to the other, $B$, and it may thus be 
considered as an instrument which {\sl carries} $A$ to $B$: so that a 
vector may be employed to indicate a definite {\sl translation} in space. 

[The term " currency " has been suggested by Cayley for use 
instead of the somewhat vague suggestion sometimes taken to 
be involved in the word "direction." Thus parallel lines have 
the same direction, though they may have similar or opposite 
currencies. The definition of a vector essentially includes its 
currency.] 

{\bf 17}. We may here remark, once for all, that in establishing a 
new Calculus, we are at liberty to give any definitions whatever 
of our symbols, provided that no two of these interfere with, or 
contradict, each other, and in doing so in Quaternions {sl simplicity}
and (so to speak) {\sl naturalness} were the inventor's aim. 

{\bf 18}. Let $\overline{AB}$ be represented by $\alpha$, we know that 
$\alpha$ involves 
{\sl three} separate numbers, and that these depend solely upon the 
position of $B$ {\sl relatively} to $A$. 
Now if $CD$ be equal in length to $AB$ 
and if these lines be parallel, and have the same currency, we may 
evidently write 
$$\overline{CD} = \overline{AB} = \alpha$$
where it will be seen that the sign of equality between vectors 
contains implicitly {\sl equality in length}, {\sl parallelism in direction}, 
and {\sl concurrency}. So far we have {\sl extended} the meaning of an 
algebraical symbol. And it is to be noticed that an equation 
between vectors, as 
$$\alpha = \beta$$
contains {\sl three} distinct equations between mere numbers. 

{\bf 19}. We must now define $+$ (and the meaning of $-$ will follow) 
in the new Calculus. Let $A$, $B$, $C$ be any three points, and (with 
the above meaning of $=$ ) let 
$$\overline{AB} = \alpha, \overline{BC} = \beta, \overline{AC} = \gamma$$
If we define $+$ (in accordance with the idea (\S 16) that a vector 
represents a {\sl translation}) by the equation 
$$
\begin{array}{lcl}
            & \textrm{     } & \alpha + \beta = \gamma\\
            &                & \\
\textrm{or} & \textrm{     } & 
\overline{AB} + \overline{BC} = \overline{AC}
\end{array}
$$
we contradict nothing that precedes, but we at once introduce the 
idea that {\sl vectors are to be compounded}, 
{\sl in direction and magnitude},
{\sl like simultaneous velocities}. A reason for this may be seen in 
another way if we remember that by {\sl adding} the (algebraic) differences 
of the Cartesian coordinates of $B$ and $A$, to those of the 
coordinates of $C$ and $B$, we get those of the coordinates of $C$ and 
$A$. Hence these coordinates enter {\sl linearly} into the expression for 
a vector. (See, again, \S 5.) 

{\bf 20}. But we also see that if $C$ and $A$ coincide (and $C$ may be 
{\sl any} point) 
$$\overline{AC} = 0$$
for no vector is then required to carry $A$ to $C$. Hence the above 
relation may be written, in this case, 
$$\overline{AB}+\overline{BA} = 0$$
or, introducing, and by the same act defining, the symbol $-$, 
$$\overline{AB} = -\overline{BA}$$

Hence, {\sl the symbol $-$, applied to a vector, simply shows that its 
currency is to be reversed}. 
And this is consistent with all that precedes; for instance, 
$$
\begin{array}{lcrcl}
             & \textrm{     } & \overline{AB} + \overline{BC}  &=& \overline{AC}\\
\textrm{and} & \textrm{     } & \overline{AB} = \overline{AC} &-& \overline{BC} \\
\textrm{or}  & \textrm{     } & = \overline{AC} &+& \overline{CB} \\
\end{array}
$$
are evidently but different expressions of the same truth. 

{\bf 21}. In any triangle, $ABC$, we have, of course, 
$$\overline{AB} + \overline{BC} + \overline{CA} = 0$$
and, in any closed polygon, whether plane or gauche, 
$$\overline{AB}+\overline{BC}+\ldots+\overline{YZ}+\overline{ZA} = 0$$ 

In the case of the polygon we have also 
$$\overline{AB}+\overline{BC}+\ldots+\overline{YZ} = \overline{AZ}$$

These are the well-known propositions regarding composition 
of velocities, which, by Newton's second law of motion, give us 
the geometrical laws of composition of forces acting at one point. 

{\bf 22}. If we compound any number of {\bf parallel} vectors, the result
is obviously a numerical multiple of any one of them. 
Thus, if $A$, $B$, $C$ are in one straight line, 
$$\overline{BC} = x\overline{AB}$$
where $x$ is a number, positive when $B$ lies between $A$ and $C$, 
otherwise negative; but such that its numerical value, independent 
of sign, is the ratio of the length of $BC$ to that of $AB$. This is 
at once evident if $AB$ and $BC$ be commensurable; and is easily 
extended to incommensurables by the usual {\sl reductio ad absurdum}. 

{\bf 23}. An important, but almost obvious, proposition is that {\sl any 
vector may be resolved, and in one way only, into three components 
parallel respectively to any three given vectors, no two of which are 
parallel, and which are not parallel to one plane}.

\begin{center}
\includegraphics{ps/quat1.ps}
\end{center}
\vskip 0.5cm

Let $OA$, $OB$, $OC$ be the three fixed 
vectors, $OP$ any other vector. From $P$ draw 
$PQ$ parallel to $CO$, meeting the plane $BOA$ 
in $Q$. [There must be a definite point $Q$, 
else $PQ$, and therefore $CO$, would be parallel 
to $BOA$, a case specially excepted.] From $Q$ 
draw $QR$ parallel to $BO$, meeting $OA$ in $R$. 

Then we have $\overline{OP}=\overline{OR} + \overline{RQ} + \overline{QP}$
(\S 21), 
and these components are respectively parallel to the three given 
vectors. By \S 22 we may express $\overline{OR}$ as a numerical multiple 
of $\overline{OA}$, $\overline{RQ}$ of $\overline{OB}$, and 
$\overline{QP}$ of $\overline{OC}$. Hence we have, generally, for 
any vector in terms of three fixed non-coplanar vectors, $\alpha$,
$\beta$, $\gamma$
$$\overline{OP} = \rho = x\alpha + y\beta + z\gamma$$
which exhibits, in one form, the {\sl three} numbers on which a vector 
depends (\S 16). Here $x$, $y$, $z$ are perfectly definite, and can have 
but single values. 

{\bf 24}. Similarly any vector, as $\overline{OQ}$, in the same plane with 
$\overline{OA}$ and $\overline{OB}$, 
can be resolved (in one way only) into components $\overline{OR}$, 
$\overline{RQ}$, 
parallel respectively to $\overline{OA}$ and 
$\overline{OB}$; so long, at least, as these 
two vectors are not parallel to each other. 

{\bf 25}. There is particular advantage, in certain cases, in employing 
a series of {\sl three mutually perpendicular unit-vectors} as 
lines of reference. This system Hamilton denotes by $i$,$j$, $k$. 

Any other vector is then expressible as 
$$\rho= xi + yj + zk$$
Since $i$, $j$, $k$ are unit-vectors, $x$, $y$, $z$ are here the lengths of 
conterminous edges of a rectangular parallelepiped of which $\rho$
is the vector-diagonal; so that the length of $\rho$ is, in this case, 
$$\sqrt{x^2+y^2+z^2}$$
Let \hbox{\hskip 4cm}$\omega = \xi i + \eta j + \zeta k$\\
be any other vector, then (by the proposition of \S 23) the vector 
$$
\begin{array}{lcr}
\textrm{equation} & \textrm{     } & \rho = \omega
\end{array}
$$
obviously involves the following three equations among numbers, 
$$x=\xi, y=\eta, z=\zeta$$
Suppose $i$ to be drawn eastwards, $j$ northwards, and $k$ upwards, 
this is equivalent merely to saying that {\sl if two points coincide, they 
are equally to the east (or west) of any third point, equally to the 
north (or south) of it, and equally elevated above (or depressed below) 
its level.} 

{\bf 26}. It is to be carefully noticed that it is only when 
$\alpha$, $\beta$, $\gamma$
are not coplanar that a vector equation such as 
$$\rho = \omega$$
or\hbox{\hskip 3cm}
$x\alpha + y\beta + z\gamma = \xi \alpha + \eta \beta + \zeta \gamma$\\
necessitates the three numerical equations 
$$x = \xi, y = \eta, z = \zeta$$
For, if $\alpha$, $\beta$, $\gamma$ be coplanar (\S 24), 
a condition of the following form must hold 
$$\gamma = a\alpha + b\beta$$
Hence,\hbox{\hskip 3cm}$\rho=(x+za)\alpha+(y+zb)\beta$\\
\hbox{\hskip 4cm}$\omega=(\xi+\zeta a)\alpha + (\eta+\zeta b)\beta$\\
and the equation\hbox{\hskip 3cm}$\rho=\omega$\\
now requires only the two numerical conditions 
$$x+za=\xi+\zeta a\hbox{\hskip 1cm}y+zb = \eta+\zeta b$$

{\bf 27}. {\sl The Commutative and Associative Laws hold in the combination 
of vectors by the signs $+$ and $-$}. It is obvious that, if we 
prove this for the sign $+$, it will be equally proved for $-$, because 
$-$ before a vector (\S 20) merely indicates that it is to be reversed 
before being considered positive. 

Let $A$, $B$, $C$, $D$ be, in order, the corners of a parallelogram ; we 
have, obviously, 
$$\overline{AB} = \overline{DC}\hbox{\hskip 1cm}\overline{AD}=\overline{BC}$$
And \hbox{\hskip 2cm}$\overline{AB}+\overline{BC}=
\overline{AC}=
\overline{AD}+\overline{DC}=
\overline{BC}+\overline{AB}$

Hence the commutative law is true for the addition of any two 
vectors, and is therefore generally true. 

Again, whatever four points are represented by $A$, $B$, $C$, $D$, we 
$$\overline{AD}=\overline{AB}+\overline{BD}=\overline{AC}+\overline{CD}$$
or substituting their values for $\overline{AD}$, $\overline{BD}$, 
$\overline{AC}$ respectively, in these three expressions, 
$$\overline{AB}+\overline{BC}+\overline{CD}=
\overline{AB}+(\overline{BC}+\overline{CD})=
(\overline{AB}+\overline{BC})+\overline{CD}$$
And thus the truth of the associative law is evident. 

{\bf 28}. The equation 
$$\rho = x\beta$$
where $\rho$ is the vector connecting a variable point with the origin, 
$\beta$ a definite vector, and $x$ an indefinite number, represents the 
straight line drawn from the origin parallel to $\beta$ (\S 22). 

The straight line drawn from $A$, where $\overline{OA} = \alpha$, 
and parallel to $\beta$, has the equation 
\begin{equation}\label{Vec1}
\rho = \alpha + x\beta
\end{equation}
In words, we may pass directly from $O$ to $P$ by the vector $\overline{OP}$ 
or $\rho$; or we may pass first to $A$, by means of $\overline{OA}$ or 
$\alpha$, and then to $P$ along a vector parallel to $\beta$ (\S 16). 

Equation \ref{Vec1} is one of the many useful forms into which 
Quaternions enable us to throw the general equation of a straight 
line in space. As we have seen (\S 25) it is equivalent to three 
numerical equations; but, as these involve the indefinite quantity 
$x$, they are virtually equivalent to but {\sl two}, as in ordinary Geometry 
of Three Dimensions. 

{\bf 29}. A good illustration of this remark is furnished by the fact 
that the equation 
$$\rho = y\alpha + x\beta$$
which contains two indefinite quantities, is virtually equivalent to
only one numerical equation. And it is easy to see that it represents 
the plane in which the lines $\alpha$ and $\beta$ lie; or the surface 
which is formed by drawing, through every point of $OA$, a line 
parallel to $OB$. In fact, the equation, as written, is simply \S 24 
in symbols. 

And it is evident that the equation 
$$\rho = \gamma + y\alpha + x\beta$$
is the equation of the plane passing through the extremity of $\gamma$, 
and parallel to $\alpha$ and $\beta$.

It will now be obvious to the reader that the equation 
$$\rho = p_1\alpha_1+p_2\alpha_2+\ldots=\sum{p\alpha}$$
where $\alpha_1$, $\alpha_2$ , \&c. are given vectors, 
and $p_1$, $p_2$, \&c. numerical quantities, 
{\sl represents a straight line} 
if $p_1$, $p_2$, \&c. be linear functions of 
{\sl one} indeterminate number; and a {\sl plane}, if they be linear 
expressions containing two indeterminate numbers. Later (\S 31 (l)), 
this theorem will be much extended. 

Again, the equation 
$$\rho = x\alpha + y\beta +z\gamma$$
refers to {\sl any} point whatever in space, provided 
$\alpha$, $\beta$, $\gamma$ are not coplanar. (Ante, \S 23) 

{\bf 30}. The equation of the line joining any two points $A$ and $B$, 
where $\overline{OA} = \alpha$ and $\overline{OB} = \beta$, is obviously 
$$\rho = \alpha + x(\beta-\alpha)$$
or \hbox{\hskip 4.2cm}$\rho=\beta+y(\alpha-\beta)$\\
These equations are of course identical, as may be seen by putting 
$1-y$ for $x$.

The first may be written 
$$\rho+(x-1)\alpha-x\beta = 0$$
or\hbox{\hskip 4cm}$p\rho+q\alpha+r\beta=0$\\
subject to the condition $p + q + r = 0$ identically. That is -- 
A homogeneous linear function of three vectors, equated to zero, 
expresses that the extremities of these vectors are in one straight 
line, {\sl if the sum of the coefficients be identically zero}.

Similarly, the equation of the plane containing the extremities 
$A$, $B$, $C$ of the three non-coplanar vectors 
$\alpha$, $\beta$, $\gamma$ is
$$\rho=\alpha+x(\beta-\alpha)+y(\gamma-\beta)$$
where $x$ and $y$ are each indeterminate. 

This may be written 
$$p\rho + q\alpha + r\beta +s\gamma = 0$$
with the identical relation 
$$p+q+r+x=0$$
which is one form of the condition that four points may lie in one plane. 

{\bf 31}. We have already the means of proving, in a very simple 
manner, numerous classes of propositions in plane and solid 
geometry. A very few examples, however, must suffice at this 
stage; since we have hardly, as yet, crossed the threshold of the 
subject, and are dealing with mere linear equations connecting two 
or more vectors, and even with them {\sl we are restricted as yet to 
operations of mere addition}. We will give these examples with a 
painful minuteness of detail, which the reader will soon find to be 
necessary only for a short time, if at all. 

(a) {\sl The diagonals of a parallelogram bisect each other}.

Let $ABCD$ be the parallelogram, $O$ the point of intersection of 
its diagonals. Then 
$$\overline{AO}+\overline{OB}=\overline{AB}=\overline{DC}=
\overline{DO}+\overline{OC}$$
which gives\hbox{\hskip 2cm}$\overline{AO}-\overline{OC}=
\overline{DO}-\overline{OB}$\\
The two vectors here equated are parallel to the diagonals respectively. 
Such an equation is, of course, absurd unless 
\begin{enumerate}
\item The diagonals are parallel, in which case the figure 
is not a parallelogram; 
\item $\overline{AO} = \overline{OC}$, and 
$\overline{DO} = \overline{OB}$, the proposition. 
\end{enumerate}

(b) {\sl To shew that a triangle can be constructed, whose sides 
are parallel, and equal, to the bisectors of the sides of any 
triangle}. 

Let $ABC$ be any triangle, $Aa$, $Bb$, $Cc$ the bisectors of the 
sides. 

Then 
$$
\begin{array}{ccc}
\overline{Aa} & =\overline{AB}+\overline{Ba} 
                       & =\overline{AB}+\frac{1}{2}\overline{BC}\\
\overline{Bb} & \ldots & = \overline{BC} + \frac{1}{2}\overline{CA}\\
\overline{Cc} & \ldots & = \overline{CA} + \frac{1}{2}\overline{AB}
\end{array}
$$
Hence \hbox{\hskip 2cm}$\overline{Aa}+\overline{Bb}+\overline{Cc}=
\frac{3}{2}(\overline{AB}+\overline{BC}+\overline{CA})=0$\\
which (\S 21) proves the proposition. 

Also 
$$
\begin{array}{rcl}
\overline{Aa} & = & \overline{AB}+\frac{1}{2}\overline{BC}\\
              & = & \overline{AB}-\frac{1}{2}(\overline{CA}+\overline{AB})\\
              & = & \frac{1}{2}(\overline{AB}-\overline{CA})\\
              & = & \frac{1}{2}(\overline{AB}+\overline{AC})
\end{array}
$$
results which are sometimes useful. They may be easily verified 
by producing $\overline{Aa}$ to twice its length and joining the extremity 
with $B$. 

($b^{\prime}$) {\sl The bisectors of the sides of a triangle meet in a point, 
which trisects each of them}.

Taking $A$ as origin, and putting $\alpha$, $\beta$, $\gamma$
for vectors parallel, and 
equal, to the sides taken in order $BC$, $CA$, $AB$; the equation of 
$Bb$ is (\S 28 (1)) 
$$\rho=\gamma+x(\gamma+\frac{\beta}{2})=(1+x)\gamma+\frac{x}{2}\beta$$
That of $Cc$ is, in the same way, 
$$\rho=-(1+y)\beta-\frac{y}{2}\gamma$$
At the point $O$, where $Bb$ and $Cc$ intersect, 
$$\rho=(1+x)\gamma+\frac{x}{2}\beta=-(1+y)\beta-\frac{y}{2}\gamma$$
Since $\gamma$ and $\beta$ are not parallel, this equation gives 
$$1+x=-\frac{y}{2}\textrm{\ \ and\ \ }\frac{x}{2}=-(1+y)$$
From these\hbox{\hskip 3cm}$x=y=-\frac{2}{3}$

Hence\hbox{\hskip 1cm}$\overline{AO}=\frac{1}{3}(\gamma-\beta)=
\frac{2}{3}\overline{Aa}$ (See Ex. (b))\\

This equation shows, being a vector one, that $\overline{Aa}$ passes 
through $O$, and that $AO$ : $Oa$ :: 2:1. 

(c) If 
$$\overline{OA}=\alpha$$
$$\overline{OB}=\beta$$
$$\overline{OC}=l\alpha+m\beta$$
{\sl be three given co-planar vectors, $c$ the intersection of $AB$, $OC$, and 
if the lines indicated in the figure be drawn, the points 
$a_1$,$b_1$,$c_1$ lie in a straight line. }

\begin{center}
\includegraphics{ps/quat2.ps}
\end{center}
\vskip 0.5cm

We see at once, by the process indicated in \S 30, that 
$$\overline{Oc}=\frac{l\alpha+m\beta}{l+m},\hbox{\hskip 1cm}
\overline{Ob}=\frac{l\alpha}{1-m},\hbox{\hskip 1cm}
\overline{Oa}=\frac{m\beta}{1-l}$$
Hence we easily find 
$$\overline{Oa_1}=-\frac{m\beta}{1-l-2m},\hbox{\hskip 0.5cm}
\overline{Ob_1}=-\frac{l\alpha}{1-2l-m},\hbox{\hskip 0.5cm}
\overline{Oc_1}=\frac{-l\alpha+m\beta}{m-l}$$
These give 
$$-(1-l-2m)\overline{Oa_1}+(1-2l-m)\overline{Ob_1}-(m-l)\overline{Oc_1}=0$$
But\hbox{\hskip 1cm}$-(1-l-2m)+(1-2l-m)-(m - l)=0$ identically. 

This, by \S 30, proves the proposition. 

(d) Let $\overline{OA} = \alpha$, 
$\overline{OB} = \beta$, be any two vectors. If $MP$ be a 
given line parallel to $OB$; and $OQ$, $BQ$, be drawn parallel to $AP$, 
$OP$ respectively ; the locus of $Q$ is a straight line parallel to $OA$. 

\begin{center}
\includegraphics{ps/quat3.ps}
\end{center}
\vskip 0.5cm

\noindent
Let \hbox{\hskip 4cm}$\overline{OM}=e\alpha$\\
Then \hbox{\hskip 3cm}$\overline{AP}=\overline{e-1}\alpha+x\beta$

Hence the equation of $OQ$ is 
$$\rho=y(\overline{e-1}\alpha+x\beta)$$
and that of $BQ$ is\hbox{\hskip 1cm}$\rho=\beta+z(e\alpha+x\beta)$\\
At Q we have, therefore, 
$$
\left.
\begin{array}{c}
xy=1+zx\\
y(e-1)=ze
\end{array}
\right\}
$$
These give $xy = e$, and the equation of the locus of $Q$ is 
$$\rho = e\beta+y^{\prime}\alpha$$
i.e. a straight line parallel to $OA$, drawn through $N$ in $OB$ 
produced, so that 
$$ON : OB :: OM : OA$$

COR. If $BQ$ meet $MP$ in $q$, $\overline{Pq} = \beta$; 
and if $AP$ meet $NQ$ in $p$, $\overline{Qp} = \alpha$. 

Also, for the point $R$ we have $\overline{pR} = \overline{AP}$, 
$\overline{QR} = \overline{Bq}$. 

Further, the locus of $R$ is a hyperbola, of which $MP$ and $NQ$ 
are the asymptotes. See, in this connection, \S 31 (k) below. 

Hence, {\sl if from any two points, $A$ and $B$, lines be drawn intercepting 
a given length $Pq$ on a given line $Mq$ ; and if, from $R$ their 
point of intersection, $Rp$ be laid off $= PA$, and $RQ = qB$ ; $Q$ and $p$ 
lie on a fixed straight line, and the length of $Qp$ is constant}. 

(e) {\sl To find the centre of inertia of any system of masses.}
 
If $\overline{OA} = \alpha$, $\overline{OB} = \alpha_1$, 
be the vector sides of any triangle, the 
vector from the vertex dividing the base $AB$ in $C$ so that 
$$BC : CA :: m : m_1$$
is \hbox{\hskip 4cm}$\frac{m\alpha+m_1\alpha_1}{m+m_1}$\\

For $AB$ is $\alpha_1-\alpha$, and therefore $\overline{AC}$ is 
$$\frac{m_1}{m+m_1}(\alpha_1-\alpha)$$

Hence\hbox{\hskip 3cm}$\overline{OC}=\overline{OA}+\overline{AC}$
$$=\alpha+\frac{m_1}{m+m_1}(\alpha_1-\alpha)$$
$$=\frac{m\alpha +m_1\alpha_1}{m+m_1}$$
This expression shows how to find the centre of inertia of two 
masses ; $m$ at the extremity of $\alpha$, $m_1$ at that of $\alpha_1$. 
Introduce $m_2$ at the extremity of $a_2$, 
then the vector of the centre of inertia of the 
three is, by a second application of the formula, 
$$\frac{(m+m_1)(\frac{m\alpha+m_1\alpha_1}{m+m_1})+m_2\alpha_2}
{(m+m_1)+m_2}=\frac{m\alpha+m_1\alpha_1+m_2\alpha_2}{m+m_1+m_2}$$
From this it is clear that, for any number of masses, expressed 
generally by $m$ at the extremity of the vector $\alpha$, the vector of the 
centre of inertia is 
$$\beta=\frac{\sum(m\alpha)}{\sum(m)}$$
This may be written\hbox{\hskip 1cm}$\sum m(\alpha-\beta)=0$\\
Now a $\alpha_1-\beta$ 
is the vector of $m_1$ with respect to the centre of inertia. 
Hence the theorem, {\sl If the vector of each element of a mass, drawn 
from the centre of inertia, be increased in length in proportion to the 
mass of the element, the sum of all these vectors is zero.}

(f) We see at once that the equation 

\begin{center}
\includegraphics{ps/quat4.ps}
\end{center}
\vskip 0.5cm

$$\rho=\alpha t +\frac{\beta t^2}{2}$$
where $t$ is an indeterminate 
number, and $\alpha$, $\beta$ given vectors, 
represents a parabola. 
The origin, $O$, is a point on 
the curve, $\beta$ is parallel to 
the axis, i.e. is the diameter 
$OB$ drawn from the origin, 
and $\alpha$ is $OA$ the tangent at the origin. In the figure 
$$\overline{QP}=\alpha t,\hbox{\hskip 1cm}\overline{OQ}=\frac{\beta t^2}{2}$$

The secant joining the points where $t$ has the values $t$ and $t^{\prime}$ is 
represented by the equation 
$$
\begin{array}{rcl}
\rho&=&\alpha t +\frac{\beta t^2}{2}+
x\left(\alpha t^{\prime}+\frac{\beta t^{'2}}{2}
-\alpha t-\frac{\beta t^2}{2}\right)\hbox{\hskip 1cm}(\S 30)\\
&=&\alpha t+\frac{\beta t^2}{2}+
x(t^{\prime}-t)\left\{\alpha+\beta\frac{t^{\prime}-t}{2}
\right\}
\end{array}
$$
Write $x$ for $x(t^{\prime}-t)$ [which may have any value], then put 
$t^{\prime}=t$, and the equation of the tangent at the point ($t$) is 
$$\rho=\alpha t + \frac{\beta t^2}{2}+x(\alpha+\beta t)$$
In this put $x = -t$, and we have 
$$\rho=-\frac{\beta t^2}{2}$$
or the intercept of the tangent on the diameter is equal in length 
to the abscissa of the point of contact, but has the opposite 
currency. 

Otherwise: the tangent is parallel to the vector $\alpha+\beta t$ or 
$\alpha t + \beta t^2$ or $\frac{\beta t^2}{2}+\alpha t+\frac{\beta t^2}{2}$
or $\overline{OQ}+\overline{OP}$. 
But $\overline{TP}=\overline{TO}+\overline{OP}$,
hence $\overline{TO} = \overline{OQ}$. 

(g) Since the equation of any tangent to the parabola is 
$$\rho=\alpha t + \frac{\beta t^2}{2} + x(\alpha+\beta t)$$
let us find the tangents which can be drawn from a given point. 
Let the vector of the point be 
$$\rho=p\alpha + q\beta\hbox{\hskip 0.5cm}(\S 24)$$
Since the tangent is to pass through this point, we have, as con 
ditions to determine $t$ and $x$, 
$$t+x=p$$
$$\frac{t^2}{2}+xt=q$$
by equating respectively the coefficients of $\alpha$ and $\beta$.

Hence\hbox{\hskip 3.5cm}$t=p \pm \sqrt{p^2-2q}$

Thus, in general, {\sl two} tangents can be drawn from a given point. 
These coincide if $$p^2=2q$$
that is, if the vector of the point from which they are to be drawn 
is $$\rho=p\alpha+q\beta=p\alpha+\frac{p^2}{2}\beta$$
i.e. if the point lies on the parabola. They are imaginary if 
$2q > p^2$, that is, if the point be 
$$\rho=p\alpha+\left(\frac{p^2}{2}+r\right)\beta$$
$r$ being {\sl positive}. Such a point is evidently {\sl within} the curve, 
as at $R$, where $\overline{OQ}=\frac{p^2}{2}\beta$, 
$\overline{QP}=p\alpha$, $\overline{PR} = r\beta$. 

(h) Calling the values of $t$ for the two tangents found in (g) 
$t_1$ and $t_2$ respectively, it is obvious that the vector joining the 
points of contact is 
$$\alpha t_1+\frac{\beta t_1^2}{2}-\alpha t_2 - \frac{\beta t_2^2}{2}$$
which is parallel to\hbox{\hskip 2cm}
$\alpha+\beta\frac{t_1+t_2}{2}$
or, by the values of $t_1$ and $t_2$ in (g), 
$$\alpha+p\beta$$
Its direction, therefore, does not depend on $q$. In words, {\sl If pairs of 
tangents be drawn to a parabola from points of a diameter produced, 
the chords of contact are parallel to the tangent at the vertex of the 
diameter.} This is also proved by a former result, for we must have 
$\overline{OT}$ for each tangent equal to $\overline{QO}$. 

(i) The equation of the chord of contact, for the point whose vector is 
$$\rho=p\alpha+q\beta$$
is thus\hbox{\hskip 3cm}
$\rho=\alpha t_1+\frac{\beta t_1^2}{2}+y(\alpha+p\beta)$

Suppose this to pass always through the point whose vector is 
$$\rho=a\alpha+b\beta$$
Then we must have
$$
\left.
\begin{array}{rcl}
t_1+y & = & a\\
\frac{t_1^2}{2}+py & = & b
\end{array}
\right\}
$$
or\hbox{\hskip 4cm}$t_1=p\pm\sqrt{p^2-2p\alpha+2\beta}$

Comparing this with the expression in (g), we have 
$$
q = pa - b 
$$
that is, the point from which the tangents are drawn has the vector 
a straight line (\S 28 (1)).

The mere form of this expression contains the proof of the usual 
properties of the pole and polar in the parabola ; but, for the sake 
of the beginner, we adopt a simpler, though equally general, process. 

Suppose $\alpha = 0$. This merely restricts the pole to the particular 
diameter to which we have referred the parabola. Then the pole 
is $Q$, where $$\rho = b\beta$$
and the polar is the line $TU$, for which 
$$\rho=-b\beta+p\alpha$$
{\sl Hence the polar of any point is parallel to the tangent at the 
extremity of the diameter on which the point lies, and its intersection 
with that diameter is as far beyond the vertex as the pole 
is within, and vice versa. }

(j) As another example let us prove the following theorem. 
{\sl If a triangle be inscribed in a parabola, the three points in which 
the sides are met by tangents at the angles lie in a straight line. }

Since $O$ is any point of the curve, we may take it as one corner 
of the triangle. Let $t$ and $t_1$ determine the others. Then, if 
$\omega_1$,$\omega_2$,$\omega_3$
represent the vectors of the points of intersection of the 
tangents with the sides, we easily find 
$$
\begin{array}{rcl}
\omega_1 & = & \frac{t_1^2}{2t_1-t}
\left(\alpha+\frac{t}{2}\beta\right)\\
&&\\
\omega_2 & = & \frac{t^2}{2t-t_1}
\left(\alpha+\frac{t_1}{2}\beta\right)\\
&&\\
\omega_3 & = & \frac{tt_1}{t_1+t}\alpha
\end{array}
$$
These values give 
$$\frac{2t_1-t}{t_1}\omega_1 -
\frac{2t-t_1}{t}\omega_2 -
\frac{t_1^2-t^2}{tt_1}\omega_3 = 0$$
Also
$$\frac{2t_1-t}{t_1} -
\frac{2t-t_1}{t} -
\frac{t_1^2-t^2}{tt_1} = 0$$
identically. 

Hence, by \S 30, the proposition is proved. 

(k) Other interesting examples of this method of treating 
curves will, of course, suggest themselves to the student. Thus 
$$\rho = \alpha\cos t + \beta\sin t$$
or
$$\rho=\alpha x + \beta\sqrt{1-x^2}$$
represents an ellipse, of which the given vectors $\alpha$ and $\beta$ 
are semiconjugate diameters. If $t$ represent time, the radius-vector of this 
ellipse traces out equal areas in equal times. [We may anticipate 
so far as to write the following : 
$$2 \textrm{Area} = T\int V \rho d\rho = TV\alpha\beta.\int dt$$
which will be easily understood later.] 

Again, 
$$\rho=\alpha t+\frac{\beta}{t}\textrm{  or  }
\rho=\alpha\tan x + \beta\cot x$$
evidently represents a hyperbola referred to its asymptotes. [If 
$t$ represent time, the sectorial area traced out is proportional to 
$\log t$, taken between proper limits.] 
Thus, also, the equation 
$$\rho = \alpha(t + \sin t)+\beta\cos t$$
in which $\alpha$ and $\beta$ are of equal lengths, and at right angles to one 
another, represents a cycloid. The origin is at the middle point of 
the axis ($2\beta$) of the curve. [It may be added that, if t represent 
{\sl time}, this equation shows the motion of the tracing point, provided 
the generating circle rolls uniformly, revolving at the rate of a 
radian per second.] 

When the lengths of $\alpha$, $\beta$ are not equal, this equation gives the 
cycloid distorted by elongation of its ordinates or abscissae : {\sl not} a 
trochoid. The equation of a trochoid may be written 
$$\rho = \alpha(et + \sin t)+\beta\cos t$$
$e$ being greater or less than 1 as the curve is prolate or curtate. 
The lengths of $\alpha$ and $\beta$ are still taken as equal. 

But, so far as we have yet gone with the explanation of the 
calculus, as we are not prepared to determine the lengths or 
inclinations of vectors, we can investigate only a very limited class of 
the properties of curves, represented by such equations as those 
above written. 

(l) We may now, in extension of the statement in \S 29, make 
the obvious remark that 
$$\rho = \sum p\alpha$$
(where, as in \S 23, the number of vectors, $\alpha$, can always be reduced 
to {\sl three}, at most) is the equation of a curve in space, if the 
numbers $p_1$, $p_2$, \&c.  are functions of one indeterminate. In such 
a case the equation is sometimes written 
$$\rho=\phi(t)$$
But, if $p_1$, $p_2$, \&c. be functions of {\sl two} indeterminates, 
the locus of the extremity of $\rho$ is a {\sl surface}; 
whose equation is sometimes written 
$$\rho = \phi(t,u)$$

[It may not be superfluous to call the reader's attention to the 
fact that, in these equations, $\phi(t)$ or $\phi(t, u)$ 
is necessarily a vector expression, since it is equated to a vector, $\rho$.] 

(m) Thus the equation 
\begin{equation}\label{Quat1}
\rho = \alpha\cos t+\beta\sin t + \gamma t
\end{equation}
belongs to a helix, 
while 
\begin{equation}\label{Quat2}
\rho = \alpha\cos t+\beta\sin t + \gamma u
\end{equation}
represents a cylinder whose generating lines are parallel to $\gamma$,
and 
whose base is the ellipse 
$$\rho=\alpha\cos t + \beta\sin t$$
The helix above lies wholly on this cylinder. 

Contrast with (2) the equation 
$$\rho = u(\alpha\cos t + \beta\sin t + \gamma)\eqno(3)$$
which represents a cone of the second degree
made up, in fact, 
of all lines drawn from the origin to the ellipse 
$$\rho=\alpha\cos t + \beta\sin t + \gamma$$
If, however, we write 
$$\rho = u(\alpha\cos t + \beta\sin t + \gamma t)$$
we form the equation of the transcendental cone whose vertex is 
at the origin, and on which lies the helix (1). 

In general 
$$\rho=u\phi(t)$$
is the cone whose vertex is the origin, and on which lies the curve 
$$\rho=\phi(t)$$
while\hbox{\hskip 4cm}$\rho=\phi(t)+u\alpha$\\
is a cylinder, with generating lines parallel to $\alpha$, standing on the 
same curve as base. 

Again,\hbox{\hskip 3cm}$\rho=p\alpha+q\beta+r\gamma$\\
with a condition of the form 
$$ap^2+bq^2+cr^2=1$$
belongs to a central surface of the second order, of which 
$\alpha$, $\beta$, $\gamma$
are the directions of conjugate diameters. If $a$, $b$, $c$ be all positive, 
the surface is an ellipsoid. 

{\bf 32}. In Example ($f$) above we performed an operation equivalent 
to the differentiation of a vector with reference to a single 
{\sl numerical} variable of which it was given as an explicit function. 
As this process is of very great use, especially in quaternion 
investigations connected with the motion of a particle or point; and as it 
will afford us an opportunity of making a preliminary step towards 
overcoming the novel difficulties which arise in quaternion differentiation; 
we will devote a few sections to a more careful, though 
very elementary, exposition of it. 

{\bf 33}. It is a striking circumstance, when we consider the way 
in which Newton's original methods in the Differential Calculus 
have been decried, to find that Hamilton was {\sl obliged} to employ 
them, and not the more modern forms, in order to overcome the 
characteristic difficulties of quaternion differentiation. Such a thing 
as {\sl a differential coefficient has absolutely no meaning in quaternions}, 
except in those special cases in which we are dealing with degraded 
quaternions, such as numbers, Cartesian coordinates, \&c. But a 
quaternion expression has always a {\sl differential}, which is, simply, 
what Newton called a {\sl fluxion}. 

As with the Laws of Motion, the basis of Dynamics, so with the 
foundations of the Differential Calculus ; we are gradually coming 
to the conclusion that Newton s system is the best after all. 

{\bf 34}. Suppose $\rho$ to be the vector of a curve in space. Then, 
generally, $\rho$ may be expressed as the sum of a number of terms, 
each of which is a multiple of a constant vector by a function of some 
{\sl one} indeterminate; or, as in \S 31 ($l$), 
if $P$ be a point on the curve, 
$$\overline{OP}=\rho=\phi(t)$$

And, similarly, if $Q$ be {\sl any other} point on the curve, 
$$\overline{OQ}=\rho_1=\rho+\delta\rho=\phi(t_1)=\phi(t+\delta t)$$
where $\delta t$ is {\sl any number whatever}. 

The vector-chord $\overline{PQ}$ is therefore, rigorously, 
$$\delta p = \rho_1-\rho = \phi(t+\delta t)-\phi t$$

{\bf 35}. It is obvious that, in the present case, {\sl because the vectors 
involved in $\phi$ are constant, and their numerical multipliers alone vary}, 
the expression $\phi(t+\delta t)$ is, by Taylor's Theorem, equivalent to 
$$\phi(t)+\frac{d\phi(t)}{dt}\delta t+
\frac{d^2\phi(t)}{dt^2}\frac{(\delta t)^2}{1\textrm{ . }2}+\ldots$$

Hence, 
$$\delta \rho=\frac{d\phi(t)}{dt}\delta t+
\frac{d^2\phi(t)}{dt^2}\frac{(\delta t)^2}{1\textrm{ . }2}+\textrm{\&c.}$$
And we are thus entitled to write, when $\delta t$ has been made 
indefinitely small, 
$$\textrm{Limit}\left(
\begin{array}{c}
\delta p\\
\delta t
\end{array}
\right)_{\delta t=0}
=\frac{d\rho}{dt}
=\frac{d\phi(t)}{dt}
=\phi^{\prime}(t)$$

In such a case as this, then, we are permitted to differentiate, 
or to form the differential coefficient of, a vector, according to the 
ordinary rules of the Differential Calculus. But great additional 
insight into the process is gained by applying Newton's method. 

{\bf 36}. Let $\overline{OP}$ be 
$$\rho=\phi(t)$$
and $overline{OQ}_1$
$$\rho_1=\phi(t+dt)$$
where $dt$ is any number whatever. 

\begin{center}
\includegraphics{ps/quat5.ps}
\end{center}
\vskip 0.5cm

The number $t$ may here be taken 
as representing {\sl time}, i.e. we may 
suppose a point to move along the 
curve in such a way that the value 
of $t$ for the vector of the point $P$ of 
the curve denotes the interval which 
has elapsed (since a fixed epoch) when the moving point has 
reached the extremity of that vector. If, then, $dt$ represent any 
interval, finite or not, we see that 
$$\overline{OQ}_1=\phi(t+dt)$$
will be the vector of the point after the additional interval $dt$. 

But this, in general, gives us little or no information as to the 
velocity of the point at $P$. We shall get a better approximation 
by halving the interval $dt$, and finding $Q_2$ , 
where $\overline{OQ}_2 = \phi(t + \frac{1}{2}dt)$,
as the position of the moving point at that time. Here the vector 
virtually described in $\frac{1}{2}dt$ is 
$\overline{PQ}_2$ . To find, on this supposition, 
the vector described in $dt$, we must double 
$\overline{PQ}_2$ , and we find, as a 
second approximation to the vector which the moving point would 
have described in time $dt$, if it had moved for that period in the 
direction and with the velocity it had at $P$, 
$$
\begin{array}{rcl}
\overline{Pq}_2=2\overline{PQ}_2 & = & 2(\overline{OQ}_2-\overline{OP})\\
& = & 2\{\phi(t+\frac{1}{2}dt)-\phi(t)\}
\end{array}
$$
The next approximation gives 
$$
\begin{array}{rcl}
\overline{Pq}_3=3\overline{PQ}_3 & = & 3(\overline{OQ}_3-\overline{OP})\\
& = & 3\{\phi(t+\frac{1}{3}dt)-\phi(t)\}
\end{array}
$$
And so on, each step evidently leading us nearer the sought truth. 
Hence, to find the vector which would have been described in time 
$dt$ had the circumstances of the motion at $P$ remained undisturbed, 
we must find the value of 
$$d\rho=\overline{Pq}=L_{x=\infty}x\left\{\phi\left(t+\frac{1}{x}dt\right)
-\phi(t)\right\}$$

We have seen that in this particular case we may use Taylor's 
Theorem. We have, therefore, 
$$
\begin{array}{rcl}
d\rho & = & L_{x=\infty}x
\left\{
\phi^{\prime}(t)\frac{1}{x}dt+
\phi^{\prime\prime}(t)\frac{1}{x^2}\frac{(dt)^2}{1\textrm{ . }2}+
\textrm{\&c}
\right\}\\
&&\\
& = & \phi^{\prime}(t)dt
\end{array}
$$
And, if we choose, we may now write 
$$\frac{d\rho}{dt}=\phi^{\prime}(t)$$

{\bf 37}. But it is to be most particularly remarked that in the 
whole of this investigation no regard whatever has been paid to 
the magnitude of $dt$. The question which we have now answered 
may be put in the form -- {\sl A point describes a given curve in a given 
manner. At any point of its path its motion suddenly ceases to be 
accelerated. What space will it describe in a definite interval?} As 
Hamilton well observes, this is, for a planet or comet, the case 
of a 'celestial Atwood's machine'. 

{\bf 38}. If we suppose the variable, in terms of which $\rho$ is expressed, 
to be the arc, $s$, of the curve measured from some fixed point, we 
find as before 
$$d\rho = \phi^{\prime}(x)ds$$
From the very nature of the question it is obvious that the length 
of $dp$ must in this case be $ds$, so that $\phi^{\prime}(s)$ 
is necessarily a unit-vector. 
This remark is of importance, as we shall see later; and 
it may therefore be useful to obtain afresh the above result without 
any reference to time or velocity. 

{\bf 39}. Following strictly the process of Newton s VIIth Lemma, 
let us describe on $Pq_2$ an arc similar to $PQ_2$, and so on. Then 
obviously, as the subdivision of $ds$ is carried farther, the new arc 
(whose length is always $ds$) more and more nearly (and without 
limit) coincides with the line which expresses the corresponding 
approximation to $dp$. 

{\bf 40}. As additional examples let us take some well-known 
{\sl plane} curves; and first the hyperbola (\S 31 ($k$))
$$\rho=\alpha t + \frac{\beta}{t}$$
Here
$$d\rho=\left(\alpha-\frac{\beta}{t^2}\right)dt$$
This shows that the tangent is parallel to the vector 
$$\alpha t - \frac{\beta}{t}$$
In words, {\sl if the vector (from the centre) of a point in a hyperbola 
be one diagonal of a parallelogram, two of whose sides coincide with 
the asymptotes, the other diagonal is parallel to the tangent at the 
point, and cuts off a constant area from the space between the 
asymptotes}. (For the sides of this triangular area are $t$ times the 
length of $\alpha$, and $1/t$
times the length of $\beta$, respectively; the angle 
between them being constant.) 

Next, take the cycloid, as in \S 31 ($k$), 
$$\rho=\alpha(t+\sin t)+\beta\cos t$$
We have 
$$d\rho=\{\alpha(1+\cos t)-\beta\sin t\}dt$$
At the vertex 
$$t=0,\hbox{\hskip 0.5cm}
\cos t=1,\hbox{\hskip 0.5cm}
\sin t=0,\hbox{\hskip 0.5cm}
\textrm{ and }d\rho=2\alpha dt$$
At a cusp 
$$t=\pi,\hbox{\hskip 0.5cm}
\cos t=-1,\hbox{\hskip 0.5cm}
\sin t=0,\hbox{\hskip 0.5cm}
\textrm{ and }d\rho = 0$$

This indicates that, at the cusp, the tracing point is (
instantaneously) at rest. To find the direction of the tangent, and the 
form of the curve in the vicinity of the cusp, put $t=\pi+\tau$,
where powers of $\tau$ above the second are omitted. We have 
$$d\rho=\beta\tau dt + \frac{\alpha\tau^2}{2}dt$$
so that, at the cusp, the tangent is parallel to $\beta$. By making the 
same substitution in the expression for $\rho$, we find that the part of 
the curve near the cusp is a semicubical parabola, 
$$\rho=\alpha(\pi+\tau^3/6)-\beta(1-\tau^2/2)$$
or, if the origin be shifted to the cusp ($\rho=\pi\alpha-\beta$),
$$\rho=\alpha\tau^3/6+\beta\tau^2/2$$

{\bf 41}. Let us reverse the first of these questions, and {\sl seek the 
envelope of a line which cuts off from two fixed axes a triangle of 
constant area}. 

If the axes be in the directions of $\alpha$ and $\beta$, the intercepts may 
evidently be written $\alpha t$ and $\frac{\beta}{t}$.
Hence the equation of the line is (\S 30) 
$$\rho=\alpha t + x\left(\frac{\beta}{t}-\alpha t\right)$$

The condition of envelopment is, obviously, (see Chap. IX.) 
$$d\rho = 0$$
This gives 
$0 = \left\{\alpha-x\left(\frac{\beta}{t^2}+\alpha\right)\right\}dt+
\left(\frac{\beta}{t}-\alpha t\right)dx$
%tpdhere -- this should use an asterisk, a number looks like a superscript
\setcounter{footnote}{1}%% use a dagger because number looks like superscript
\footnote{
Here we have opportunity for a remark (very simple indeed, but) 
of the utmost importance. {\sl We are not to equate separately to zero the 
coefficients of dt and dx}; for we must remember that this equation is 
of the form 
$$0=p\alpha + q\beta$$
where $p$ and $q$ are numbers; and that, so long as $\alpha$ and $\beta$ 
are actual and non-parallel vectors, the existence of such an equation 
requires (\S 24)}

%tpdhere this is a cheesy use of \leqno. figure out how to do it right.
\noindent
$$\leqno{\textrm{Hence}}\hbox{\hskip 4cm}(1-x)dt-tdx=0$$
%tpdhere -- this text comes out little. 
$$\leqno{\textrm{and}}\hbox{\hskip 4cm}-\frac{x}{t^2}dt+\frac{dx}{t}=0$$
From these, at once, $x = \frac{1}{2}$, since $dx$ and $dt$ are indeterminate. 
Thus the equation of the envelope is 
$$\begin{array}{rcl}
\rho & = & \alpha t + \frac{1}{2}\left(\frac{\beta}{t}-\alpha t\right)\\
     & = & \frac{1}{2}\left(\alpha t + \frac{\beta}{t}\right)
\end{array}$$
the hyperbola as before; $\alpha$, $\beta$ being portions of its asymptotes. 

{\bf 42}. It may assist the student to a thorough comprehension 
of the above process, if we put it in a slightly different form. 
Thus the equation of the enveloping line may be written 
$$\rho=\alpha t(1-x)+\beta\frac{x}{t}$$
which gives 
$$d\rho = 0 = \alpha d\{t(1-x)\}+\beta d\left(\frac{x}{t}\right)$$
Hence, as $\alpha$ is not parallel to $\beta$, we must have 
$$d\{t(1-x)\}=0,\hbox{\hskip 1cm}d\left(\frac{x}{t}\right)=0$$
and these are, when expanded, the equations we obtained in the 
preceding section. 

{\bf 43}. For farther illustration we give a solution not directly 
employing the differential calculus. The equations of any two of 
the enveloping lines are 
$$\rho=\alpha t + x\left(\frac{\beta}{t}-\alpha t\right)$$
$$\rho=\alpha t_1 + x_1\left(\frac{\beta}{t_1}-\alpha t_1\right)$$
$t$ and $t_1$ being given, while $x$ and $x_1$ are indeterminate. 

At the point of intersection of these lines we have (\S 26), 
$$
\left.
\begin{array}{rcl}
t(1-x) & = & t_1(1-x_1)\\
\frac{x}{t} & = & \frac{x_1}{t_1}
\end{array}
\right\}$$
These give, by eliminating $x_1$
$$t(1-x)=t_1\left(1-\frac{t_1}{t}x\right)$$
$$\leqno{\textrm{or}}\hbox{\hskip 4cm}x=\frac{t}{t_1+t}$$
Hence the vector of the point of intersection is 
$$\rho=\frac{\alpha tt_1+\beta}{t_1+t}$$
and thus, for the ultimate intersections, where $L\frac{t_1}{t}=1$,
$$\rho=\frac{1}{2}\left(\alpha t+\frac{\beta}{t}\right)
\textrm{ as before }$$
COR. If. instead of the {\sl ultimate} intersections, we consider 
the intersections of pairs of these lines related by some law, we 
obtain useful results. Thus let 
$$tt_1 = 1$$
$$\rho=\frac{\alpha+\beta}{t+\frac{1}{t}}$$
or the intersection lies in the diagonal of the parallelogram on 
$\alpha$, $\beta$.

If $t_1=mt$, where $m$ is constant, 
$$\rho=\frac{mt\alpha+\frac{\beta}{t}}{m+1}$$
But we have also $x=\frac{1}{m+1}$

Hence {\sl the locus of a point which divides in a given ratio a line 
cutting off a given area from two fixed axes, is a hyperbola of which 
these axes are the asymptotes}. 

If we take either 
$$tt_1(t+t_1)=\textrm{constant, or }
\frac{t^2t_1^2}{t+t_1}=\textrm{constant}$$
the locus is a parabola; and so on. 

It will be excellent practice for the student, at this stage, to 
work out in detail a number of similar questions relating to the 
envelope of, or the locus of the intersection of selected pairs from, a 
series of lines drawn according to a given law. And the process 
may easily be extended to planes. Thus, for instance, we may 
form the general equation of planes which cut off constant tetrahedra 
from the axes of coordinates. Their envelope is a surface of 
the third degree whose equation may be written 
$$\rho=x\alpha+y\beta+z\gamma$$
$$\leqno{\textrm{where}}\hbox{\hskip 4cm}xyz=\alpha^3$$

Again, find the locus of the point of intersection of three of 
this group of planes, such that 
the first intercepts on $\beta$ and $\gamma$, 
the second on $\gamma$ and $\alpha$, 
the third on $\alpha$ and $\beta$, lengths all equal to one 
another, \&c. But we must not loiter with such simple matters as 
these. 

{\bf 44}. The reader who is fond of Anharmonic Ratios and Trans 
versals will find in the early chapters of Hamilton's {\sl Elements of 
Quaternions} an admirable application of the composition of vectors 
to these subjects. The Theory of Geometrical Nets, in a plane, 
and in space, is there very fully developed; and the method is 
shown to include, as particular cases, the corresponding processes of 
Grassmann's {\sl Ausdehnungslehre} and M\"obius' {\sl Barycentrische Calcul}. 
Some very curious investigations connected with curves and surfaces 
of the second and third degrees are also there founded upon the 
composition of vectors. 

\section{Examples To Chapter 1.}

1. The lines which join, towards the same parts, the extremities 
of two equal and parallel lines are themselves equal and parallel. 
({\sl Euclid}, I. xxxiii.) 

2. Find the vector of the middle point of the line which joins 
the middle points of the diagonals of any quadrilateral, plane or 
gauche, the vectors of the corners being given; and so prove that 
this point is the mean point of the quadrilateral. 

If two opposite sides be divided proportionally, and two new 
quadrilaterals be formed by joining the points of division, the mean 
points of the three quadrilaterals lie in a straight line. 

Show that the mean point may also be found by bisecting the 
line joining the middle points of a pair of opposite sides. 

3. Verify that the property of the coefficients of three vectors 
whose extremities are in a line (\S 30) is not interfered with by 
altering the origin. 

4. If two triangles $ABC$, $abc$, be so situated in space that $Aa$, 
$Bb$, $Cc$ meet in a point, the intersections of $AB$, $ab$, of $BC$, $bc$, 
and of $CA$, $ca$, lie in a straight line. 

5. Prove the converse of 4, i.e. if lines be drawn, one in each 
of two planes, from any three points in the straight line in which 
these planes meet, the two triangles thus formed are sections of a 
common pyramid. 

6. If five quadrilaterals be formed by omitting in succession 
each of the sides of any pentagon, the lines bisecting the diagonals 
of these quadrilaterals meet in a point. (H. Fox Talbot.) 

7. Assuming, as in \S 7, that the operator 
$$\cos\theta + \sqrt{-1}\sin\theta$$
turns any radius of a given circle through an angle $\theta$ in the 
positive direction of rotation, without altering its length, deduce 
the ordinary formulae for $\cos(A+B)$, $\cos(A-B)$, $\sin(A+B)$, and 
$\sin(A-B)$, in terms of sines and cosines of $A$ and $B$. 

8. If two tangents be drawn to a hyperbola, the line joining 
the centre with their point of intersection bisects the lines join 
ing the points where the tangents meet the asymptotes : and the 
secant through the points of contact bisects the intercepts on 
the asymptotes. 

9. Any two tangents, limited by the asymptotes, divide each 
other proportionally. 

10. If a chord of a hyperbola be one diagonal of a parallelogram 
whose sides are parallel to the asymptotes, the other diagonal passes 
through the centre. 

11. Given two points $A$ and $B$, and a plane, $C$. Find the 
locus of $P$, such that if $AP$ cut $C$ in $Q$, and $BP$ cut $C$ in $R$, 
$\overline{QR}$ may be a given vector. 

12. Show that\hbox{\hskip 1cm} $\rho = x^2\alpha+y^2\beta+(x+y)^2\gamma$\\
is the equation of a cone of the second degree, and that its section 
by the plane 
$$\rho=\frac{p\alpha+q\beta+r\gamma}{p+q+r}$$
is an ellipse which touches, at their middle points, the sides of the 
triangle of whose corners $\alpha$, $\beta$, $\gamma$ 
are the vectors. (Hamilton, {\sl Elements}, p. 96.) 

13. The lines which divide, proportionally, the pairs of opposite 
sides of a gauche quadrilateral, are the generating lines of a 
hyperbolic paraboloid. ({\sl Ibid}. p. 97.) 

14. Show that\hbox{\hskip 2cm} $\rho=x^3\alpha+y^3\beta+z^3\gamma$\\
where\hbox{\hskip 4cm} $x+y+z=0$\\
represents a cone of the third order, and that its section by the plane 
$$\rho=\frac{p\alpha+q\beta+r\gamma}{p+q+r}$$
is a cubic curve, of which the lines 
$$\rho=\frac{p\alpha+q\beta}{p+q},\textrm{ \&c}$$
are the asymptotes and the three (real) tangents of inflection. Also 
that the mean point of the triangle formed by these lines is a 
conjugate point of the curve. Hence that the vector $\alpha+\beta+\gamma$
is a conjugate ray of the cone. ({\sl Ibid}. p. 96.) 

\section{Products And Quotients of Vectors}

{\bf 45}. We now come to the consideration of questions in which 
the Calculus of Quaternions differs entirely from any previous 
mathematical method; and here we shall get an idea of what a 
Quaternion is, and whence it derives its name. These questions 
are fundamentally involved in the novel use of the symbols of 
multiplication and division. And the simplest introduction to 
the subject seems to be the consideration of the quotient, or ratio, 
of two vectors. 

{\bf 46}. If the given vectors be parallel to each other, we have 
already seen (\S 22) that either may be expressed as a numerical 
multiple of the other; the multiplier being simply the ratio of 
their lengths, taken positively if they have similar currency, 
negatively if they run opposite ways. 

{\bf 47}. If they be not parallel, let $\overline{OA}$ and 
$\overline{OB}$ be drawn parallel 
and equal to them from any point $O$; and the question is reduced 
to finding the value of the ratio of two vectors drawn from the 
same point. Let us first find {\sl upon how many distinct numbers this 
ratio depends}.

We may suppose $\overline{OA}$ to be changed into 
$\overline{OB}$ by the following successive processes. 

1st. Increase or diminish the length of $\overline{OA}$ till it becomes 
equal to that of $\overline{OB}$. For this only one number is required, viz. 
the ratio of the lengths of the two vectors. As Hamilton remarks, 
this is a positive, or rather a {\sl signless}, number. 

2nd. Turn $\overline{OA}$ about $O$, in the common plane of the two 
vectors, until its direction coincides with that of $\overline{OB}$, and 
(remembering the effect of the first operation) we see that the two vectors 
now coincide or become identical. To specify this operation three 
numbers are required, viz. two angles (such as node and inclination 
in the case of a planet's orbit) to fix the plane in which the rotation 
takes place, and {\sl one} angle for the amount of this rotation. 

Thus it appears that the ratio of two vectors, or the multiplier 
required to change one vector into another, in general depends upon 
{\sl four} distinct numbers, whence the name QUATERNION. 

A quaternion q is thus {\sl defined} as expressing a relation 
$$\beta=q\alpha$$
between two vectors $\alpha$, $\beta$. 
By what precedes, the vectors $\alpha$, $\beta$, 
which serve for the definition of a given quaternion, must be in a 
given plane, at a given inclination to each other, and with their 
lengths in a given ratio ; but it is to be noticed that they may be 
{\sl any} two such vectors. [{\sl Inclination} is understood to include sense, 
or currency, of rotation from $\alpha$ to $\beta$.] 

The particular case of perpendicularity of the two vectors, where 
their quotient is a vector perpendicular to their plane, is fully 
considered below; \S\S 64, 65, 72, \&c. 

{\bf 48}. It is obvious that the operations just described may be 
performed, with the same result, in the opposite order, being perfectly 
independent of each other. Thus it appears that a quaternion, 
considered as the factor or agent which changes one definite vector 
into another, may itself be decomposed into two factors of which 
the order is immaterial. 

The {\sl stretching} factor, or that which performs the first operation 
in \S 47, is called the TENSOR, and is denoted by prefixing $T$ to the 
quaternion considered. 

The {\sl turning factor}, or that corresponding to the second operation 
in \S 47, is called the VERSOR, and is denoted by the letter $U$ prefixed 
to the quaternion. 

{\bf 49}. Thus, if $\overline{OA} = \alpha$, 
$\overline{OB} = \beta$, and if $q$ be the quaternion 
which changes $\alpha$ to $\beta$, we have 
$$\beta = q\alpha$$
which we may write in the form 
$$\frac{\beta}{\alpha} = q\textrm{,  or  }\beta\alpha^{-1}=q$$
if we agree to {\sl define} that 
$$\frac{\beta}{\alpha}\alpha = \beta\alpha^{-1}\alpha = \beta$$
Here it is to be particularly noticed that we write $q$ 
{\sl before} $\alpha$ to 
signify that $\alpha$ is multiplied by (or operated on by) $q$, not $q$ 
multiplied by $\alpha$.

This remark is of extreme importance in quaternions, for, as we 
shall soon see, the Commutative Law does not generally apply to 
the factors of a product. 

We have also, by \S\S 47, 48, 
$$q=TqUq=UqTq$$
where, as before, $Tq$ depends merely on the relative lengths of 
$\alpha$ and $\beta$, and $Uq$ depends solely on their directions. 

Thus, if $\alpha_1$ and $\beta_1$ 
be vectors of unit length parallel to $\alpha$ and $\beta$
respectively, 
$$T\frac{\beta_1}{\alpha_1} = T\beta_1/T\alpha_1 = 1\textrm{,     }
U\frac{\beta_1}{\alpha_1} = U\beta_1/U\alpha_1 = U\frac{\beta}{\alpha}$$
As will soon be shown, when $\alpha$ is perpendicular to $\beta$, 
i.e. when the versor of the quotient is quadrantal, it is a unit-vector. 

{\bf 50}. We must now carefully notice that the quaternion which 
is the quotient when $\beta$ is divided by $\alpha$ in no way depends upon 
the {\sl absolute} lengths, or directions, of these vectors. Its value 
will remain unchanged if we substitute for them any other pair 
of vectors which 

\noindent
\hbox{\hskip 2cm}(1) have their lengths in the same ratio,

\noindent
\hbox{\hskip 2cm}(2) have their common plane the same or parallel,

\noindent
and\hbox{\hskip 1.32cm} (3) make the same angle with each other. 

Thus in the annexed figure 

\begin{center}
\includegraphics{ps/quat6.ps}
\end{center}
\vskip 0.5cm

$$\frac{O_1B_1}{O_1A_1} = \frac{\overline{OB}}{\overline{OA}}$$
if, and only if, 

\noindent
\hbox{\hskip 2cm}$(1)\textrm{     }\frac{O_1B_1}{O_1A_1} = \frac{OB}{OA}$

\noindent
\hbox{\hskip 2cm}$(2)\textrm{     plane }AOB\textrm{ parallel to plane }
A_1O_1B_1$

\noindent
\hbox{\hskip 2cm}$(3)\textrm{     }\angle{}AOB = \angle A_1O_1B_1$

[Equality of angles is understood to include 
concurrency of rotation. Thus in the annexed 
figure the rotation about an axis drawn upwards 
from the plane is negative (or clock- wise) from 
$OA$ to $OB$, and also from $O_1A_1$ to $O_1B_1$.] 

It thus appears that if 
$$\beta = q\alpha\textrm{,  }\delta = q\gamma$$
the vectors $\alpha$, $\beta$, $\gamma$, $\delta$
are parallel to one plane, and may be repre 
sented (in a highly extended sense) as {\sl proportional} to one another, 
thus: --
$$\beta : \alpha = \delta : \gamma$$

And it is clear from the previous part of this section that this 
may be written not only in the form 
$$\alpha : \beta = \gamma : \delta$$
but also in either of the following forms: --
$$\gamma : \alpha = \delta : \beta$$
$$\alpha : \gamma = \beta : \delta$$

While these proportions are true as equalities of ratios, they 
do not usually imply equalities of products. 

Thus, as the first of these was equivalent to the equation 
$$\frac{\beta}{\alpha}=\frac{\delta}{\gamma}=q\textrm{,  or  }
\beta\alpha^{-1}=\delta\gamma^{-1}=q$$
the following three imply separately, (see next section) 
$$\frac{\alpha}{\beta}=\frac{\gamma}{\delta}=q^{-1}\textrm{,   }
\frac{\gamma}{\alpha}=\frac{\delta}{\beta}=r\textrm{,   }
\frac{\alpha}{\gamma}=\frac{\beta}{\delta}=r^{-1}$$
or, if we please, 
$$\alpha\beta^{-1}=\gamma\delta^{-1}=q^{-1}\textrm{,   }
\gamma\alpha^{-1}=\delta\beta^{-1}=r\textrm{,   }
\alpha\gamma^{-1}=\beta\delta^{-1}=r^{-1}$$
where $r$ is a {\sl new} quaternion, which has not necessarily anything 
(except its plane), in common with $q$. 

But here great caution is requisite, for we are {\sl not} entitled to 
conclude from these that 
$$\alpha\delta=\beta\gamma\textrm{, \&c.}$$

This point will be fully discussed at a later stage. Meanwhile 
we may merely {\sl state} that from 
$$\frac{\alpha}{\beta}=\frac{\gamma}{\delta}\textrm{,  or  }
\frac{\beta}{\alpha} = \frac{\delta}{\gamma}$$
we are entitled to deduce a number of equivalents such as 
$$\alpha\beta^{-1}\delta=\gamma\textrm{, or  }
\alpha=\gamma\delta^{-1}\beta\textrm{, or  }
\beta^{-1}\delta=\alpha^{-1}\gamma\textrm{, \&c}$$

{\bf 51}. The {\sl Reciprocal} of a quaternion $q$ is defined by the 
equation 
$$\frac{1}{q}q=q^{-1}=1=q\frac{1}{q}=qqe^{-1}$$
Hence if
$$\frac{\beta}{\alpha}=q\textrm{,  or}$$
$$\beta=q\alpha$$
we must have
$$\frac{\alpha}{\beta}=\frac{1}{q}=q^{-1}$$
For this gives
$$\frac{\alpha}{\beta}\beta=q^{-1}q\alpha$$
and each member of the equation is evidently equal to $\alpha$.
Or thus: --
$$\beta=q\alpha$$
Operate {\sl by} $q^{-1}$
$$q^{-1}\beta = \alpha$$
Operate {\sl on} $\beta^{-1}$
$$q^{-1} = \alpha\beta^{-1} = \frac{\alpha}{\beta}$$

Or, we may reason thus: -- since $q$ changes $\overline{OA}$ to 
$\overline{OA}$, $q^{-1}$ must
change $\overline{OB}$ to $\overline{OA}$, 
and is therefore expressed by $\frac{\alpha}{\beta}$ (\S 49). 

The tensor of the reciprocal of a quaternion is therefore the 
reciprocal of the tensor; and the versor differs merely by the 
{\sl reversal} of its representative angle. The versor, it must be 
remembered, gives the plane and angle of the turning -- it has 
nothing to do with the extension. 

[{\sl Remark}. In \S\S 49--51, above, we had such expressions as 
$\frac{\beta}{\alpha}=\beta\alpha^{-1}$. 
We have also met with $\alpha^{-1}\beta$. Cayley suggests that this 
also may be written in the ordinary fractional form by employing 
the following distinctive notation: --
$$\frac{\beta}{\alpha}=\beta\alpha^{-1}=\frac{\beta|}{|\alpha}\textrm{,   }
\alpha^{-1}\beta=\frac{|\beta}{\alpha|}$$

(It might, perhaps, be even simpler to use the {\sl solidus} as 
recommended by Stokes, along with an obviously correlative 
type:-- thus, 
$$\frac{\beta}{\alpha}=\beta\alpha^{-1}=\beta/\alpha\textrm{,   }
\alpha^{-1}\beta=\alpha\\ \beta$$

I have found such notations occasionally convenient for private 
work, but I hesitate to introduce changes unless they are abso 
lutely required. See remarks on this point towards the end of the 
{\sl Preface to the Second Edition} reprinted above.] 

{\bf 52}. The {\sl Conjugate} of a quaternion $q$, written $Kq$, has the 
same tensor, plane, and angle, only the angle is taken the reverse 
way; or the versor of the conjugate is the reciprocal of the versor 
of the quaternion, or (what comes to the same thing) the versor of 
the reciprocal. 

\begin{center}
\includegraphics{ps/quat7.ps}
\end{center}
\vskip 0.5cm

Thus, if $OA$, $OB$, $OA^{\prime}$ , lie in one plane, and if 
$OA^{\prime} = OA$, and $\angle A^{\prime}OB = \angle BOA$, we have 
$$\frac{\overline{OB}}{\overline{OA}}=q$$, 
and 
$$\frac{\overline{OB}}{\overline{OA^{\prime}}}=\textrm{  congugate of }q=Kq$$

By last section we see that 
$$Kq=(Tq)^2q^{-1}$$
Hence\hbox{\hskip 4cm}$qKq=Kqq=(Tq)^2$

This proposition is obvious, if we recollect that 
the tensors of $q$ and $Kq$ are equal, and that the 
versors are such that either {\sl annuls} the effect of the other; while 
the order of their application is indifferent. The joint effect of 
these factors is therefore merely to multiply twice over by the 
common tensor. 

{\bf 53}. It is evident from the results of \S 50 that, if $\alpha$ and $\beta$
be of equal length, they may be treated as of unit-length so far as 
their quaternion quotient is concerned. This quotient is therefore 
a versor (the tensor being unity) and may be represented indifferently 
by any one of an infinite number of concurrent arcs of 
given length lying on the circumference of a circle, of which the 
two vectors are radii. This is of considerable importance in the 
proofs which follow. 

\begin{center}
\includegraphics{ps/quat8.ps}
\end{center}
\vskip 0.5cm

Thus the versor 
${\displaystyle \frac{\overline{OB}}{\overline{OA}}}$ may be represented 
in magnitude, plane, and currency of rotation (\S 50) 
by the arc $AB$, which may in this extended sense be written 
${\stackrel{\frown}{AB}}$. 

And, similarly, the versor 
${\displaystyle \frac{\overline{OB_1}}{\overline{OA_1}}}$ 
may be represented by 
${\stackrel{\frown}{A_1B_1}}$
which is equal to (and concurrent with) 
${\stackrel{\frown}{AB}}$ if
$$\angle A_1OB_1 = \angle AOB$$
i.e. if the versors are {\sl equal}, in the quaternion meaning of the 
word. 

{\bf 54}. By the aid of this process, when a versor is represented as 
an arc of a great circle on the unit-sphere, we can easily prove 
that {\sl quaternion multiplication is not generally commutative}.

\begin{center}
\includegraphics{ps/quat9.ps}
\end{center}
\vskip 0.5cm

Thus let $q$ be the versor ${\stackrel{\frown}{AB}}$ or
${\displaystyle \frac{\overline{OB}}{\overline{OA}}}$,
where $O$ is the centre of the sphere. 

Take ${\stackrel{\frown}{BC}} = {\stackrel{\frown}{AB}}$, 
(which, it must be remembered, makes the points $A$, $B$, $C$, lie 
in one great circle), then $q$ may also be 
represented by ${\displaystyle \frac{\overline{OC}}{\overline{OB}}}$.

In the same way any other versor $r$ may be represented by 
${\stackrel{\frown}{DB}}$ or ${\stackrel{\frown}{BE}}$ and by 
${\displaystyle \frac{\overline{OB}}{\overline{OD}}}$ or
${\displaystyle \frac{\overline{OE}}{\overline{OB}}}$.

[The line $OB$ in the figure is definite, and is given by the 
intersection of the planes of the two versors.] 

Now $r\overline{OD} = \overline{OB}$, and $q\overline{OB}=\overline{OC}$. 

Hence $qr\overline{OD} = \overline{OC}$, 

or $qr = {\displaystyle \frac{\overline{OC}}{\overline{OD}}}$, 
and may therefore be represented by the arc ${\stackrel{\frown}{DC}}$ of 
a great circle. 

But $rq$ is easily seen to be represented by the arc 
${\stackrel{\frown}{AE}}$. 

For $q\overline{OA}=\overline{OB}$, and $r\overline{OB}=\overline{OE}$, 

whence $rq\overline{OA} = \overline{OE}$. and 
$rq = {\displaystyle \frac{\overline{OE}}{\overline{OA}}}$.

Thus the versors $rq$ and $qr$, though represented by arcs of equal 
length, are not generally in the same plane and are therefore 
unequal: unless the planes of $q$ and $r$ coincide. 

Remark. We see that we have assumed, or defined, in the 
above proof, that $q . r\alpha = qr . \alpha$. 
and $r.q\alpha = rq.\alpha$ in the special case 
when $q\alpha$, $r\alpha$, $q.r\alpha$ and $r.q\alpha$ are all {\sl vectors}. 

{\bf 55}. Obviously ${\stackrel{\frown}{CB}}$ is $Kq$, 
${\stackrel{\frown}{BD}}$ is $Kr$, and 
${\stackrel{\frown}{CD}}$ is $K (qr)$. But 
${\stackrel{\frown}{CD}} = {\stackrel{\frown}{BD}}.{\stackrel{\frown}{CB}}$
as we see by applying both to OC. This gives us 
the very important theorem 
$$K (qr) = Kr . Kq$$
i.e. {\sl the conjugate of the product of two versors is the product of their 
conjugates in inverted order}. This will, of course, be extended to 
any number of factors as soon as we have proved the associative 
property of multiplication. (\S 58 below.) 

{\bf 56}. The propositions just proved are, of course, true of quater 
nions as well as of versors; for the former involve only an additional 
numerical factor which has reference to the length merely, and not 
the direction, of a vector (\S 48), and is therefore commutative with 
all other factors. 

{\bf 57}. Seeing thus that the commutative law does not in general 
hold in the multiplication of quaternions, let us enquire whether 
the Associative Law holds generally. That is if $p$, $q$, $r$ be three 
quaternions, have we 
$$p.qr = pq.r?$$

This is, of course, obviously true if $p$, $q$, $r$ be numerical quantities, 
or even any of the imaginaries of algebra. But it cannot be con 
sidered as a truism for symbols which do not in general give 

$$pq = qp$$

We have assumed it, in definition, for the special case when $r$, 
$qr$, and $pqr$ are all vectors. (\S 54.) But we are not entitled to 
assume any more than is absolutely required to make our 
definitions complete. 

{\bf 58}. In the first place we remark that $p$, $q$, and $r$ may be 
considered as versors only, and therefore represented by arcs of  
great circles on the unit sphere, for their tensors may obviously 
(\S 48) be divided out from both sides, being commutative with the 
versors. 

Let ${\stackrel{\frown}{AB}}=p$,
${\stackrel{\frown}{ED}} = {\stackrel{\frown}{CA}} = q$, and 
${\stackrel{\frown}{FE}} = r$. 

Join $BC$ and produce the great circle till it meets $EF$ in $H$, and 
make ${\stackrel{\frown}{KH}}={\stackrel{\frown}{FE}} = r$, 
and ${\stackrel{\frown}{HG}} = {\stackrel{\frown}{CB}} =pq$ (\S 54).
 
\begin{center}
\includegraphics{ps/quat10.ps}
\end{center}
\vskip 0.5cm

Join $GK$. Then 
${\stackrel{\frown}{KG}} = 
{\stackrel{\frown}{HG}} . {\stackrel{\frown}{KH}} = pq . r$. 

Join $FD$ and produce it to meet $AB$ in $M$. Make 
$${\stackrel{\frown}{LM}} = {\stackrel{\frown}{FD}}
\textrm{,  and  }{\stackrel{\frown}{MN}} = {\stackrel{\frown}{AB}}$$ 

and join $NL$. Then 
$${\stackrel{\frown}{LN}}= {\stackrel{\frown}{MN}} .
{\stackrel{\frown}{LM}}=p.qr$$. 

Hence to show that $p . qr = pq . r$ 

all that is requisite is to prove that $LN$, and $KG$, described as 
above, are {\sl equal arcs of the same great circle}, since, by the figure, 
they have evidently similar currency. This is perhaps most easily 
effected by the help of the fundamental properties of the curves 
known as {\sl Spherical Conics}. As they are not usually familiar to 
students, we make a slight digression for the purpose of proving 
these fundamental properties ; after Chasles, by whom and Magnus 
they were discovered. An independent proof of the associative 
principle will presently be indicated, and in Chapter VIII. we shall 
employ quaternions to give an independent proof of the theorems 
now to be established. 

{\bf 59}.* DEF. {\sl A spherical conic is the curve of intersection of a 
cone of the second degree with a sphere, the vertex of the cone being 
the centre of the sphere}. 

LEMMA. If a cone have one series of circular sections, it has 
another series, and any two circles belonging to different series lie 
on a sphere. This is easily proved as follows. 

Describe a sphere, $A$, cutting the cone in one circular section, 
$C$, and in any other point whatever, and let the side $OpP$ of the 
cone meet $A$ in $p$, $P$ ; $P$ being a point in $C$. Then $PO.Op$ is 
constant, and, therefore, since $P$ lies in a plane, $p$ lies on a sphere, 
$a$, passing through $0$. Hence the locus, $c$, of $p$ is a circle, being 
the intersection of the two spheres $A$ and $a$. 

Let $OqQ$ be any other side of the cone, $q$ and $Q$ being points in 
$c$, $C$ respectively. Then the quadrilateral $qQPp$ is inscribed in a 
circle (that in which its plane cuts the sphere $A$) and the exterior
 
\begin{center}
\includegraphics{ps/quat11.ps}
\end{center}
\vskip 0.5cm

angle at $p$ is equal to the interior angle at $Q$. If $OL$, $OM$ be the 
lines in which the plane $POQ$ cuts the {\sl cyclic planes} (planes through 
$O$ parallel to the two series of circular sections) they are obviously 
parallel to $pq$, $QP$, respectively; and therefore 

$$\angle LOp = \angle Opq = \angle OQP = \angle MOQ$$ 

Let any third side, $OrR$, of the cone be drawn, and let the 
plane $OPR$ cut the cyclic planes in $0l$, $Om$ respectively. Then, 
evidently, 
$$\angle lOL = \angle qpr$$
$$\angle MOm = \angle QPR$$

and these angles are independent of the position of the points $p$ 
and $P$, if $Q$ and $R$ be fixed points. 

\begin{center}
\includegraphics{ps/quat12.ps}
\end{center}
\vskip 0.5cm

In the annexed section of the above space-diagram by a sphere 
whose centre is $O$, $lL$, $Mm$ are the great circles which represent 
the cyclic planes, $PQR$ is the spherical conic which represents the 
cone. The point $P$ represents the line $OpP$, and so with the 
others. The propositions above may now be stated thus, 

$$\textrm{Arc  } PL = \textrm{arc } MQ$$ 

and, if $Q$ and $R$ be fixed, $Mm$ and $lL$ are constant arcs whatever be 
the position of $P$. 

{\bf 60}. The application to \S 58 is now obvious. In the figure of 
that article we have 
$$
{\stackrel{\frown}{FE}}={\stackrel{\frown}{KH}}\textrm{,  }
{\stackrel{\frown}{ED}}={\stackrel{\frown}{CA}}\textrm{,  }
{\stackrel{\frown}{HG}}={\stackrel{\frown}{CB}}\textrm{,  }
{\stackrel{\frown}{LM}}={\stackrel{\frown}{FD}}
$$

Hence $L$, $C$, $G$, $D$ are points of a spherical conic whose cyclic 
planes are those of $AB$, $FE$. Hence also $KG$ passes through $L$, 
and with $LM$ intercepts on $AB$ an arc equal to 
${\stackrel{\frown}{AB}}$. That is, it 
passes through $N$, or $KG$ and $LN$ are arcs of the same great circle : 
and they are equal, for $G$ and $L$ are points in the spherical 
conic. 

Also, the associative principle holds for any number of 
quaternion factors. For, obviously, 

$$qr . st = qrs . t = \textrm{\&c., \&c.,}$$ 

since we may consider $qr$ as a single quaternion, and the above 
proof applies directly. 

{\bf 61}. That quaternion addition, and therefore also subtraction, 
is commutative, it is easy to show. 

\begin{center}
\includegraphics{ps/quat13.ps}
\end{center}
\vskip 0.5cm

For if the planes of two quaternions, 
$q$ and $r$, intersect in the line $OA$, we 
may take any vector $\overline{OA}$ in that line, 
and at once find two others, $\overline{OB}$ and 
$\overline{OC}$, such that 

$$\overline{OB} = q\overline{OA}$$ 
and\hbox{\hskip 4cm}$\overline{CO} = r\overline{OA}$ 

And\hbox{\hskip 2cm}$(q + r)\overline{OA}
\overline{OB}+\overline{OC}=\overline{OC}+\overline{OB}=
(r + q) \overline{OA}$ 

since vector addition is commutative (\S 27). 

Here it is obvious that $(q + r) \overline{OA}$, being the diagonal of the 
parallelogram on $\overline{OB}$, $\overline{OC}$, 
divides the angle between $OB$ and $OC$ 
in a ratio depending solely on the ratio of the lengths of these 
lines, i.e. on the ratio of the tensors of $q$ and $r$. This will be useful 
to us in the proof of the distributive law, to which we proceed. 

{\bf 62}. Quaternion multiplication, and therefore division, is 
distributive. One simple proof of this depends on the possibility, 
shortly to be proved, of representing {\sl any} quaternion as a linear 
function of three given rectangular unit- vectors. And when the 
proposition is thus established, the associative principle may readily 
be deduced from it. 

[But Hamilton seems not to have noticed that we may employ 
for its proof the properties of Spherical Conies already employed 

\begin{center}
\includegraphics{ps/quat14.ps}
\end{center}
\vskip 0.5cm

in demonstrating the truth of the associative principle. "For 
continuity we give an outline of the proof by this process. 

Let ${\stackrel{\frown}{BA}}$, 
${\stackrel{\frown}{CA}}$ 
represent the versors of $q$ and $r$, and be the great 
circle whose plane is that of $p$. 

Then, if we take as operand the vector $\overline{OA}$, it is obvious that 
$U (q + r)$ will be represented by some such arc as 
${\stackrel{\frown}{DA}}$ where 
$B$, $D$, $C$ are in one great circle; 
for $(q + r) \overline{OA}$ is in the same plane 
as $q\overline{OA}$ and $r\overline{OA}$, 
and the relative magnitude of the arcs $BD$ and 
$DC$ depends solely on the tensors of $q$ and $r$. Produce $BA$, $DA$, 
$CA$ to meet be in $b$, $d$, $c$ respectively, and make 

$${\stackrel{\frown}{Eb}} = {\stackrel{\frown}{BA}}, 
{\stackrel{\frown}{Fd}} = {\stackrel{\frown}{DA}}, 
{\stackrel{\frown}{Gc}} = {\stackrel{\frown}{CA}}$$ 

Also make 
${\stackrel{\frown}{b\beta}} = 
{\stackrel{\frown}{d\delta}} = 
{\stackrel{\frown}{c\gamma}}=p$. Then $E$, $F$, $G$, $A$ lie on a spherical 
conic of which $BC$ and $bc$ are the cyclic arcs. And, because 
${\stackrel{\frown}{b\beta}} = 
{\stackrel{\frown}{d\delta}} = 
{\stackrel{\frown}{c\gamma}}$, 
${\stackrel{\frown}{\beta E}}$, 
${\stackrel{\frown}{\delta F}}$, 
${\stackrel{\frown}{\gamma G}}$, when produced, meet in a point $H$ 
which is also on the spherical conic (\S 59*). Let these arcs meet $BC$
in $J$, $L$, $K$ respectively. Then we have 
$${\stackrel{\frown}{JH}}={\stackrel{\frown}{E\beta}}=pUq$$
$${\stackrel{\frown}{LH}}={\stackrel{\frown}{F\delta}}=pU(q+r)$$
$${\stackrel{\frown}{KH}}={\stackrel{\frown}{G\gamma}}=pUr$$
Also\hbox{\hskip 4cm}${\stackrel{\frown}{LJ}}={\stackrel{\frown}{DB}}$\\
and\hbox{\hskip 4cm}${\stackrel{\frown}{KL}}={\stackrel{\frown}{CD}}$

And, on comparing the portions of the figure bounded respectively 
by $HKJ$ and by $ACB$ we see that (when considered with reference 
to their effects as factors multiplying 
$\overline{OH}$ and $\overline{OA}$ respectively) 

\hbox{\hskip 2cm}$pU(q4+r)$ bears the same relation to $pUq$ and $pUr$\\ 
that\hbox{\hskip 1cm}$U(q+r)$ bears to $Uq$ and $Ur$.\\
But\hbox{\hskip 1cm}$T(q+r)U(q+r)=q+r=TqUq+TrUr$. \\
Hence\hbox{\hskip 1cm}$T(q+r).pU(q+r)=Tq .pUq+Tr.pUr$;\\
or, since the tensors are mere numbers and commutative with all 
other factors, 
$$p(q+r)=pq+pr$$
In a similar manner it may be proved that 
$$(q+ )p=qp+rp$$
And then it follows at once that 
$$(p + q) (r + s) = pr + ps + qr + qs$$ 
where, by \S 61, the order of the partial products is immaterial.] 

{\bf 63}. By similar processes to those of \S 53 we see that versors, 
and therefore also quaternions, are subject to the index-law 
$$q^m.q^n=q^{m+n}$$
at least so long as $m$ and $n$ are positive integers. 

The extension of this property to negative and fractional 
exponents must be deferred until we have defined a negative or 
fractional power of a quaternion. 

{\bf 64}. We now proceed to the special case of {\sl quadrantal} versors, 
from whose properties it is easy to deduce all the foregoing 
results of this chapter. It was, in fact, these properties whose 
invention by Hamilton in 1843 led almost intuitively to the 
establishment of the Quaternion Calculus. We shall content 
ourselves at present with an assumption, which will be shown 
to lead to consistent results ; but at the end of the chapter we 
shall show that no other assumption is possible, following for this 
purpose a very curious quasi-metaphysical speculation of Hamilton. 

{\bf 65}. Suppose we have a system of three mutually perpendicular 
unit-vectors, drawn from one point, which we may call for shortness 
{\bf i}, {\bf j}, {\bf k}. 
Suppose also that these are so situated that a positive 
(i.e. {\sl left-handed}) rotation through a right angle about {\bf i} 
as an axis 
brings {\bf j} to coincide with {\bf k}. Then it is obvious that positive 
quadrantal rotation about {\bf j} will make {\bf k} coincide with {\bf i}; 
and, about {\bf k}, will make {\bf i} coincide with {\bf j}. 

For defniteness we may suppose {\bf i} to be drawn {\sl eastwards}, {\bf j} 
{\sl northwards}, and {\bf k} {\sl upwards}. 
Then it is obvious that a positive 
(left-handed) rotation about the eastward line ({\bf i}) brings the northward
line ({\bf j}) into a vertically upward position ({\bf k}) ; and so of the 
others. 

{\bf 66}. Now the operator which turns {\bf j} into {\bf k} is a quadrantal 
versor (\S 53) ; and, as its axis is the vector {\bf i}, 
we may call it {\sl i}. 

Thus $$\frac{{\rm {\bf k}}}{{\rm {\bf j}}}=
i\textrm{, or }{\rm {\bf k}}=i{\rm {\bf j}}\eqno{(1)}$$

Similary we may put$$\frac{{\rm {\bf i}}}{{\rm {\bf k}}}=
j\textrm{, or }{\rm {\bf i}}=j{\rm {\bf k}}\eqno{(2)}$$

and $$\frac{{\rm {\bf j}}}{{\rm {\bf i}}}=
k\textrm{, or }{\rm {\bf j}}=k{\rm {\bf i}}\eqno{(3)}$$

[It may be here noticed, merely to show the symmetry of the 
system we arc explaining, that if the three mutually perpendicular 
vectors {\bf i}, {\bf j}, {\bf k} 
be made to revolve about a line equally inclined to 
all, so that {\bf i} is brought to coincide with {\bf j}, 
{\bf j} will then coincide 
with {\bf k}, and {\bf k} with {\bf i}: 
and the above equations will still hold good, 
only (1) will become (2), (2) will become (3), and (3) will become (1).] 

{\bf 67}. By the results of \S 50 we see that 
$$\frac{-{\rm {\bf j}}}{\rm {\bf k}}=\frac{{\rm {\bf k}}}{\rm {\bf j}}$$
i.e. a southward unit- vector bears the same ratio to an upward 
unit-vector that the latter does to a northward one; and therefore 
we have 

Thus $$\frac{-{\rm {\bf j}}}{{\rm {\bf k}}}=
i\textrm{, or }-{\rm {\bf j}}=i{\rm {\bf k}}\eqno{(4)}$$

Similary t$$\frac{-{\rm {\bf k}}}{{\rm {\bf i}}}=
j\textrm{, or }-{\rm {\bf k}}=j{\rm {\bf i}}\eqno{(5)}$$

and $$\frac{-{\rm {\bf i}}}{{\rm {\bf j}}}=
k\textrm{, or }-{\rm {\bf i}}=k{\rm {\bf j}}\eqno{(6)}$$

{\bf 68}. By (4) and (1) we have 

$$-j = ik = i(ij)\textrm{  (by the assumption in \S 54) }= i^2j$$

Hence 
$$i^2 = - 1\eqno{(7)}$$

Arid in the same way, (5) and (2) give 
$$j^2=-1\eqno{(8)}$$
and (6) and (3)
$$k^2=-1\eqno{(9)}$$

Thus, as the directions of {\bf i}, {\bf j}, {\bf k} 
are perfectly arbitrary, we see that 
{\sl the square of every quadrantal versor is negative unity}. 

[Though the following proof is in principle exactly the same as 
the foregoing, it may perhaps be of use to the student, in showing 
him precisely the nature as well as the simplicity of the step we 
have taken. 

\begin{center}
\includegraphics{ps/quat15.ps}
\end{center}
\vskip 0.5cm

Let $ABA^{\prime}$ be a semicircle, whose centre 
is $0$, and let $OB$ be perpendicular to $AOA^{\prime}$. 

Then ${\displaystyle\frac{\overline{OB}}{\overline{OA^{\prime}}}}=q$ 
suppose, is a quadrantal versor, and is evidently equal to 
${\displaystyle\frac{\overline{OA^{\prime}}}{\overline{OB}}}$ ;

\S\S 50, 53. Hence 

$$q^2=\frac{\overline{OA^{\prime}}}{\overline{OB}}.
\frac{\overline{OB}}{\overline{OA}}=
\frac{\overline{OA^{\prime}}}{\overline{OA}}=-1]$$

{\bf 69}. 
Having thus found that the squares of {\sl i}, {\sl j}, {\sl k} are each 
equal to negative unity ; it only remains that we find the values of 
their products two and two. For, as we shall see, the result is such 
as to show that the value of any other combination whatever of 
{\sl i},{\sl j}, {\sl k} 
(as factors of a product) may be deduced from the values of 
these squares and products. 

Now it is obvious that 
$$\frac{{\rm {\bf k}}}{\rm {\bf -i}}=
\frac{{\rm {\bf i}}}{\rm {\bf k}}=j$$
(i.e. the versor which turns a westward unit-vector into an upward 
one will turn the upward into an eastward unit) ; or 
$${\rm {\bf k}}=j({\rm {\bf -i}}) = -j{\rm {\bf i}}\eqno{(10)}$$

Now let us operate on the two equal vectors in (10) by the 
same versor, {\sl i}, and we have 
$$i{\rm {\bf k}} = i(-j{\rm {\bf i}}) = -j{\rm {\bf i}}$$ 
But by (4) and (3) 
$$i{\rm {\bf k}}={\rm {\bf -j}}=-k{\rm {\bf i}}$$

Comparing these equations, we have 
$$-ij{\rm {\bf i}}=-k{\rm {\bf i}}$$
$$
\left.
\begin{array}{lr}
\textrm{or, \S 54 (end), } &ij=k\\
\textrm{and symmetry gives}&jk=i\\
                           &ki=j\\
\end{array}
\right\}\eqno{(11)}
$$

The meaning of these important equations is very simple ; and 
is, in fact, obvious from our construction in \S 54 for the multiplication 
of versors ; as we see by the annexed figure, where we must 
remember that {\sl i}, {\sl j}, {\sl k} 
are quadrantal versors whose planes are at 
right angles, so that the figure represents 
a hemisphere divided into quadrantal 
triangles. [The arrow-heads indicate the 
direction of each vector arc.] 

\begin{center}
\includegraphics{ps/quat16.ps}
\end{center}
\vskip 0.5cm

Thus, to show that $ij = k$, we have, 
$O$ being the centre of the sphere, $N$, $E$, 
$S$, $W$ the north, east, south, and west, 
and $Z$ the zenith (as in \S 65) ; 

$$j\overline{OW}=\overline{OZ}$$
whence\hbox{\hskip 2cm}$ij\overline{OW}=i\overline{OZ}=
\overline{OS} = k\overline{OW}$ 

* The negative sign, being a mere numerical factor, is evidently commutative 
with $j$  indeed we may, if necessary, easily assure ourselves of the fact 
that to turn the negative (or reverse) of a vector through a right 
(or indeed any) angle, is the same thing as to turn the vector through 
that angle and then reverse it. 

{\bf 70}. But, by the same figure, 
$$i\overline{ON}=\overline{OZ}$$
whence\hbox{\hskip 1cm}$ji\overline{ON} =j\overline{OZ}
= \overline{OE} = -\overline{OW} = - k\overline{ON}$. 

{\bf 71}. From this it appears that 
$$
\left.
\begin{array}{c}
ji = -k\\
kj = -i\\
ik = -j\\
\end{array}   
\right\}\eqno{(12)}
$$
and thus, by comparing (11), 
$$
\left.
\begin{array}{c}
ij=-ji=k\\
jk=-kj=i\\
ki=-ik=j\\
\end{array}
\right\}\eqno{(11),(12)}
$$

These equations, along with 
$$i^2=j^2=k^2=-1\eqno{((7),(8),(9))}$$
contain essentially the whole of Quaternions. But it is easy to see 
that, for the first group, we may substitute the single equation 
$$ijk=-1\eqno{(13)}$$
since from it, by the help of the values of the squares of 
{\sl i}, {\sl j}, {\sl k}, all 
the other expressions may be deduced. We may consider it proved 
in this way, or deduce it afresh from the figure above, thus 
$$k\overline{ON}=\overline{OW}$$
$$jk\overline{ON}=j\overline{OW}=\overline{OZ}$$
$$ijk\overline{ON}=ij\overline{OW}=i\overline{OZ}=
\overline{OS}=-\overline{ON}$$

{\bf 72}. One most important step remains to be made, to wit the 
assumption referred to in \S 64. We have treated 
{\sl i}, {\sl j}, {\sl k} simply as 
quadrantal versors ; and 
{\bf i}, {\bf j}, {\bf k} as unit-vectors at right angles to 
each other, and coinciding with the axes of rotation of these versors. 
But if we collate and compare the equations just proved we have 

\hbox{\hskip 4cm {\Huge \{}
\vbox{
\hbox{$i^2=-1$\hbox{\hskip 5cm}(7)}
\hbox{${\rm {\bf i}}^2=-1$\hbox{\hskip 5cm}(\S 9)}}}

\hbox{\hskip 4cm {\Huge \{}
\vbox{
\hbox{$ij=k$\hbox{\hskip 5.3cm}(11)}
\hbox{$i{\rm {\bf j}}={\rm {\bf k}}$\hbox{\hskip 5.3cm}(1)}}}

\hbox{\hskip 4cm {\Huge \{}
\vbox{
\hbox{$ji=-k$\hbox{\hskip 5cm}(11)}
\hbox{$j{\rm {\bf i}}=-{\rm {\bf k}}$\hbox{\hskip 5cm}(1)}}}

with the other similar groups symmetrically derived from them. 

Now the meanings we have assigned to {\sl i}, {\sl j}, {\sl k} are quite 
independent of, and not inconsistent with, those assigned to 
{\rm {\bf i}}, {\rm {\bf j}}, {\rm {\bf k}}. 
And it is superfluous to use two sets of characters when one will 
suffice. Hence it appears that {\sl i}, {\sl j}, {\sl k} 
may be substituted for {\rm {\bf i}}, {\rm {\bf j}}, {\rm {\bf k}}; 
in other words, {\sl a unit-vector when employed as a factor may be 
considered as a quadrantal versor whose plane is perpendicular to the 
vector}. (Of course it follows that every vector can be treated as the 
product of a number and a quadrantal versor.) This is one of the 
main elements of the singular simplicity of the quaternion calculus. 

{\bf 73}. Thus {\sl the product, and therefore the quotient, of two 
perpendicular vectors is a third vector perpendicular to both}.

Hence the reciprocal (\S 51) of a vector is a vector which has 
the {\sl opposite} direction to that of the vector, arid its length is the 
reciprocal of the length of the vector. 

The conjugate (\S 52) of a vector is simply the vector reversed. 

Hence, by \S 52, if $\alpha$ be a vector 
$$(Ta)^2 = \alpha K\alpha = \alpha ( - \alpha) = -\alpha{}^2$$

{\bf 74}. We may now see that {\sl every versor may be represented by 
a power of a unit-vector}.

For, if $\alpha$ be any vector perpendicular to $i$ (which is 
{\sl any} definite unit-vector), 
$i\alpha = \beta$ is a vector equal in length to $\alpha$,
but perpendicular to both $i$ and $\alpha$
$$
\begin{array}{ccl}
i^2\alpha  & = & -\alpha\\
\i^3\alpha & = & -i\alpha = -\beta\\
\i^4\alpha & = & -i\beta = -i^2\alpha = \alpha
\end{array}
$$
Thus, by successive applications of $i$, $\alpha$. 
is turned round $i$ as an axis 
through successive right angles. Hence it is natural to {\sl define} 
$i^m$ {\sl as 
a versor which turns any vector perpendicular to i through m right 
angles in the positive direction of rotation about i as an axis}. Here 
$m$ may have any real value whatever, whole or fractional, for it is 
easily seen that analogy leads us to interpret a negative value of $m$ 
as corresponding to rotation in the negative direction. 

{\bf 75}. From this again it follows that {\sl any quaternion may be 
expressed as a power of a vector}. For the tensor and versor 
elements of the vector may be so chosen that, when raised to the 
same power, the one may be the tensor and the other the versor 
of the given quaternion. The vector must be, of course, perpen 
dicular to the plane of the quaternion. 

{\bf 76}. And we now see, as an immediate result of the last two 
sections, that the index-law holds with regard to powers of a 
quaternion (\S 63). 

{\bf 77}. So far as we have yet considered it, a quaternion has been 
regarded as the {\sl product} of a tensor and a versor: we are now to 
consider it as a {\sl sum}. The easiest method of so analysing it seems 
to be the following. 

\begin{center}
\includegraphics{ps/quat17.ps}
\end{center}
\vskip 0.5cm

Let ${\displaystyle\frac{\overline{OB}}{\overline{OA}}}$
represent any quaternion. Draw $BC$ perpendicular to $OA$, 
produced if necessary. 

Then, \S 19, $\overline{OB} = \overline{OC} + \overline{CB}$\\ 

But, \S 22, $\overline{OC}=x\overline{OA}$\\ 
where $x$ is a number, whose sign is the same 
as that of the cosine of $\angle AOB$. 

Also, \S 73, since $CB$ is perpendicular to $OA$, 
$$\overline{CB}=\gamma\overline{OA}$$
where $\gamma$ is a vector perpendicular to $OA$ and $CB$, i.e. to the plane 
of the quaternion; and, as the figure is drawn, directed {\sl towards} the 
reader. 

Hence 
$$\frac{\overline{OB}}{\overline{OA}}=
\frac{x\overline{OA}+\gamma\overline{OA}}{\overline{OA}}=
x+\gamma$$

Thus a quaternion, in general, may be decomposed into the sum 
of two parts, one numerical, the other a vector. Hamilton calls 
them the SCALAR, and the VECTOR, and denotes them respectively 
by the letters $S$ and $V$ prefixed to the expression for the 
quaternion. 

{\bf 78}. Hence $q = Sq+ Vq$, and if in the above example 
$$\frac{\overline{OB}}{\overline{OA}}=q$$
then
$$\overline{OB}=\overline{OC}+\overline{CB}=
Sq.\overline{OA}+Vq.\overline{OA}
\setcounter{footnote}{1}%% use a dagger because number looks like superscript
\footnote{
The points are inserted to show that $S$ and $V$ 
apply only to $q$, and not to $q\overline{OA}$.} 
$$

The equation above gives 
$$\overline{OC}=Sq.\overline{OA}$$
$$\overline{CB}=Vq.\overline{OA}$$

{\bf 79}. If, in the last figure, we produce $BC$ to $D$, so as to double 
its length, and join $OD$, we have, by \S 52, 
$$\frac{\overline{OD}}{\overline{OA}}=Kq=SKq+VKq$$
so that\hbox{\hskip 1cm}
$\overline{OD}=\overline{OC}+\overline{CD}=
SKq.\overline{OA}+VKq.\overline{OA}$\\
Hence\hbox{\hskip 3.6cm}$\overline{OC}=SKq.\overline{OA}$\\
and\hbox{\hskip 4cm}$\overline{CD}=VKq.\overline{OA}$\\ 
Comparing this value of $\overline{OC}$ with that in last section, we find 
$$SKq=Sq\eqno{(1)}$$
or {\sl the scalar of the conjugate of a quaternion is equal to the scalar of 
the quaternion}. 

Again, $\overline{CD} = -\overline{CB}$ 
by the figure, and the substitution of their values gives 
$$VKq=-Vq\eqno{(2)}$$
or {\sl the vector of the conjugate of a quaternion is the vector of the 
quaternion reversed}. 

We may remark that the results of this section are simple con 
sequences of the fact that the symbols $S$, $V$, $K$ are commutative
\setcounter{footnote}{1}%% use a dagger because number looks like superscript
\footnote{
It is curious to compare the properties of these quaternion symbols with those 
of the Elective Symbols of Logic, as given in BOOLE'S 
wonderful treatise on the {\sl Laws of Thought}; and to think that the 
same grand science of mathematical analysis, by processes remarkably 
similar to each other, reveals to us truths in the science of position 
far beyond the powers of the geometer, and truths of deductive 
reasoning to which unaided thought could never have led the logician. }. 

Thus\hbox{\hskip 3cm}$SKq = KSq = Sq$, \\
since the conjugate of a number is the number itself; and 
$$VKq=KVq=-Vq (\S 73)$$

Again, it is obvious that, 
$$\sum{Sq}=S\sum{q},\;\;\;\;\sum{Vq}=V\sum{q}$$
and thence\hbox{\hskip 3cm}$\sum{Kq}=K\sum{q}$

{\bf 80}. Since any vector whatever may be represented by 
$$xi+yj+zk$$
where $x$, $y$, $z$ are numbers (or Scalars), 
and $i$, $j$, $k$ may be any three 
non-coplanar vectors, \S\S 23, 25 though they are usually understood 
as representing a rectangular system of unit-vectors and 
since any scalar may be denoted by $w$; we may write, for any 
quaternion $q$, the expression 
$$q=w+xi+yj+zk (\S 78)$$

Here we have the essential dependence on four distinct numbers, 
from which the quaternion derives its name, exhibited in the most 
simple form. 

And now we see at once that an equation such as 
$$q^{\prime}=q$$
where\hbox{\hskip 3cm}$q^{\prime}=
w^{\prime}+x^{\prime}i+y^{\prime}j+z^{\prime}k$\\
involves, of course, the {\sl four} equations 
$$
w^{\prime}=w\textrm{,  }
x^{\prime}=x\textrm{,  }
y^{\prime}=y\textrm{,  }
z^{\prime}=z
$$

{\bf 81}. We proceed to indicate another mode of proof of the distributive 
law of multiplication. 

We have already defined, or assumed (\S 61), that 
$$\frac{\beta}{\alpha}+\frac{\gamma}{\alpha}=\frac{\beta+\gamma}{\alpha}$$
or\hbox{\hskip 3cm}$\beta\alpha^{-1}+\gamma\alpha^{-1}=
(\beta+\gamma)\alpha^{-1}$\\
and have thus been able to understand what is meant by adding 
two quaternions. 

But, writing $\alpha$ for $\alpha^{-1}$, 
we see that this involves the equality 
$$(\beta+\gamma)\alpha = \beta\alpha+\gamma\alpha$$
from which, by taking the conjugates of both sides, we derive 
$$\alpha^{\prime}(\beta^{\prime}+\gamma^{\prime})=
\alpha^{\prime}\beta^{\prime}+\alpha^{\prime}\gamma^{\prime}
(\S 55)$$
And a combination of these results (putting 
$\beta+\gamma$ for $\alpha^{\prime}$ in the latter, for instance) gives 
$$
\begin{array}{lcr}
(\beta+\gamma)(\beta^{\prime}+\gamma^{\prime}) & = &
(\beta+\gamma)\beta^{\prime}+(\beta+\gamma)\gamma^{\prime}\\
& = & \beta\beta^{\prime}+\gamma\beta^{\prime}+
\beta\gamma^{\prime}+\gamma\gamma^{\prime}
\end{array}
$$
by the former.

Hence the {\sl distributive principle is true in the multiplication of 
vectors}.

It only remains to show that it is true as to the scalar and 
vector parts of a quaternion, and then we shall easily attain the 
general proof. 

Now, if $a$ be any scalar, $\alpha$ any vector, and $q$ any quaternion, 
$$(a+\alpha)q=aq+\alpha q$$

For, if $\beta$ be the vector in which the plane of $q$ is intersected by 
a plane perpendicular to $\alpha$, we can find other two vectors, 
$\gamma$ and $\delta$ one in each of these planes such that 
$$\alpha=\frac{\gamma}{\beta},\;\;\;\;\;q=\frac{\beta}{\delta}$$
And, of course, $a$ may be written 
${\displaystyle\frac{a\beta}{\beta}}$; so that 
$$
\begin{array}{ccl}
(a+\alpha)q & = & \frac{a\beta+\gamma}{\beta}.\frac{\beta}{\delta}
=\frac{a\beta+\gamma}{\delta}\\
& & \\
& = & a\frac{\beta}{\delta}+\frac{\gamma}{\delta}=
a\frac{\beta}{\delta}+\frac{\gamma}{\beta}.\frac{\beta}{\delta}\\
& & \\
& = & aq + \alpha q
\end{array}
$$
And the conjugate may be written 
$$q^{\prime}(a^{\prime}+\alpha^{\prime})=
q^{\prime}a^{\prime}+q^{\prime}\alpha^{\prime} (\S 55)$$
Hence, generally, 
$$(a+\alpha)(b+\beta)=ab+a\beta+b\alpha+\alpha\beta$$
or, breaking up $a$ and $b$ each into the sum of two scalars, and 
$\alpha$, $\beta$ each into the sum of two vectors, \\
$(a_1+a_2+\alpha_1+\alpha_2)(b_1+b_2+\beta_1+\beta_2)$
$$=(a_1+a_2)(b_1+b_2)
+(a_1+a_2)(\beta_1+\beta_2)
+(b_1+b_2)(\alpha_1+\alpha_2)
+(\alpha_1+\alpha_2)(\beta_1+\beta_2)
$$
(by what precedes, all the factors on the right are distributive, so 
that we may easily put it in the form) 
$$=(a_1+\alpha_1)(b_1+\beta_1)
+(a_1+\alpha_1)(b_2+\beta_2)
+(a_2+\alpha_2)(b_1+\beta_1)
+(a_2+\alpha_2)(b_2+\beta_2)
$$

Putting $a_1+\alpha_1=p,\;\;\;$ 
$a_2+\alpha_2=q,\;\;\;$ 
$b_1+\beta_1=r,\;\;\;$ $b_2+\beta_2=s$,\\
we have $(p+q)(r+s)=pr+ps+qr+qs$

{\bf 82}. Cayley suggests that the laws of quaternion multiplication 
may be derived more directly from those of vector multiplication, 
supposed to be already established. Thus, let $\alpha$ be the unit vector 
perpendicular to the vector parts of $q$ and of $q^{\prime}$. Then let 
$$\rho=q.\alpha,\;\;\;\sigma=-\alpha .q^{\prime}$$
as is evidently permissible, and we have 
$$p\alpha=q.\alpha\alpha=-q;\;\;\;\alpha\sigma=
-\alpha\alpha.q^{\prime}=q^{\prime}$$
so that\hbox{\hskip 4cm}$-q.q^{\prime}=\rho\alpha.\alpha\sigma=-\rho.\sigma$

The student may easily extend this process. 

For variety, we shall now for a time forsake the geometrical 
mode of proof we have hitherto adopted, and deduce some of our 
next steps from the analytical expression for a quaternion given in 
\S 80, and the properties of a rectangular system of unit-vectors as 
in \S 71. 

We will commence by proving the result of \S 77 anew. 

{\bf 83}. Let 
$$\alpha=xi+yj+zk$$
$$\beta=x^{\prime}i+y^{\prime}j+z^{\prime}k$$
Then, because by \S 71 every product or quotient of $i$, $j$, $k$ is reducible 
to one of them or to a number, we are entitled to assume 
$$q=\frac{\beta}{\alpha}=\omega+\xi i+\eta j +\zeta k$$
where $\omega$, $\xi$, $\eta$, $\zeta$ are numbers. 
This is the proposition of \S 80. 

[Of course, with this expression for a quaternion, there is no 
necessity for a formal proof of such equations as 
$$p + (q+r) = (p + q) + r$$
where the various sums are to be interpreted as in \S 61. 

All such things become obvious in view of the properties of $i$, $j$ ,$k$.] 

{\bf 84}. But it may be interesting to find $\omega$, $\xi$, $\eta$, $\zeta$ 
in terms of $x$, $y$, $z$, $x^{\prime}$, $y^{\prime}$ , $z^{\prime}$ . 

We have 
$$\beta=q\alpha$$
or 
$$x^{\prime}i+y^{\prime}j+z^{\prime}k=(\omega+\xi i+\eta j+\zeta k)(xi+yj+zk)$$
$$=-(\xi x+\eta y+\zeta z)
+(\omega x+\eta z-\zeta y)i
+(\omega y+\zeta x-\xi z)j
+(\omega z+\xi y-\eta x)k
$$
as we easily see by the expressions for the powers and products of 
$i$, $j$, $k$ given in \S 71. But the student must pay particular attention 
to the {\sl order} of the factors, else he is certain to make mistakes. 

This (\S 80) resolves itself into the four equations 
$$
\begin{array}{lllllllll}
0      & = &          &   & \xi x & + & \eta y & + & \zeta z\\
x^{\prime}  & = & \omega x &   &       & + & \eta z & - & \zeta y\\
y^{\prime}  & = & \omega y & - & \xi z &   &        & + & \zeta x\\
z^{\prime}  & = & \omega z & + & \xi y & - & \eta x\\
\end{array}
$$
The three last equations give 
$$xx^{\prime}+yy^{\prime}+zz^{\prime}=\omega(x^2+y^2+z^2)$$
which determines $\omega$. 

Also we have, from the same three, by the help of the first, 
$$\xi x^{\prime}+\eta y^{\prime}+\zeta z^{\prime} = 0$$
which, combined with the first, gives
$$\frac{\xi}{yz^{\prime}-zy^{\prime}}
=\frac{\eta}{zx^{\prime}-xz^{\prime}}
=\frac{\zeta}{xy^{\prime}-yx^{\prime}}
$$
and the common value of these three fractions is then easily seen 
to be 
$$\frac{1}{x^2+y^2+z^2}$$

It is easy enough to interpret these expressions by means of 
ordinary coordinate geometry : but a much simpler process will 
be furnished by quaternions themselves in the next chapter, and, in 
giving it, we shall refer back to this section. 

{\bf 85}. The associative law of multiplication is now to be proved 
by means of the distributive (\S 81). We leave the proof to the 
student. He has merely to multiply together the factors 
$$w+xi+yj+zk,\;\;\;\; 
w+x^{\prime}i+y^{\prime}j+z^{\prime}k,\;\;\;\;\textrm{ and }
w^{\prime\prime} + x^{\prime\prime}i + y^{\prime\prime}j + 
z^{\prime\prime}k$$

as follows : 

First, multiply the third factor by the second, and then multiply 
the product by the first; next, multiply the second factor by the 
first and employ the product to multiply the third: always remembering 
that the multiplier in any product is placed {\sl before} the 
multiplicand. He will find the scalar parts and the coefficients of 
$i$, $j$, $k$, in these products, respectively equal, each to each. 

{\bf 86}. 
With the same expressions for $\alpha$, $\beta$, as in section 83, we 
have 
$$\alpha\beta=(xi+yj+zk)(x^{\prime}i+y^{\prime}j+z^{\prime}k)$$
$$\;\;=-(xx^{\prime}+yy^{\prime}+zz^{\prime})
+(yz^{\prime}-zy^{\prime})i
+(zx^{\prime}-xz^{\prime})j
+(xy^{\prime}-yx^{\prime})k
$$

But we have also 
$$\beta\alpha=
-(xx^{\prime}+yy^{\prime}+zz^{\prime})
-(yz^{\prime}-zy^{\prime})i
-(zx^{\prime}-xz^{\prime})j
-(xy^{\prime}-yx^{\prime})k
$$

The only difference is in the sign of the vector parts. Hence 
$$S\alpha\beta=S\beta\alpha\eqno{(1)}$$
$$V\alpha\beta=-V\beta\alpha\eqno{(2)}$$
$$\alpha\beta+\beta\alpha=2S\alpha\beta\eqno{(3)}$$
$$\alpha\beta-\beta\alpha=2V\alpha\beta\eqno{(4)}$$
$$\alpha\beta=K.\beta\alpha\eqno{(5)}$$

{\bf 87}. If $\alpha=\beta$ we have of course (\S 25) 
$$x=x^{\prime},\;\;\;\;y=y^{\prime},\;\;\;\;z=z^{\prime}$$
and the formulae of last section become 
$$\alpha\beta=\beta\alpha=\alpha^2=-(x^2+y^2+z^2)$$
which was anticipated in \S 73, where we proved the formula 
$$(T\alpha)^2=-\alpha^2$$
and also, to a certain extent, in \S 25. 

{\bf 88}. Now let $q$ and $r$ be any quaternions, then 
$$
\begin{array}{rcl}
S.qr & = & S.(Sq+Vq)(Sr+Vr)\\
 & = & S.(SqSr+Sr.Vq+Sq.Vr+VqVr)\\
 & = & SqSr+S.VqVr
\end{array}
$$
since the two middle terms are vectors. 
Similarly,
$$S.rq=SrSq+S.VrVq$$
Hence, since by (1) of \S 86 we have 
$$S.VqVr=S.VrVq$$
we see that
$$S.qr=S.rq\eqno{(1)}$$
a formula of considerable importance. 

It may easily be extended to any number of quaternions, 
because, $r$ being arbitrary, we may put for it $rs$. Thus we have 
$$
\begin{array}{rcl}
S.qrs & = & S.rsq\\
& = & S.sqr
\end{array}
$$
by a second application of the process. In words, we have the 
theorem {\sl the scalar of the product of any number of given 
quaternions depends only upon the cyclical order in which they are 
arranged}.

{\bf 89}. An important case is that of three factors, each a vector. 
The formula then becomes 
$$S.\alpha\beta\gamma=S.\beta\gamma\alpha=S.\gamma\alpha\beta$$
But 
$$\begin{array}{rcll}
S.\alpha\beta\gamma & = & S\alpha(S\beta\gamma+V\beta\gamma) &\\
 & = & S\alpha V\beta\gamma & \textrm{since }\alpha S\beta\gamma
\textrm{ is a vector}\\
 & = & -S\alpha V\gamma\beta & \textrm{by (2) of \S 86}\\
 & = & -S\alpha(S\gamma\beta+V\gamma\beta) &\\
 & = & -S.\alpha\gamma\beta
\end{array}
$$
Hence {\sl the scalar of the product of three vectors changes sign when 
the cyclical order is altered.}

By the results of \S\S 55, 73, 79 we see that, for any number 
of vectors, we have 
$$K.\alpha\beta\gamma\ldots\phi\chi=
\pm\chi\phi\ldots\gamma\beta\alpha$$
(the positive sign belonging to the product of an even number of 
vectors) so that 
$$S.\alpha\beta\ldots\phi\chi=\pm S.\chi\phi\ldots\beta\alpha$$

Similarly 
$$V.\alpha\beta\ldots\phi\chi=\mp V.\chi\phi\ldots\beta\alpha$$
Thus we may generalize (3) and (4) of \S 86 into 
$$2S.\alpha\beta\ldots\phi\chi=
\alpha\beta\ldots\chi\phi\pm\phi\chi\ldots\beta\alpha$$
$$2V.\alpha\beta\ldots\phi\chi=
\alpha\beta\ldots\chi\phi\mp\phi\chi\ldots\beta\alpha$$
the upper sign still being used when the -number of factors is 
even. 

Other curious propositions connected with this will be given 
later (some, indeed, will be found in the Examples appended to 
this chapter), as we wish to develop the really fundamental 
formulae in as compact a form as possible. 

{\bf 90}. By (4) of \S 86, 
$$2V\beta\gamma=\beta\gamma-\gamma\beta$$
Hence
$$2V.\alpha V\beta\gamma=V.\alpha(\beta\gamma-\gamma\beta)$$
(by multiplying both by $\alpha$, and taking the vector parts of each side) 
$$=V(\alpha\beta\gamma+\beta\alpha\gamma-\beta\alpha\gamma-\alpha\gamma\beta)$$
(by introducing the null term $\beta\alpha\gamma-\beta\alpha\gamma$).

\noindent
That is 
$$2V.\alpha V\beta\gamma=V.(\alpha\beta+\beta\alpha)\gamma
-V(\beta S\alpha\gamma+\beta V\alpha\gamma+S\alpha\gamma .\beta+
V\alpha\gamma .\beta$$
$$=V.(2S\alpha\beta)\gamma-2V\beta S\alpha\gamma$$
(if we notice that $V(V\alpha\gamma .\beta)=-V.\beta V\alpha\gamma$
by (2) of \S 86). 
Hence 
$$V.\alpha V\beta\gamma=\gamma S\alpha\beta-\beta S\gamma\alpha\eqno{(1)}$$
a formula of constant occurrence. 

Adding $\alpha S\beta\gamma$ to both sides, we get another most valuable 
formula 
$$V.\alpha\beta\gamma
=\alpha S\beta\gamma
-\beta S\gamma\alpha
+\gamma S\alpha\beta\eqno{(2)}
$$
and the form of this shows that we may interchange $\gamma$ and $\alpha$
without altering the right-hand member. This gives 
$$V.\alpha\beta\gamma = V.\gamma\beta\alpha$$
a formula which may be greatly extended. (See \S89, above.) 

Another simple mode of establishing (2) is as follows : 
$$
\begin{array}{rcl}
K.\alpha\beta\gamma & = & -\gamma\beta\alpha\\
\therefore 2V.\alpha\beta\gamma & = & 
\alpha\beta\gamma-K.\alpha\beta\gamma\textrm{ (by \S 79(2))}\\
& = & \alpha\beta\gamma + \gamma\beta\alpha\\
& = & \alpha(\beta\gamma+\gamma\beta)
-(\alpha\gamma+\gamma\alpha)\beta
+\gamma(\alpha\beta+\beta\alpha)\\
& = & 2\alpha S\beta\gamma-2\beta S\alpha\gamma+2\gamma S\alpha\beta
\end{array}
$$

{\bf 91}. We have also 
$$VV\alpha\beta V\gamma\delta = -VV\gamma\delta V\alpha\beta\;\;\;\;
\textrm{ by (2) of \S 86}$$
$$=\delta S\gamma V\alpha\beta-\gamma S\delta V\alpha\beta
=\delta S.\alpha\beta\gamma-\gamma S.\alpha\beta\delta$$
$$=-\beta S\alpha V\gamma\delta+\alpha S\beta V\gamma\delta
=-\beta S.\alpha\gamma\delta+\alpha S.\beta\gamma\delta$$
all of these being arrived at by the help of \S 90 (1) and of \S 89; 
and by treating alternately $V\alpha\beta$ and 
$V\gamma\delta$ as {\sl simple} vectors. 

Equating two of these values, we have 
$$\delta S.\alpha\beta\gamma
=\alpha S.\beta\gamma\delta
+\beta S.\gamma\alpha\delta 
+\gamma S.\alpha\beta\delta\eqno{(3)}
$$
a very useful formula, expressing any vector whatever in terms 
of three given vectors. [This, of course, presupposes that
$\alpha$, $\beta$, $\gamma$
are not coplanar, \S 23. In fact, if they be coplanar, the factor 
$S.\alpha\beta\gamma$ vanishes, 
and thus (3) does not give an expression for $\delta$.
This will be shown in \S 101 below.] 

{\bf 92}. That such an expression as (3) is possible we knew already 
by \S 23. For variety we may seek another expression of a similar 
character, by a process which differs entirely from that employed 
in last section. 

$\alpha$, $\beta$, $\gamma$
being any three non-coplanar vectors, we may derive 
from them three others $V\alpha\beta$, $V\beta\gamma$, $V\gamma\alpha$
and, as these will not be 
coplanar, any other vector $\delta$ may be expressed as the sum of the 
three, each multiplied by some scalar. It is required to find this 
expression for $\delta$.

Let 
$$\delta=xV\alpha\beta+yV\beta\gamma+zV\gamma\alpha$$
Then
$$S\gamma\delta=xS.\gamma\alpha\beta =xS.\alpha\beta\gamma$$
the terms in y and z going out, because 
$$S\gamma V\beta\gamma = S.\gamma\beta\gamma=S\beta\gamma^2
=\gamma^2 S\beta=0$$
for $\gamma^2$ is (\S 73) a number. 

Similarly 
$$S\beta\delta=zS.\beta\gamma\alpha=zS.\alpha\beta\gamma$$
and 
$$S\alpha\delta=qS.\alpha\beta\gamma$$
Thus 
$$\delta S.\alpha\beta\gamma=V\alpha\beta S\gamma\delta
+V\beta\gamma S\alpha\delta
+V\gamma\alpha S\beta\delta\eqno{(4)}
$$

{\bf 93}. We conclude the chapter by showing (as promised in \S 64) 
that the assumption that the product of two parallel vectors is 
a number, and the product of two perpendicular vectors a third 
vector perpendicular to both, is not only useful and convenient, 
but absolutely inevitable, if our system is to deal indifferently with 
all directions in space. We abridge Hamilton s reasoning. 

Suppose that there is no direction in space pre-eminent, and 
that the product of two vectors is something which has quantity, 
so as to vary in amount if the factors are changed, and to have its 
sign changed if that of one of them is reversed ; if the vectors be 
parallel, their product cannot be, in whole or in part, a vector 
{\sl inclined} to them, for there is nothing to determine the direction in 
which it must lie. It cannot be a vector {\sl parallel} to them; for by 
changing the signs of both factors the product is unchanged, 
whereas, as the whole system has been reversed, the product 
vector ought to have been reversed. Hence it must be a number. 
Again, the product of two perpendicular vectors cannot be wholly 
or partly a number, because on inverting one of them the sign of 
that number ought to change; but inverting one of them is simply 
equivalent to a rotation through two right angles about the other, 
and (from the symmetry of space) ought to leave the number 
unchanged. Hence the product of two perpendicular vectors must 
be a vector, and a simple extension of the same reasoning shows 
that it must be perpendicular to each of the factors. It is easy to 
carry this farther, but enough has been said to show the character 
of the reasoning. 

\section{Examples To Chapter 2.}

{\bf 1}. It is obvious from the properties of polar triangles that any 
mode of representing versors by the {\sl sides} of a spherical triangle 
must have an equivalent statement in which they are represented 
by {\sl angles} in the polar triangle. 

Show directly that the product of two versors represented 
by two angles of a spherical triangle is a third versor represented 
by the {\sl supplement} of the remaining angle of the triangle ; and 
determine the rule which connects the {\sl directions} in which these 
angles are to be measured. 

{\bf 2}. Hence derive another proof that we have not generally 
$$pq=qp$$

{\bf 3}. Hence show that the proof of the associative principle, 
\S 57, may be made to depend upon the fact that if from any point 
of the sphere tangent arcs be drawn to a spherical conic, and also 
arcs to the foci, the inclination of either tangent arc to one of the 
focal arcs is equal to that of the other tangent arc to the other 
focal arc. 

{\bf 4}. Prove the formulae 
$$2S.\alpha\beta\gamma = \alpha\beta\gamma-\gamma\beta\alpha$$
$$2V.\alpha\beta\gamma = \alpha\beta\gamma+\gamma\beta\alpha$$

{\bf 5}. Show that, whatever odd number of vectors be represented 
by $\alpha$, $\beta$, $\gamma$ \&c., we have always 
$$
V.\alpha\beta\gamma\delta\epsilon=V.\epsilon\delta\gamma\beta\alpha
$$
$$
V.\alpha\beta\gamma\delta\epsilon\zeta\eta
=V.\eta\zeta\epsilon\delta\gamma\beta\alpha,\textrm{ \&c.}
$$

{\bf 6}. Show that 
$$
S.V\alpha\beta V\beta\gamma V\gamma\alpha=-(S.\alpha\beta\gamma)^2
$$
$$
V.V\alpha\beta V\beta\gamma V\gamma\alpha=
V\alpha\beta(\gamma^2S\alpha\beta-S\beta\gamma S\gamma\alpha)+\ldots
$$
and
$$
V(V\alpha\beta V.V\beta\gamma V\gamma\alpha)
=(\beta S\alpha\gamma-\alpha S\beta\gamma)S.\alpha\beta\gamma
$$

{\bf 7}. If $\alpha$, $\beta$, $\gamma$
be any vectors at right angles to each other, show that 
$$
(\alpha^3+\beta^3+\gamma^3)S.\alpha\beta\gamma
=\alpha^4V\beta\gamma
+\beta^4V\gamma\alpha
+\gamma^4V\alpha\beta
$$
$$
(\alpha^{2n-1}+\beta^{2n-1}+\gamma^{2n-1})S.\alpha\beta\gamma
=\alpha^{2n}V\beta\gamma
+\beta^{2n}V\gamma\alpha
+\gamma^{2n}V\alpha\beta
$$

{\bf 8}. If $\alpha$, $\beta$, $\gamma$
be non-coplanar vectors, find the relations among 
the six scalars, $x$, $y$, $z$ and $\xi$, $\eta$, $\zeta$
which are implied in the 
equation 
$$
x\alpha+y\beta+z\gamma
=\xi V\beta\gamma+\eta V\gamma\alpha+\zeta V\alpha\beta
$$

{\bf 9}. If $\alpha$, $\beta$, $\gamma$
be any three non-coplanar vectors, express any 
fourth vector, $\delta$, as a linear function of each of the following sets of 
three derived vectors. 
$$
V.\gamma\alpha\beta,\;\;\;\;V.\alpha\beta\gamma,\;\;\;\;
V.\beta\gamma\alpha
$$
and
$$
V.V\alpha\beta V\beta\gamma V\gamma\alpha,\;\;\;\;
V.V\beta\gamma V\gamma\alpha V\alpha\beta,\;\;\;\;
V.V\gamma\alpha V\alpha\beta V\beta\gamma
$$

{\bf 10}. Eliminate $\rho$ from the equations 
$$
S\alpha\rho=a,\;\;\;\;
S\beta\rho=b,\;\;\;\;
S\gamma\rho=c,\;\;\;\;
S\delta\rho=d
$$
where $\alpha$, $\beta$, $\gamma$, $\delta$
are vectors, and $a$, $b$, $c$, $d$ scalars. 

{\bf 11}. In any quadrilateral, plane or gauche, the sum of the 
squares of the diagonals is double the sum of the squares of the 
lines joining the middle points of opposite sides. 

\section{Interpretations And Transformations} 

{\bf 94}. Among the most useful characteristics of the Calculus of 
Quaternions, the ease of interpreting its formulae geometrically, 
and the extraordinary variety of transformations of which the 
simplest expressions are susceptible, deserve a prominent place. 
We devote this Chapter to some of the more simple of these, 
together with a few of somewhat more complex character but of 
constant occurrence in geometrical and physical investigations. 
Others will appear in every succeeding Chapter. It is here, 
perhaps, that the student is likely to feel most strongly the 
peculiar difficulties of the new Calculus. But on that very account 
he should endeavour to master them, for the variety of forms 
which any one formula may assume, though puzzling to the 
beginner, is of the utmost advantage to the advanced student, not 
alone as aiding him in the solution of complex questions, but 
as affording an invaluable mental discipline. 

{\bf 95}. If we refer again to the figure of \S 77 we see that 
$$OC=OB\cos AOB$$
$$CB=OB \sin AOB$$
Hence if 
$$\overline{AB}=\alpha,\;\;\;\;
\overline{OB}=\beta,\;\;\;\;\textrm{ and }
\angle AOB=\theta
$$
we have
$$OB=T\beta,\;\;\;\;OA=T\alpha$$
$$OC=T\beta\cos\theta,\;\;\;\;CB=T\beta\sin\theta
$$
Hence
$$S\frac{\beta}{\alpha}=
\frac{OC}{OA}=
\frac{T\beta}{T\alpha}\cos\theta
$$
Similarly,
$$
TV\frac{\beta}{\alpha}=\frac{CB}{OA}=\frac{T\beta}{T\alpha}\sin\theta
$$

Hence, if $\eta$ be a unit-vector perpendicular to 
$\alpha$ and $\beta$, and such 
that positive rotation about it, through the angle $\theta$, turns $\alpha$
towards $\beta$ or 
$$
\eta=
\frac{U\overline{CB}}{U\overline{OA}}=
U\frac{\overline{CB}}{\overline{OA}}=
UV\frac{\beta}{\alpha}
$$
we have
$$
V\frac{\beta}{\alpha}=
\frac{T\beta}{T\alpha}\sin\theta .\eta\;\;\;\;\;\textrm{ (See, again, \S 84)}
$$

{\bf 96}. In the same way, or by putting 
$$
\begin{array}{rcl}
\alpha\beta & = & S\alpha\beta+V\alpha\beta\\
 & = & S\beta\alpha - V\beta\alpha\\
 & = & \alpha^2\left(S\frac{\beta}{\alpha}-V\frac{\beta}{\alpha}\right)\\
 & = & T\alpha^2\left(-S\frac{\beta}{\alpha}+V\frac{\beta}{\alpha}\right)
\end{array}
$$
we may show that 
$$S\alpha\beta=-T\alpha T\beta\cos\theta$$
$$TV\alpha\beta = T\alpha T\beta\sin\theta$$
and
$$V\alpha\beta=T\alpha T\beta\sin\theta . \eta$$
where
$$\eta=UV\alpha\beta = U(-V\beta\alpha)=UV\frac{\beta}{\alpha}$$

Thus {\sl the scalar of the product of two vectors is the continued 
product of their tensors and of the cosine of the supplement of the 
contained angle}. 

{\sl The tensor of the vector of the product of two vectors is the con 
tinued product of their tensors and the sine of the contained angle ; 
and the versor of the same is a unit-vector perpendicular to both, 
and such that the rotation about it from the first vector (i. e. the 
multiplier) to the second is left-handed or positive}. 

{\sl Hence also $TV\alpha\beta$ 
is double the area of the triangle two of whose 
sides are $\alpha$, $\beta$.}

{\bf 97}. (a) In any plane triangle $ABC$ we have 
$$\overline{AC}=\overline{AB}+\overline{BC}$$
Hence,
$$
\overline{AC}^2=S.\overline{AC}\overline{AC}=
S.\overline{AC}(\overline{AB}+\overline{BC})
$$

With the usual notation for a plane triangle the interpretation 
of this formula is 
$$b^2 = -bc\cos A-ab\cos C$$
or
$$b=c\cos C+c\cos A$$

(b) Again we have, obviously, 
$$
\begin{array}{rcl}
V.\overline{AB}\;\overline{AC}&=&V.\overline{AB}(\overline{AB}+\overline{BC})\\
&=&V.\overline{AB}\;\overline{BC}
\end{array}
$$
or
$$cb\sin A = ca\sin B$$
whence
$$\frac{\sin A}{a}=\frac{\sin B}{b}=\frac{\sin C}{c}$$

These are truths, but not truisms, as we might have been led 
to fancy from the excessive simplicity of the process employed. 

{\bf 98}. 
From \S 96 it follows that, if $\alpha$ and $\beta$ be both actual (i. e. 
real and non-evanescent) vectors, the equation 
$$S\alpha\beta = 0$$


shows that $\cos\theta=0$, or that 
$\alpha$ is {\sl perpendicular} to $\beta$. And, in fact, 
we know already that the product of two perpendicular vectors is 
a vector. 

Again if 
$$V\alpha\beta=0$$
we must have $\sin\theta=0$, or 
$\alpha$ is {\sl parallel} to $\beta$. We know already 
that the product of two parallel vectors is a scalar. 

Hence we see that 
$$S\alpha\beta=0$$
is equivalent to
$$\alpha=V\gamma\beta$$
where $\gamma$ is an undetermined vector; and that 
$$V\alpha\beta=0$$
is equivalent to
$$\alpha=x\beta$$
where $x$ is an undetermined scalar. 

{\bf 99}. If we write, as in \S\S 83, 84, 
$$\alpha=ix+jy+kz$$
$$\beta=ix^{\prime}+jy^{\prime}+kz^{\prime}$$
we have, at once, by \S 86, 
$$\begin{array}{rcl}
S\alpha\beta&=&-xx^{\prime}-yy^{\prime}-zz^{\prime}\\
&=&-rr^{\prime}\left(
\frac{x}{r}\frac{x^{\prime}}{r^{\prime}}+
\frac{y}{r}\frac{y^{\prime}}{r^{\prime}}+
\frac{z}{r}\frac{z^{\prime}}{r^{\prime}}
\right)
\end{array}
$$
where
$$
r=\sqrt{x^2+y^2+z^2},\;\;\;\;
r^{\prime}=\sqrt{x^{'2}+y^{'2}+z^{'2}}
$$
Also
$$
V\alpha\beta=rr^{\prime}\left\{
\frac{yz^{\prime}-zy^{\prime}}{rr^{\prime}}i+
\frac{zx^{\prime}-xz^{\prime}}{rr^{\prime}}j+
\frac{xy^{\prime}=yx^{\prime}}{rr^{\prime}}k
\right\}
$$

These express in Cartesian coordinates the propositions we have 
just proved. In commencing the subject it may perhaps assist 
the student to see these more familiar forms for the quaternion 
expressions ; and he will doubtless be induced by their appearance 
to prosecute the subject, since he cannot fail even at this stage to 
see how much more simple the quaternion expressions are than 
those to which he has been accustomed. 

{\bf 100}. The expression
$$S.\alpha\beta\gamma$$
may be written 
$$SV(\alpha\beta)\gamma$$
because the quaternion $\alpha\beta\gamma$ may be broken up into 
$$S(\alpha\beta)\gamma+V(\alpha\beta)\gamma$$
of which the first term is a vector. 

But, by \S 96, 
$$SV(\alpha\beta)\gamma=T\alpha T\beta\sin\theta S\eta\gamma$$
Here $T\eta=1$, let $\phi$ be the angle between $\eta$ and $\gamma$, 
then finally 
$$S.\alpha\beta\gamma = -T\alpha T\beta T\gamma\sin\theta\cos\phi$$

But as $\eta$ is perpendicular to $\alpha$ and $\beta$, 
$T\gamma\cos\phi$ is the length of the 
perpendicular from the extremity of $\gamma$ 
upon the plane of $\alpha$, $\beta$. And 
as the product of the other three factors is (\S 96) the area of the 
parallelogram two of whose sides are $\alpha$, $\beta$, we see that the 
magnitude of $S.\alpha\beta\gamma$, 
independent of its sign, is {\sl the volume of the 
parallelepiped of which three coordinate edges 
are $\alpha$, $\beta$, $\gamma$};
or six times the volume of the pyramid which has 
$\alpha$, $\beta$, $\gamma$ for edges. 

{\bf 101}. Hence the equation 
$$S.\alpha\beta\gamma=0$$
if we suppose $\alpha\beta\gamma$ to be actual vectors, shows either that 
$$\sin\theta=0$$
or
$$\cos\phi=0$$
i. e. {\sl two of the three vectors are parallel}, 
or {\sl all three are parallel to one plane}. 

This is consistent with previous results, for if $\gamma=p\beta$ we have 
$$S.\alpha\beta\gamma=pS.\alpha\beta^2=0$$
and, if $\gamma$ be coplanar with $\alpha$,$\beta$, we have 
$\gamma=p\alpha+q\beta$ and
$$S.\alpha\beta\gamma=S.\alpha\beta(p\alpha+q\beta)=0$$

{\bf 102}. 
This property of the expression $S.\alpha\beta\gamma$ prepares us to 
find that it is a determinant. And, in fact, if we take $\alpha$,$\beta$ as in 
\S 83, and in addition 
$$\gamma=ix^{\prime\prime}+jy^{\prime\prime}+kz^{\prime\prime}$$
we have at once 
$$S.\alpha\beta\gamma=-x^{\prime\prime}(yz^{\prime}-zy^{\prime})-
y^{\prime\prime}(zx^{\prime}-xz^{\prime})-
z^{\prime\prime}(xy^{\prime}-yx^{\prime})$$
$$
=-\left\vert
\begin{array}{ccc}
x & y & z\\
x^{\prime} & y^{\prime} & z^{\prime}\\
x^{\prime\prime}&y^{\prime\prime}&z^{\prime\prime}
\end{array}
\right\vert
$$
The determinant changes sign if we make any two rows change 
places. This is the proposition we met with before (\S 89) in the 
form 
$$S.\alpha\beta\gamma=-S.\beta\alpha\gamma=S.\beta\gamma\alpha
\textrm{, \&c}$$

If we take three new vectors 
$$\alpha_1=ix+jx^{\prime}+kx^{\prime\prime}$$
$$\beta_1 =iy+jy^{\prime}+ky^{\prime\prime}$$
$$\gamma_1=iz+jz^{\prime}+kz^{\prime\prime}$$
we thus see that they are coplanar if $\alpha$, $\beta$, $\gamma$ are so. 
That is, if 
$$S.\alpha\beta\gamma=0$$
then
$$S.\alpha_1\beta_1\gamma_1=0$$

{\bf 103}. We have, by \S 52, 
$$
\begin{array}{rcl}
(Tq)^2 &=&qKq = (Sq+Vq)(Sq-Vq)\;\;\;\;\textrm{(\S 79)}\\
&=&(Sq)^2-(Vq)^2\;\;\;\;\;\textrm{by algebra}\\
&=&(Sq)^2+(TVq)^2\;\;\;\;\textrm{(\S 73)}\\
\end{array}
$$
If $q=\alpha\beta$, we have $Kq = \beta\alpha$, and the formula becomes 
$$\alpha\beta . \beta\alpha = \alpha^2\beta^2=
(S\alpha\beta)^2-(V\alpha\beta)^2
$$

In Cartesian coordinates this is\\
\vskip 0.1cm
$(x^2+y^2+z^z)(x^{'2}+y^{'2}+z^{'2})$
$$
=(xx^{\prime}+yy^{\prime}+zz^{\prime})^2+(yz^{\prime}-zy^{\prime})^2+
(zx^{\prime}-xz^{\prime})^2+(xy^{\prime}-yx^{\prime})^2
$$
More generally we have 
$$
\begin{array}{rcl}
(T(qr))^2&=&(Tq)^2(Tr)^2\\
&=&(S.qr)^2-(V.qr)^2
\end{array}
$$
If we write 
$$q=w+\alpha=w+ix+jy+kz$$
$$r=w^{\prime}+\beta=w^{\prime}+ix^{\prime}+jy^{\prime}+kz^{\prime}$$
this becomes 
$$(w^2+x^2+y^2+z^2)(w^{'2}+x^{'2}+y^{'2}+z^{'2})$$
$$=(ww^{\prime}-xx^{\prime}-yy^{\prime}-zz^{\prime})^2+
(wx^{\prime}+w^{\prime}x+yz^{\prime}-zy^{\prime})^2$$
$$=(xy^{\prime}+w^{\prime}y+zx^{\prime}-xz^{\prime})^2+
(wz^{\prime}+w^{\prime}z+xy^{\prime}-yx^{\prime})^2$$
a formula of algebra due to Euler. 

{\bf 104}. We have, of course, by multiplication, 
$$
(\alpha+\beta)^2=
\alpha^2+\alpha\beta+\beta\alpha+\beta^2=
\alpha^2+2S\alpha\beta+\beta^2\;\;\;\;\;\textrm{(\S 86 (3))}
$$
Translating into the usual notation of plane trigonometry, this 
becomes 
$$c^2=a^2-2ab\cos C+b^2$$
the common formula. 

Again,
$$
V.(\alpha+\beta)(\alpha-\beta)=
-V\alpha\beta+V\beta\alpha=
-2V\alpha\beta\;\;\;\;\;\textrm{(\S 86 (2)}
$$
Taking tensors of both sides we have the theorem, {\sl the parallelogram 
whose sides are parallel and equal to the diagonals of a 
given parallelogram, has double its area} (\S 96). 

Also 
$$S(\alpha+\beta)(\alpha-\beta)=\alpha^2-\beta^2$$
and vanishes only when $\alpha^2=\beta^2$, 
or $T\alpha=T\beta$; that is, {\sl the diagonals 
of a parallelogram are at right angles to one another, when, and 
only when, it is a rhombus}. 

Later it will be shown that this contains a proof that the angle 
in a semicircle is a right angle. 

{\bf 105}. The expression\hbox{\hskip 1cm}$\rho=\alpha\beta\alpha^{-1}$\\
obviously denotes a vector whose tensor is equal to that of $\beta$. 

But we have\hbox{\hskip 2cm}$S.\beta\alpha\rho=0$\\
so that $\rho$ is in the plane of $\alpha$, $\beta$

Also we have\hbox{\hskip 2cm}$S\alpha\rho=S\alpha\beta$\\
so that $\beta$ and $\rho$ make equal angles with $\alpha$, 
evidently on opposite 
sides of it. Thus if $\alpha$ be the perpendicular to a reflecting surface 
and $\beta$ the path of an incident ray, $-\rho$ will be the path of the 
reflected ray. 

Another mode of obtaining these results is to expand the above 
expression, thus, \S 90 (2), 
$$
\begin{array}{rcl}
\rho&=&2\alpha^{-1}S\alpha\beta-\beta\\
&=&2\alpha^{-1}S\alpha\beta-\alpha^{-1}(S\alpha\beta+V\alpha\beta)\\
&=&\alpha^{-1}(S\alpha\beta-V\alpha\beta)
\end{array}
$$
so that in the figure of \S 77 we see that if $\overline{OA}=\alpha$,
and $\overline{OB}=\beta$, we
have $\overline{OD} = \rho = \alpha\beta\alpha^{-1}$

Or, again, we may get the result at once by transforming the 
equation to $\frac{\rho}{\alpha}=K(\alpha^{-1}\rho)=K\frac{\beta}{\alpha}$

{\bf 106}. For any three coplanar vectors the expression 
$$\rho=\alpha\beta\gamma$$
is (\S 101) a vector. It is interesting to determine what this vector 
is. The reader will easily see that if a circle be described about 
the triangle, two of whose sides are (in order) $\alpha$ and $\beta$, 
and if from 
the extremity of $\beta$ a line parallel to $\gamma$ be drawn, 
again cutting the 
circle, the vector joining the point of intersection with the origin 
of $\alpha$ is the direction of the vector $\alpha\beta\gamma$. 
For we may write it in the form 
$$
\rho=\alpha\beta^2\beta^{-1}\gamma=
-(T\beta)^2\alpha\beta^{-1}\gamma=
-(T\beta)^2\frac{\alpha}{\beta}\gamma
$$
which shows that the versor $\displaystyle\left(\frac{\alpha}{\beta}\right)$
which turns $\beta$ into a direction 
parallel to $\alpha$, turns $\gamma$ into a direction parallel to $\rho$. 
And this expresses the long-known property of opposite angles of a 
quadrilateral inscribed in a circle. 

Hence if $\alpha$, $\beta$, $\gamma$ 
be the sides of a triangle taken in order, the 
tangents to the circumscribing circle at the angles of the triangle 
are parallel respectively to 
$$
\alpha\beta\gamma,\;\;\;\;
\beta\gamma\alpha,\;\;\;\;\textrm{ and }
\gamma\alpha\beta
$$

Suppose two of these to be parallel, i. e. let 
$$\alpha\beta\gamma=x\beta\gamma\alpha=x\alpha\gamma\beta\;\;\;\;(\S 90)$$
since the expression is a vector. Hence 
$$\beta\gamma=x\gamma\beta$$
which requires either 
$$x=1,\;\;\;\;V\gamma\beta=0\;\;\;\;\textrm{ or }\gamma \vert\vert \beta$$
a case not contemplated in the problem; or 
$$x=-1,\;\;\;\;S\beta\gamma=0$$
i. e. the triangle is right-angled. And geometry shows us at once 
that this is correct. 

Again, if the triangle be isosceles, the tangent at the vertex is 
parallel to the base. Here we have 
$$x\beta=\alpha\beta\gamma$$
or
$$x(\alpha+\gamma)=\alpha(\alpha+\gamma)\gamma$$
whence $x=\gamma^2=\alpha^2$, or $T\gamma=T\alpha$, as required. 

As an elegant extension of this proposition the reader may 
prove that the vector of the continued product $\alpha\beta\gamma\delta$ 
of the vectorsides of any quadrilateral inscribed in a sphere 
is parallel to the radius drawn to the corner ($\alpha$, $\delta$). 
[For, if $\epsilon$ be the vector from $\delta$,
$\alpha$ to $\beta$, $\gamma$, $\alpha\beta\epsilon$ and 
$\epsilon\gamma\delta$ are (by what precedes) vectors {\sl touching} the 
sphere at $\alpha$, $\delta$. And their product (whose vector part must be 
parallel to the radius at $\alpha$, $\delta$) is 
$$\alpha\beta\epsilon . \epsilon\gamma\delta=\epsilon^2 . 
\alpha\beta\gamma\delta]$$

{\bf 107}. To exemplify the variety of possible transformations 
even of simple expressions, we will take cases which are of 
frequent occurrence in applications to geometry. 

Thus $$T(\rho+\alpha)=T(\rho-\alpha)$$
[which expresses that if 
$$
\overline{OA}=\alpha\;\;\;\;
\overline{OA^{\prime}}=-\alpha\;\;\;\;\textrm{ and }\;\;\;\;
\overline{OP}=\rho
$$
we have\hbox{\hskip 4cm}$AP=A^{\prime}P$\\
and thus that $P$ is any point equidistant from two fixed points,] 
may be written $$(\rho+\alpha)^2=(\rho-\alpha)^2$$
or\hbox{\hskip 3cm}$\rho^2+2S\alpha\rho+\alpha^2=
\rho^2-2S\alpha\rho+\alpha^2\;\;\;\;\textrm{(\S 104)}$\\
whence\hbox{\hskip 4cm}$S\alpha\rho=0$\\
This may be changed to 
$$\alpha\rho+\rho\alpha=0$$
or
$$\alpha\rho+K\alpha\rho=0$$
$$SU\frac{\rho}{\alpha}=0$$
or finally,
$$TVU\frac{\rho}{\alpha}=1$$
all of which express properties of a plane. 

Again,\hbox{\hskip 4cm}$T\rho=T\alpha$\\
may be written\hbox{\hskip 3.2cm}$\displaystyle T\frac{\rho}{\alpha}=1$
$$\left(S\frac{\rho}{\alpha}\right)^2-\left(V\frac{\rho}{\alpha}\right)^2=1$$
$$(\rho+\alpha)^2-2S\alpha(\rho+\alpha)=0$$
$$\rho=(\rho+\alpha)^{-1}\alpha(\rho+\alpha)$$
$$S(\rho+\alpha)(\rho-\alpha)=0$$
or finally,
$$T.(\rho+\alpha)(\rho-\alpha)=2TV\alpha\rho$$

All of these express properties of a sphere. They will be 
interpreted when we come to geometrical applications. 

{\bf 108}. {\sl To find the space relation among five points.}

A system of five points, so far as its internal relations are 
concerned, is fully given by the vectors from one to the other four. 
If three of these be called $\alpha$, $\beta$, $\gamma$, the fourth, 
$\delta$, is necessarily expressible as 
$x\alpha+y\beta+z\gamma$. Hence the relation required must be 
independent of x, y, z. 

But 
$$
\left.
\begin{array}{rlll}
S\alpha\delta &=\;x\alpha^2     &+\;yS\alpha\beta &+\;zS\alpha\gamma\\
S\beta\delta  &=\;xS\beta\alpha &+\;y\beta^2      &+\;zS\beta\gamma\\
S\gamma\delta &=\;xS\gamma\alpha &+\;yS\gamma\beta &+\;z\gamma^2\\
S\delta\delta=\delta^2 &=\;xS\delta\alpha &+\;yS\delta\beta &+\;zS\delta\gamma
\end{array}
\right\}\eqno{(1)}
$$
The elimination of $x$, $y$, $z$ gives a determinant of the fourth order, 
which may be written 
$$
\left\vert
\begin{array}{cccc}
S\alpha\alpha & S\alpha\beta & S\alpha\gamma & S\alpha\delta\\
S\beta\alpha  & S\beta\beta  & S\beta\gamma  & S\beta\delta\\
S\gamma\alpha & S\gamma\beta & S\gamma\gamma & S\gamma\delta\\
S\delta\alpha & S\delta\beta & S\delta\gamma & S\delta\delta
\end{array}
\right\vert=0
$$
Now each term may be put in either of two forms, thus 
$$S\beta\gamma=\frac{1}{2}\left\{\beta^2+\gamma^2-(\beta-\gamma)^2\right\}=
-T\beta T\gamma\cos\widehat{\beta\gamma}$$


If the former be taken we have the expression connecting the 
distances, two and two, of five points in the form given by Muir 
(Proc. R. S. E. 1889) ; if we use the latter, the tensors divide out 
(some in rows, some in columns), and we have the relation among 
the cosines of the sides and diagonals of a spherical quadrilateral. 

We may easily show (as an exercise in quaternion manipulation 
merely) that this is the {\sl only} condition, by showing that from it 
we can get the condition when any other of the points is taken as 
origin. Thus, let the origin be at $\alpha$, the vectors are 
$\alpha$, $\beta-\alpha$, $\gamma-\alpha$, $\delta-\alpha$.
But, by changing the signs of the first row, and first 
column, of the determinant above, and then adding their values 
term by term to the other rows and columns, it becomes 
$$
\left\vert
\begin{array}{cccc}
S(\;\;\;-\alpha)(-\alpha) & S(\;\;\;-\alpha)(\beta-\alpha) 
& S(\;\;\;-\alpha)(\gamma-\alpha) & S(\;\;\;-\alpha)(\delta-\alpha)\\
S(\beta-\alpha)(-\alpha)  & S(\beta-\alpha)(\beta-\alpha)  
& S(\beta-\alpha)(\gamma-\alpha)  & S(\beta-\alpha)(\delta-\alpha)\\
S(\gamma-\alpha)(-\alpha) & S(\gamma-\alpha)(\beta-\alpha) 
& S(\gamma-\alpha)(\gamma-\alpha) & S(\gamma-\alpha)(\delta-\alpha)\\
S(\delta-\alpha)(-\alpha) & S(\delta-\alpha)(\beta-\alpha) 
& S(\delta-\alpha)(\gamma-\alpha) & S(\delta-\alpha)(\delta-\alpha)
\end{array}
\right\vert
$$
which, when equated to zero, gives the same relation as before. 
[See Ex. 10 at the end of this Chapter.] 

An additional point, with $\epsilon=x^{\prime}\alpha+
y^{\prime}\beta+z^{\prime}\gamma$
gives six additional equations like (1) ; i. e. 
$$
\begin{array}{rlll}
S\alpha\epsilon&=x^{\prime}\alpha^2&+
y^{\prime}S\alpha\beta&+z^{\prime}S\alpha\gamma\\
S\beta\epsilon&=x^{\prime}S\beta\alpha&+y^{\prime}\beta^2&+
z^{\prime}S\beta\gamma\\
S\gamma\epsilon&=x^{\prime}S\gamma\alpha&+y^{\prime}S\gamma\beta&+
z^{\prime}\gamma^2\\
S\delta\epsilon&=x^{\prime}S\delta\alpha&+y^{\prime}S\delta\beta&+
z^{\prime}S\delta\gamma\\
&=xS\epsilon\alpha&+yS\epsilon\beta&+zS\epsilon\gamma\\
\epsilon^2&=x^{\prime}S\alpha\epsilon&+y^{\prime}S\beta\epsilon&+
z^{\prime}S\gamma\epsilon
\end{array}
$$
from which corresponding conclusions may be drawn. 

Another mode of solving the problem at the head of this 
section is to write the {\sl identity}
$$
\sum m(\alpha-\theta)^2=\sum m\alpha^2-sS.\theta\sum m\alpha+\theta^2\sum m
$$
where the $m$s are undetermined scalars, and the $\alpha$s are given 
vectors, while $\theta$ is any vector whatever. 

Now, {\sl provided that the number of given vectors exceeds four}, we 
do not completely determine the ms by imposing the conditions 
$$\sum m=0,\;\;\;\;\sum m\alpha=0$$
Thus we may write the above identity, for each of five vectors 
successively, as 
$$
\begin{array}{rcl}
\sum m(\alpha-\alpha_1)^2 &=& \sum m\alpha^2\\
\sum m(\alpha-\alpha_2)^2 &=& \sum m\alpha^2\\
\ldots\ldots &=& \ldots\\
\sum m(\alpha-\alpha_n)^2 &=& \sum m\alpha^2\\
\end{array}
$$
Take, with these,\hbox{\hskip 3cm}$\sum m = 0$\\
and we have six linear equations from which to eliminate the $m$s. 
The resulting determinant is 
$$
\left\vert
\begin{array}{cccccc}
\overline{\alpha_1-\alpha_1^2} & \overline{\alpha_1-\alpha_s^2} &
\overline{\alpha_1-\alpha_3^2} & . & 
\overline{\alpha_1-\alpha_5^2} & 1\\
\overline{\alpha_2-\alpha_1^2} & \overline{\alpha_2-\alpha_s^2} &
\overline{\alpha_2-\alpha_3^2} & . & 
\overline{\alpha_2-\alpha_5^2} & 1\\
. & . & . & & . & \\
. & . & . & & . & \\
\overline{\alpha_5-\alpha_1^2} & \overline{\alpha_5-\alpha_s^2} &
\overline{\alpha_5-\alpha_3^2} & . & 
\overline{\alpha_5-\alpha_5^2} & 1\\
1 & 1 & . & . & 1 & 0\\
\end{array}
\right\vert
\sum m\alpha^2=0
$$

This is equivalent to the form in which Cayley gave the 
relation among the mutual distances of five points. (Camb. Math. 
Journ. 1841.) 

{\bf 109}. We have seen in \S 95 that a quaternion may be divided 
into its scalar and vector parts as follows: 
$$
\frac{\beta}{\alpha}=S\frac{\beta}{\alpha}+V\frac{\beta}{\alpha}=
\frac{T\beta}{T\alpha}(\cos\theta+\epsilon\sin\theta)
$$
where $\theta$ is the angle between the directions of 
$\alpha$ and $\beta$ and $\displaystyle \epsilon=UV\frac{\beta}{\alpha}$
is the unit-vector perpendicular to the plane of $\alpha$ 
and $\beta$ so situated 
that positive (i.e. left-handed) rotation about it turns 
$\alpha$ towards $\beta$

Similarly we have (\S 96) 
$$
\begin{array}{rl}
\alpha\beta&=S\alpha\beta + V\alpha\beta\\
&=T\alpha T\beta(-\cos\theta +\epsilon\sin\theta)
\end{array}
$$
$\theta$ and $\epsilon$ having the same signification as before. 

{\bf 110}. Hence, considering the versor parts alone, we have 
$$U\frac{\beta}{\alpha}=\cos\theta+\epsilon\sin\theta$$
Similarly
$$U\frac{\gamma}{\beta}=\cos\phi+\epsilon\sin\phi$$
$\phi$ being the positive angle between the directions of 
$\gamma$ and $\beta$, and $\epsilon$
the same vector as before, if $\alpha$, $\beta$, $\gamma$ be coplanar. 

Also we have 
$$U\frac{\gamma}{\alpha}=\cos(\theta+\phi)+\epsilon\sin(\theta+\phi)$$
But we have always 
$$\frac{\gamma}{\beta}.\frac{\beta}{\alpha}=\frac{\gamma}{\alpha}$$
and therefore
$$U\frac{\gamma}{\beta}.U\frac{\beta}{\alpha}=U\frac{\gamma}{\alpha}$$
or
$$
\cos(\phi+\theta)+\epsilon\sin(\phi+\theta)=
(\cos\phi+\epsilon\sin\phi)(\cos\theta+\epsilon\sin\theta)
$$
$$
=\cos\phi\cos\theta-\sin\phi\sin\theta+
\epsilon(\sin\phi\cos\theta+\cos\phi\sin\theta)
$$
from which we have at once the fundamental formulae for the 
cosine and sine of the sum of two arcs, by equating separately the 
scalar and vector parts of these quaternions. 

And we see, as an immediate consequence of the expressions 
above, that 
$$\cos m\theta+\epsilon\sin m\theta=(\cos\theta+\epsilon\sin\theta)^m$$
if $m$ be a positive whole number. For the left-hand side is a versor 
which turns through the angle $m\theta$ at once, while the right-hand 
side is a versor which effects the same object by $m$ successive turn 
ings each through an angle $\theta$. See \S\S 8, 9. 

{\bf 111}. To extend this proposition to fractional indices we have 
only to write $\displaystyle \frac{\theta}{n}$ for $\theta$,
when we obtain the results as in ordinary trigonometry. 

From De Moivre's Theorem, thus proved, we may of course 
deduce the rest of Analytical Trigonometry. And as we have 
already deduced, as interpretations of self-evident quaternion 
transformations (\S\S 97, 104), the fundamental formulae for the solution 
of plane triangles, we will now pass to the consideration of spherical 
trigonometry, a subject specially adapted for treatment by qua 
ternions; but to which we cannot afford more than a very few 
sections. (More on this subject will be found in Chap. XI in 
connexion with the Kinematics of rotation.) The reader is referred to 
Hamilton s works for the treatment of this subject by quaternion 
exponentials. 

{\bf 112}. Let $\alpha$, $\beta$, $\gamma$
be unit-vectors drawn from the centr to the 
corners $A$, $B$, $C$ of a triangle on the unit-sphere. Then it is evident 
that, with the usual notation, we have (\S 96), 
$$
S\alpha\beta=-\cos c,\;\;\;\;
S\beta\gamma=-\cos a,\;\;\;\;
S\gamma\alpha=-\cos b
$$
$$
TV\alpha\beta=\sin c,\;\;\;\;
TV\beta\gamma=\sin a,\;\;\;\;
TV\gamma\alpha=\sin b
$$
Also $UV\alpha\beta$, $UV\beta\gamma$, $UV\gamma\alpha$
are evidently the vectors of the corners of the polar triangle. 

Hence 
$$S.UV\alpha\beta UV\beta\gamma=\cos B\textrm{, \&c.}$$
$$TV.UV\alpha\beta UV\beta\gamma=\sin B\textrm{, \&c.}$$

Now (\S 90 (1)) we have 
$$
\begin{array}{rcl}
SV\alpha\beta V\beta\gamma&=&S.\alpha V(\beta V\beta\gamma)\\
&=&-S\alpha\beta S\beta\gamma + \beta^2S\alpha\gamma
\end{array}
$$
Remembering that we have 
$$
SV\alpha\beta V\beta\gamma=
TV\alpha\beta TV\beta\gamma S.UV\alpha\beta UV\beta\gamma
$$
we see that the formula just written is equivalent to 
$$\sin a \sin c \cos B = -\cos a \cos c + \cos b$$
or
$$\cos b = \cos a \cos c + \sin a \sin c \cos B$$

{\bf 113}. Again,
$$V.V\alpha\beta V\beta\gamma=-\beta S\alpha\beta\gamma$$
which gives 
$$
TV.V\alpha\beta V\beta\gamma=
TS.\alpha\beta\gamma=
TS.\alpha V\beta\gamma=
TS.\beta V\gamma\alpha=
TS.\gamma V\alpha\beta
$$
or
$$\sin a \sin c \sin B = \sin a \sin p_a = \sin b \sin p_b = \sin c \sin p_c$$
where $p_a$ is the arc drawn from $A$ perpendicular to $BC$, \&c. 
Hence
$$\sin p_a = \sin c \sin B$$
$$\sin p_b = \frac{\sin a \sin c}{\sin b} \sin B$$
$$\sin p_c = \sin a \sin B$$

{\bf 114}. Combining the results of the last two sections, we have 
$$V\alpha\beta .V\beta\gamma=\sin a \sin c \cos B-\beta \sin a \sin c \sin B$$
$$= \sin a \sin c (\cos B - \beta \sin B)$$
$$
\left.
\begin{array}{lcl}
\textrm{Hence} & \hbox{\hskip 1cm} 
& U.V\alpha\beta V\beta\gamma=(\cos B-\beta\sin B)\\
\textrm{and} & \hbox{\hskip 1.1cm} 
& U.V\gamma\beta V\beta\alpha=(\cos B+\beta\sin B)
\end{array}
\right\}
$$
These are therefore versors which turn all vectors perpendicular to 
$OB$ negatively or positively about $OB$ through the angle $B$. 

[It will be shown later (\S 119) that, in the combination 
$$(\cos B+\beta\sin B)(\;\;\;\;)(\cos B -\beta\sin B)$$
the system operated on is made to rotate, as if rigid, round the 
vector axis $\beta$ through an angle $2B$.] 

As another instance, we have 
$$
\begin{array}{rl}
\tan B &=\displaystyle \frac{\sin B}{\cos B}\\
&\\
&=\displaystyle 
\frac{TV.V\alpha\beta V\beta\gamma}{S.V\alpha\beta V\beta\gamma}\\
&\\
&=\displaystyle 
-\beta^{-1}\frac{V.V\alpha\beta V\beta\gamma}{S.V\alpha\beta V\beta\gamma}\\
&\\
&=\displaystyle 
-\frac{S.\alpha\beta\gamma}{S\alpha\gamma+S\alpha\beta S\beta\gamma}
=\textrm{ \&c}
\end{array}
\eqno{(1)}
$$

The interpretation of each of these forms gives a different theorem 
in spherical trigonometry. 

{\bf 115}. Again, let us square the equal quantities 
$$
V.\alpha\beta\gamma\;\;\;\;\textrm{   and   }\;\;\;\;
\alpha S\beta\gamma-\beta S\alpha\gamma+\gamma S\alpha\beta$$
supposing $\alpha$, $\beta$, $\gamma$ to be any unit-vectors whatever. We have 
$$
-(V.\alpha\beta\gamma)^2=
S^2\beta\gamma + S^2\gamma\alpha + 
S^2\alpha\beta + 2S\beta\gamma S\gamma\alpha S\alpha\beta
$$
But the left-hand member may be written as 
$$T^2.\alpha\beta\gamma-S^2.\alpha\beta\gamma$$
whence
$$
1-S^2.\alpha\beta\gamma=S^2\beta\gamma +S^2\gamma\alpha +
S^2\alpha\beta+2S\beta\gamma S\gamma\alpha S\alpha\beta
$$
or
$$
1-\cos^2 a - \cos^2 b - \cos^2 c + 2 \cos a \cos b \cos c$$
$$= \sin^2 a \sin^2 p_a = \textrm{ \&c.}$$
$$= \sin^2 a \sin^2 b \sin^2 C  = \textrm{ \&c.}$$
all of which are well-known formulae. 

{\bf 116}. Again, for any quaternion, 
$$q=Sq+Vq$$
so that, if $n$ be a positive integer, 
$$q^n=(Sq)^n+n(Sq)^{n-1}Vq+
\frac{n.\overline{n-1}}{1.2}(Sq)^{n-2}(Vq)^2+\ldots$$
From this at once 
$$S.q^n=(Sq)^n-\frac{n.\overline{n-1}}{1.2}(Sq)^{n-2}T^2Vq$$
$$+\frac{n.\overline{n-1}.\overline{n-2}.\overline{n-3}}{1.2.3.4}
(Sq)^{n-4}T^4(Vq)-\textrm{\&c.,}$$
$$V.q^n=Vq\left[n(Sq)^{n-1}-
\frac{n.\overline{n-1}.\overline{n-2}}{1.2.3}
(Sq)^{n-3}T^2Vq+\textrm{\&c.,}\right]$$
If $q$ be a versor we have 
$$q=\cos u +\theta\sin u$$
so that 
$$
\begin{array}{rl}
S.q^n & =\displaystyle
(\cos u)^n-\frac{n.\overline{n-1}}{1.2}(\cos u)^{n-2}(\sin u)^2+\ldots\\
&\\
&=\displaystyle\cos nu;\\
&\\
V.q^n & =\displaystyle
\theta\sin u\left[n(\cos u)^{n-1}-
\frac{n.\overline{n-1}.\overline{n-2}}{1.2.3}
(\cos u)^{n-3}(\sin u)^2+\ldots\right]\\
&\\
&=\displaystyle\theta\sin nu;
\end{array}
$$
as we might at once have concluded from \S 110. 

Such results may be multiplied indefinitely by any one who has 
mastered the elements of quaternions. 

{\bf 117}. A curious proposition, due to Hamilton, gives us a 
quaternion expression for the {\sl spherical excess} in any triangle. 
The following proof, which is very nearly the same as one of his, 
though by no means the simplest that can be given, is chosen here 
because it incidentally gives a good deal of other information. 
We leave the quaternion proof as an exercise. 

Let the unit-vectors drawn from the centre of the sphere to 
$A$, $B$, $C$, respectively, be $\alpha$, $\beta$, $\gamma$.
It is required to express, as an 
arc and as an angle on the sphere, the quaternion 
$$\beta\alpha^{-1}\gamma$$

\begin{center}
\includegraphics{ps/quat18.ps}
\end{center}
\vskip 0.5cm

The figure represents an orthographic projection made on a 
plane perpendicular to $\gamma$. Hence $C$ is the centre of the circle $DEe$. 
Let the great circle through $A$, $B$ meet $DEe$ in $E$, $e$, and let $DE$ be 
a quadrant. Thus 
${\stackrel{\frown}{DE}}$ represents $\gamma$ (\S 72). Also make 
${\stackrel{\frown}{EF}} = {\stackrel{\frown}{AB}}$
$=\beta\alpha^{-1}$ Then, evidently, 
$${\stackrel{\frown}{DF}}=\beta\alpha^{-1}\gamma$$
which gives the arcual representation required. 

Let $DF$ cut $Ee$ in $G$. Make $Ca = EG$, and join $D$, $a$, and $a$, $F$. 
Obviously, as $D$ is the pole of $Ee$, $Da$ is a quadrant ; and since 
$EG = Ca$, $Ga = EG$, a quadrant also. Hence $a$ is the pole of $DG$, 
and therefore the quaternion may be represented by the angle 
$DaF$. 

Make $Cb = Ca$, and draw the arcs $Pa\beta$, $Pb\alpha$ from $P$, the pole of 
$AB$. Comparing the triangles $Eb\alpha$ and $ea\beta$, 
we see that $E\alpha = e\beta$.
But, since $P$ is the pole of $AB$, $F\beta a$ is a right angle: and therefore 
as $Fa$ is a quadrant, so is $F\beta$. Thus $AB$ is the complement of $E\alpha$
or $\beta e$, and therefore 
$$\alpha\beta=2AB$$

Join $bA$. and produce it to $c$ so that $Ac = bA$; join $c$, $P$, cutting 
$AB$ in $o$. Also join $c$, $B$, and $B$, $a$. 

Since $P$ is the pole of $AB$, the angles at $o$ are right angles; 
and therefore, by the equal triangles $b\alpha A$, $coA$, we have 
$$\alpha A = Ao$$
But
$$\alpha\beta = 2AB$$
whence
$$oB=B\beta$$
and therefore the triangles $coB$ and $Ba\beta$ are equal, and $c$, $B$, $a$ 
lie on the same great circle. 

Produce $cA$ and $cB$ to meet in $H$ (on the opposite side of the 
sphere). $H$ and $c$ are diametrically opposite, and therefore $cP$, 
produced, passes through $H$. 

Now $Pa = Pb = PH$, for they differ from quadrants by the 
equal arcs $a\beta$, $b\alpha$, $oc$. Hence these arcs divide the 
triangle $Hab$ into three isosceles triangles. 

But
$$\angle PHb + \angle PHA = \angle aHb = \angle bca$$
Also
$$\angle Pab = \pi - \angle cab - \angle PaH$$
$$\angle Pba = \angle Pab = \pi - \angle cba - \angle PbH$$
Adding,
$$2\angle Pab = 2\pi - \angle cab - \angle cba - \angle bca$$
$$= \pi - (\textrm{spherical excess of }abc)$$
But, as $\angle Fa\beta$ and $\angle Dae$ are right angles, we have
$$
\textrm{angle of }\beta\alpha^{-1}\gamma =
\angle FaD = \beta ae = \angle Pab
$$
$$=\frac{\pi}{2} - \frac{1}{2}(\textrm{spherical excess of }abc)$$

[Numerous singular geometrical theorems, easily proved {\sl ab 
initio} by quaternions, follow from this: e.g. The arc $AB$, which 
bisects two sides of a spherical triangle $abc$, intersects the base at 
the distance of a quadrant from its middle point. All spherical 
triangles, with a common side, and having their other sides 
bisected by the same great circle (i.e. having their vertices in a 
small circle parallel to this great circle) have equal areas, \&c. ]

{\bf 118}. Let $\overline{Oa}=\alpha^{\prime}$, $\overline{Ob}=\beta^{\prime}$,
$\overline{Oc}=\gamma^{\prime}$, and we have 
$$
\begin{array}{rcl}
\left(\frac{\alpha^{\prime}}{\beta^{\prime}}\right)^{\frac{1}{2}}
\left(\frac{\beta^{\prime}}{\gamma^{\prime}}\right)^{\frac{1}{2}}
\left(\frac{\gamma^{\prime}}{\alpha^{\prime}}\right)^{\frac{1}{2}}&=&
{\stackrel{\frown}{Ca}}.{\stackrel{\frown}{cA}}.{\stackrel{\frown}{Bc}}\\
&=&{\stackrel{\frown}{Ca}}.{\stackrel{\frown}{BA}}\\
&=&{\stackrel{\frown}{EG}}.{\stackrel{\frown}{FE}}=
{\stackrel{\frown}{FG}}
\end{array}
$$

But $FG$ is the complement of $DF$. Hence the {\sl angle of the 
quaternion}
$$
\left(\frac{\alpha^{\prime}}{\beta^{\prime}}\right)^{\frac{1}{2}}
\left(\frac{\beta^{\prime}}{\gamma^{\prime}}\right)^{\frac{1}{2}}
\left(\frac{\gamma^{\prime}}{\alpha^{\prime}}\right)^{\frac{1}{2}}
$$
{\sl is half the spherical excess of the triangle whose angular points are 
at the extremities of the unit-vectors} $\alpha^{\prime}$, $\beta^{\prime}$, 
and $\gamma^{\prime}$.

[In seeking a purely quaternion proof of the preceding proposi 
tions, the student may commence by showing that for any three 
unit-vectors we have 
$$
\frac{\beta}{\alpha}\frac{\gamma}{\beta}\frac{\alpha}{\gamma}=
-(\beta\alpha^{-1}\gamma)^2
$$

The angle of the first of these quaternions can be easily assigned; 
and the equation shows how to find that of $\beta\alpha^{-1}\gamma$. 

Another easy method is to commence afresh by forming from 
the vectors of the corners of a spherical triangle three new vectors 
thus: 
$$
\alpha^{\prime}=\left(\frac{\beta+\gamma}{\alpha}^{2}\right)^2 .\;
\alpha,\;\;\;\;\;
\textrm{\&c.}
$$

Then the angle between the planes of $\alpha$, $\beta^{\prime}$ and
$\gamma^{\prime}$, $\alpha$; or of $\beta$, $\gamma^{\prime}$ 
and $\alpha^{\prime}$,
$\beta$; or of $\gamma$, $\alpha^{\prime}$ and $\beta^{\prime}$, $\gamma$
is obviously the spherical excess. 

But a still simpler method of proof is easily derived from the 
composition of rotations.] 

{\bf 119}. It may be well to introduce here, though it belongs 
rather to Kinematics than to Geometry, the interpretation of the 
operator 
$$q(\;\;\;)q^{-1}$$

By a rotation, about the axis of $q$, through double the angle of $q$, 
the quaternion $r$ becomes the quaternion $qrq^{-1}$ . Its tensor and 
angle remain unchanged, its plane or axis alone varies. 

\begin{center}
\includegraphics{ps/quat19.ps}
\end{center}
\vskip 0.5cm

A glance at the figure is sufficient for 
the proof, if we note that of course 
$T . qrq^{-1} = Tr$, and therefore that we need 
consider the {\sl versor} parts only. Let $Q$ 
be the pole of $q$. 
$$
{\stackrel{\frown}{AB}}=q,\;\;\;\;
{\stackrel{\frown}{AB^{-1}}}=q^{-1},\;\;\;\;
{\stackrel{\frown}{B^{\prime}C^{\prime}}}=r
$$
Join $C^{\prime}A$, and make 
${\stackrel{\frown}{AC}}={\stackrel{\frown}{C^{\prime}A}}$. Join $CB$.

Then ${\stackrel{\frown}{CB}}$ is $qrq^{-1}$, 
its arc $CB$ is evidently equal in length to that 
of $r$, $B^{\prime}C^{\prime}$; and its plane (making the same angle with 
$B^{\prime}B$ that that of 
$B^{\prime}C^{\prime}$ does) has evidently been made to revolve about $Q$, the 
pole of $q$, through double the angle of $q$. 

It is obvious, from the nature of the above proof, that this 
operation is distributive; i.e. that 
$$q(r+s)q^{-1}=qrq^{-1}+qsq^{-1}$$

If $r$ be a vector, $=\rho$, then $q\rho q^{-1}$ 
(which is also a vector) is the 
result of a rotation through double the angle of $q$ about the axis 
of $q$. Hence, as Hamilton has expressed it, if $B$ represent a rigid 
system, or assemblage of vectors, 
$$qBq^{-1}$$
is its new position after rotating through double the angle of $q$ 
about the axis of $q$. 

{\bf 120}. To compound such rotations, we have 
$$r . qBq^{-1} . r^{-1} = rq . B . (rq)^{-1}$$

To cause rotation through an angle $t$-fold the double of the angle 
of $q$ we write 
$$q^{t}Bq^{-t}$$

To reverse the direction of this rotation write
$$q^{-t}Bq^{t}$$

To {\sl translate} the body $B$ without rotation, each point of it moving 
through the vector $\alpha$, we write $\alpha + B$. 

To produce rotation of the translated body about the same axis, 
and through the same angle, as before, 
$$q(\alpha+B)q^{-1}$$

Had we rotated first, and then translated, we should have had 
$$\alpha+qBq^{-1}$$

From the point of view of those who do not believe in the 
Moon s rotation, the former of these expressions ought to be 
$$q\alpha q^{-1}+B$$
instead of 
$$q\alpha q^{-1}+qBq^{-1}$$
But to such men quaternions are unintelligible. 

{\bf 121}. The operator above explained finds, of course, some 
of its most direct applications in the ordinary questions of 
Astronomy, connected with the apparent diurnal rotation of the 
stars. If $\lambda$ be a unit-vector parallel to the polar axis, and $h$ the 
hour angle from the meridian, the operator is 
$$
\left(\cos\frac{h}{2}-\lambda\sin\frac{h}{2}\right)
\left(\;\;\;\;\right)
\left(\cos\frac{h}{2}+\lambda\sin\frac{h}{2}\right)
$$
or
$$L^{-1}\left(\;\;\;\;\right)L$$

the inverse going first, because the {\sl apparent} rotation is negative 
(clockwise). 

If the upward line be $i$, and the southward $j$, we have 
$$\lambda = i\sin l-j\cos l$$
where $l$ is the latitude of the observer. The meridian equatorial 
unit vector is 
$$\mu = i\cos l+j\sin l$$
and $\lambda$, $\mu$, $k$ of course form a rectangular unit system. 

The meridian unit-vector of a heavenly body is 
$$\delta=i\cos(l-d)+j\sin(l-d)$$
$$=\lambda\sin d+\mu\cos d$$
where $d$ is its declination. 

Hence when its hour-angle is $h$, its vector is 
$$\delta^{\prime}=L^{-1}\delta L$$

The vertical plane containing it intersects the horizon in 
$$iVi\delta^{\prime}=jSj\delta^{\prime}+kSk\delta^{\prime}$$
so that 
$$\tan(azimuth)=\frac{Sk\delta^{\prime}}{Sj\delta^{\prime}}\eqno{(1)}$$

[This may also be obtained directly from the last formula (1) 
of \S 114.] 

To find its Amplitude, i.e. its azimuth at rising or setting, 
the hour-angle must be obtained from the condition 
$$Si\delta^{\prime}=0\eqno{(2)}$$

These relations, with others immediately deducible from them, 
enable us (at once and for ever) to dispense with the hideous 
formulae of Spherical Trigonometry. 

{\bf 122}. To show how readily they can be applied, let us 
translate the expressions above into the ordinary notation. This 
is effected at once by means of the expressions for $\lambda$, $\mu$, $L$,
and $\delta$ above, which give by inspection 
$$\delta^{\prime}=\lambda\sin d+(\mu\cos h-k\sin h)\cos d$$
= x sin d + (fjb cos h k sin h) cos d, 
and we have from (1) and (2) of last section respectively 
$$
\tan(azimuth)=
\frac{\sin h\cos d}{\cos l\sin d-\sin l\cos d\cos h}\eqno{(1)}
$$
$$
\cos h+\tan l \tan d=0\eqno{(2)}
$$

In Capt. Weir s ingenious {\sl Azimuth Diagram}, these equations 
are represented graphically by the rectangular coordinates of a 
system of confocal conics: viz. 
$$
\left.
\begin{array}{c}
x = \sin h \sec l \\
y = \cos h \tan l
\end{array}
\right\}\eqno{(3)}
$$

The ellipses of this system depend upon $l$ alone, the hyperbolas 
upon $h$. Since (1) can, by means of (3), be written as 
$$\tan(azimuth)=\frac{x}{\tan d-y}$$
we see that the azimuth can be constructed at once by joining 
with the point $0$, $-\tan d$, the intersection of the proper ellipse and 
hyperbola. 

Equation (2) puts these expressions for the coordinates in the 
form 
$$
\left.
\begin{array}{c}
x=\sec l\sqrt{1-\tan^{2} l\tan^{2} d}\\
y=-\tan^{2} l \tan d
\end{array}
\right\}
$$

The elimination of $d$ gives the ellipse as before, but that of $l$
gives, instead of the hyperbolas, the circles 
$$x^{2}+y^{2}-y(\tan d-\cot d)=1$$

The radius is 
$$\frac{1}{2}(\tan d+ \cot d)$$
and the coordinates of the centre are 
$$0,\;\;\;\frac{1}{2}(\tan d- \cot d)$$

123. A scalar equation in $\rho$, the vector of an undetermined 
point, is generally the equation of a {\sl surface}; since we may use 
in it the expression 
$$\rho=x\alpha$$
where $x$ is an unknown scalar, and $\alpha$ any assumed unit-vector. 
The result is an equation to determine $x$. Thus one or more 
points are found on the vector $x\alpha$, whose coordinates satisfy the 
equation; and the locus is a surface whose degree is determined 
by that of the equation which gives the values of $x$. 

But a {\sl vector} equation in $\rho$, as we have seen, generally leads to 
three scalar equations, from which the three rectangular or other 
components of the sought vector are to be derived. Such a vector 
equation, then, usually belongs to a definite number of {\sl points} in 
space. But in certain cases these may form a {\sl line}, and even a 
{\sl surface}, the vector equation losing as it were one or two of the 
three scalar equations to which it is usually equivalent. 

Thus while the equation 
$$\alpha\rho=\beta$$
gives at once 
$$\rho=\alpha^{-1}\beta$$
which is the vector of a definite point, since by making $\rho$ a {\sl vector}
we have evidently assumed 
$$S\alpha\beta=0$$
the closely allied equation
$$V\alpha\rho=\beta$$
is easily seen to involve
$$S\alpha\beta=0$$
and to be satisfied by
$$\rho=\alpha^{-1}\beta+x\alpha$$
whatever be $x$. Hence the vector of any point whatever in the 
line drawn parallel to $\alpha$ from the extremity of $\alpha^{-1}\beta$
satisfies the given equation. [The difference between the results depends 
upon the fact that $S\alpha\rho$ is indeterminate in the second form, but 
definite (= 0) in the first.] 

{\bf 124}. Again,
$$V\alpha\rho . V\rho\beta=(V\alpha\beta)^{2}$$
is equivalent to but two scalar equations. For it shows that $V\alpha\rho$
and $V\beta\rho$ are parallel, i.e. $\rho$ 
lies in the same plane as $\alpha$ and $\beta$, and 
can therefore be written (\S 24) 
$$\rho=x\alpha+y\beta$$
where $x$ and $y$ are scalars as yet undetermined. 

We have now 
$$V\alpha\rho=yV\alpha\beta$$
$$V\rho\beta=xV\alpha\beta$$
which, by the given equation, lead to 
$$xy=1,\;\;\;\textrm{or}\;\;\;y=\frac{1}{x}$$
or finally
$$\rho=x\alpha+\frac{1}{x}\beta$$
which (\S 40) is the equation of a hyperbola whose asymptotes are 
in the directions of $\alpha$ and $\beta$.

{\bf 125}. Again, the equation 
$$V . V\alpha\beta V\alpha\rho=0$$
though apparently equivalent to three scalar equations, is really 
equivalent to one only. In fact we see by \S 91 that it may be 
written 
$$-\alpha S.\alpha\beta\rho=0$$
whence, if $\alpha$ be not zero, we have 
$$S . \alpha\beta\rho=0$$
and thus (\S 101) the only condition is that $\rho$ is coplanar with 
$\alpha$, $\beta$.
Hence the equation represents the plane in which 
$\alpha$ and $\beta$ lie. 

{\bf 126}. Some very curious results are obtained when we extend 
these processes of interpretation to functions of a {\sl quaternion}
$$q=w+\rho$$
instead of functions of a mere {\sl vector} $\rho$. 

A scalar equation containing such a quaternion, along with 
quaternion constants, gives, as in last section, the equation of a 
surface, if we assign a definite value to $w$. Hence for successive 
values of $w$, we have successive surfaces belonging to a system ; 
and thus when $w$ is indeterminate the equation represents not a 
{\sl surface}, as before, but a {\sl volume}, 
in the sense that the vector of any 
point within that volume satisfies the equation. 

Thus the equation 
$$(Tq)^2=a^2$$
or
$$w^2-\rho^2=a^2$$
or
$$(TP)^2=a^2-w^2$$
represents, for any assigned value of $w$, not greater than $a$, a sphere 
whose radius is $\sqrt{a^2-w^2}$. Hence the equation is satisfied by the 
vector of any point whatever in the {\sl volume} of a sphere of radius $a$, 
whose centre is origin. 

Again, by the same kind of investigation, 
$$(T (q-\beta))^2=a^2$$
where $q=w+\rho$, is easily seen to represent the volume of a sphere 
of radius $a$ described about the extremity of $\beta$ as centre. 

Also $S(q^2) = -a^2$ is the equation of infinite space less the space 
contained in a sphere of radius $a$ about the origin. 

Similar consequences as to the interpretation of vector 
equations in quaternions may be readily deduced by the reader. 

{\bf 127}. The following transformation is enuntiated without proof 
by Hamilton ({\sl Lectures}, p. 587, and {\sl Elements}, p. 299). 
$$r^{-1}(r^2q^2)^{\frac{1}{2}}q^{-1}=U(rq+KrKq)$$
To prove it, let
$$r^{-1}(r^2q^2)^{\frac{1}{2}}q^{-1}=t$$
then 
$$Tt=1$$
and therefore
$$Kt=t^{-1}$$
But
$$(r^2q^2)^{\frac{1}{2}}=rtq$$
or
$$r^2q^2=rtqrtq$$
or
$$rq=tqrt$$
Hence
$$KqKr=t^{-1}KrKqt^{-1}$$
or
$$KrKq=tKqKrt$$
Thus we have
$$U(rq\pm KrKq)=tU(qr\pm KqKr)t$$
or, if we put
$$s=U(qr\pm KqKr)$$
$$Ks=\pm tst$$
Hence
$$sKs=(Ts)^2=1=\pm stst$$
which, if we take the positive sign, requires 
$$st=\pm 1$$
or
$$t=\pm s^{-1}=\pm UKs$$
which is the required transformation. 

[It is to be noticed that there are other results which might 
have been arrived at by using the negative sign above ; some 
involving an arbitrary unit-vector, others involving the imaginary 
of ordinary algebra.] 

{\bf 128}. As a final example, we take a transformation of Hamilton's, 
of great importance in the theory of surfaces of the second order. 

Transform the expression 
$$(S\alpha\rho)^2+(S\beta\rho)^2+(S\gamma\rho)^2$$
in which $\alpha$, $\beta$, $\gamma$ 
are any three mutually rectangular vectors, into the form
$$\left(\frac{T(\iota\rho+\rho\kappa)}{\kappa^2-\iota^2}\right)^2$$
which involves only two vector-constants, $\iota$, $\kappa$.

[The student should remark here that $\iota$, $\kappa$, two undetermined 
vectors, involve six disposable constants : and that $\alpha$, $\beta$,
$\gamma$, being a {\sl rectangular} system, involve also only six constants.] 
$$
\begin{array}{rcl}
\{T(\iota\rho+\rho\kappa)\}^2 
&=& (\iota\rho+\rho\kappa)(\rho\iota+\kappa\rho)\;\;\;\;(\S\S 52,55)\\
&=& (\iota^2+\kappa^2)\rho^2+(\iota\rho\kappa\rho+\rho\kappa\rho\iota)\\
&=& (\iota^2+\kappa^2)\rho^2+2S.\iota\rho\kappa\rho\\
&=& (\iota-\kappa)^2\rho^2+4S\iota\rho S\kappa\rho
\end{array}
$$
Hence
$$
(S\alpha\rho)^2+(S\beta\rho)^2+(S\gamma\rho)^2=
\frac{(\iota-\kappa)^2}{(\kappa^2-\iota^2)^2}\rho^2+
4\frac{S\iota\rho S\kappa\rho}{(\kappa^2-\iota^2)^2}
$$
But
$$
\alpha^{-2}(S\alpha\rho)^2+
\beta^{-2}(S\beta\rho)^2+
\gamma^{-2}(S\gamma\rho)^2=
\rho^2\;\;\;\;(\S\S 25,73).
$$
Multiply by $\beta^2$ and subtract, we get 
$$
\left(1-\frac{\beta^2}{\alpha^2}\right)(S\alpha\rho)^2-
\left(\frac{\beta^2}{\gamma^2}-1\right)(S\gamma\rho)^2=
\left\{\frac{(\iota-\kappa)^2}{(\kappa^2-\iota^2)^2}-\beta^2\right\}\rho^2+
4\frac{S\iota\rho S\kappa\rho}{(\kappa^2-\iota^2)^2}
$$

The left side breaks up into two real factors if $\beta^2$ be intermediate 
in value to $\alpha^2$ and $\gamma^2$: 
and that the right side may do so the term 
in $\rho^2$ must vanish. This condition gives 
$$\beta^2=\frac{(\iota-\kappa)^2}{(\kappa^2-\iota^2)^2}$$
and the identity becomes 
$$
S\left\{
\alpha\sqrt{\left(1-\frac{\beta^2}{\alpha^2}\right)}+
\gamma\sqrt{\left(\frac{\beta^2}{\gamma^2}-1\right)}
\right\}
\rho S\left\{
\alpha\sqrt{\left(1-\frac{\beta^2}{\alpha^2}\right)}-
\gamma\sqrt{\left(\frac{\beta^2}{\gamma^2}-1\right)}
\right\}\rho=
4\frac{S\iota\rho S\kappa\rho}{(\kappa^2-\iota^2)^2}
$$
Hence we must have 
$$
\frac{2\iota}{\kappa^2-\iota^2}=
p\left\{
\alpha\sqrt{\left(1-\frac{\beta^2}{\alpha^2}\right)}+
\gamma\sqrt{\left(\frac{\beta^2}{\gamma^2}-1\right)}
\right\}
$$
$$
\frac{2\kappa}{\kappa^2-\iota^2}=
\frac{1}{p}\left\{
\alpha\sqrt{\left(1-\frac{\beta^2}{\alpha^2}\right)}-
\gamma\sqrt{\left(\frac{\beta^2}{\gamma^2}-1\right)}
\right\}
$$
where $\rho$ is an undetermined scalar. 

To determine $\rho$, substitute in the expression for $\beta^2$, and we find 
$$
\begin{array}{rcl}
4\beta^2=\frac{4(\iota-\kappa)^2}{(\kappa^2-\iota^2)^2}
&=&\left(p-\frac{1}{p}\right)^2(\alpha^2-\beta^2)+
\left(p+\frac{1}{p}\right)^2(\beta^2-\gamma^2)\\
&=&\left(p^2+\frac{1}{p^2}\right)(\alpha^2-\gamma^2)-
2(\alpha^2+\gamma^2)+4\beta^2
\end{array}
$$

Thus the transformation succeeds if 
$$p^2+\frac{1}{p^2}=\frac{2(\alpha^2+\gamma^2)}{\alpha^2-\gamma^2}$$
which gives
$$p+\frac{1}{p}=\pm 2\sqrt{\frac{\alpha^2}{\alpha^2-\gamma^2}}$$
$$p-\frac{1}{p}=\pm 2\sqrt{\frac{\gamma^2}{\alpha^2-\gamma^2}}$$
Hence 
$$
\frac{4(\kappa^2-\iota^2)}{(\kappa^2-\iota^2)^2}=
\left(\frac{1}{p^2}-p^2\right)(\alpha^2-\gamma^2)=
\pm 4\sqrt{\alpha^2\gamma^2}
$$
$$
(\kappa^2-\iota^2)^{-1}=\pm T\alpha T\gamma$$

Again
$$
p=\frac{T\alpha +T\gamma}{\sqrt{\gamma^2-\alpha^2}},\;\;\;\;
\frac{1}{p}=\frac{T\alpha -T\gamma}{\sqrt{\gamma^2-\alpha^2}}
$$
and therefore 
$$
2\iota=\frac{T\alpha +T\gamma}{T\alpha T\gamma}
\left(
\sqrt{\frac{\beta^2-\alpha^2}{\gamma^2-\alpha^2}}U\alpha+
\sqrt{\frac{\gamma^2-\beta^2}{\gamma^2-\alpha^2}}U\gamma
\right)
$$
$$
2\kappa=\frac{T\alpha -T\gamma}{T\alpha T\gamma}
\left(
\sqrt{\frac{\beta^2-\alpha^2}{\gamma^2-\alpha^2}}U\alpha-
\sqrt{\frac{\gamma^2-\beta^2}{\gamma^2-\alpha^2}}U\gamma
\right)
$$
Thus we have proved the possibility of the transformation, and 
determined the transforming vectors $\iota$, $\kappa$.

{\bf 129}. By differentiating the equation 
$$
(S\alpha\rho)^2+(S\beta\rho)^2+(S\gamma\rho)^2=
\left(\frac{T(\iota\rho+\rho\kappa)}{(\kappa^2-\iota^2)}\right)^2
$$
we obtain, as will be seen in Chapter IV, the following, 
$$
S\alpha\rho S\alpha\rho^{\prime}+
S\beta\rho S\beta\rho^{\prime}+
S\gamma\rho S\gamma\rho^{\prime}=
\frac{S.(\iota\rho+\rho\kappa)(\kappa\rho^{\prime}+\rho^{\prime}\iota)}
{(\kappa^2-\iota^2)^2}
$$
where $\rho$ also may be any vector whatever. 

This is another very important formula of transformation ; and 
it will be a good exercise for the student to prove its truth by 
processes analogous to those in last section. We may merely 
observe, what indeed is obvious, that by putting $\rho^{\prime}=\rho$ it becomes 
the formula of last section. And we see that we may write, with 
the recent values of $\iota$ and $\kappa$ in terms of 
$\alpha$, $\beta$, $\gamma$, the identity 
$$
\begin{array}{rcl}
\alpha S\alpha\rho+\beta S\beta\rho+\gamma S\gamma\rho
&=&\displaystyle 
\frac{(\iota^2+\kappa^2)\rho+2V.\iota\rho\kappa}{(\kappa^2-\iota^2)^2}\\
&&\\
&=&\displaystyle 
\frac{(\iota-\kappa)^2\rho+2(\iota S\kappa\rho+\kappa S\iota\rho)}
{(\kappa^2-\iota^2)^2}
\end{array}
$$

{\bf 130}. In various quaternion investigations, especially in such 
as involve {\sl imaginary} intersections of curves and surfaces, the old 
imaginary of algebra of course appears. But it is to be particularly 
noticed that this expression is analogous to a scalar and not to a 
vector, and that like real scalars it is commutative in 
multiplication with all other factors. Thus it appears, by the same proof 
as in algebra, that any quaternion expression which contains this 
imaginary can always be broken up into the sum of two parts, one 
real, the other multiplied by the first power of $\sqrt{-1}$. Such an 
expression, viz. 
$$q=q^{\prime}+\sqrt{-1}q^{\prime\prime}$$
where $q^{\prime}$ and $q^{\prime\prime}$ are real quaternions, 
is called by Hamilton a 
BIQUATERNION. [The student should be warned that the term 
Biquaternion has since been employed by other writers in the 
sense sometimes of a ``set'' of 8 elements, analogous to the 
Quaternion 4 ; sometimes for an expression $q^{\prime} + \theta q^{\prime\prime}$ 
where $\theta$ is not 
the algebraic imaginary. By them Hamilton s Biquaternion is 
called simply a quaternion with non-real constituents.] Some 
little care is requisite in the management of these expressions, but 
there is no new difficulty. The points to be observed are: first, 
that any biquaternion can be divided into a real and an imaginary 
part, the latter being the product of $\sqrt{-1}$ by a real quaternion; 
second, that this $\sqrt{-1}$ is commutative with all other quantities in 
multiplication; third, that if two biquaternions be equal, as 
$$q^{\prime}+\sqrt{-1}\;q^{\prime\prime}=
r^{\prime}+\sqrt{-1}\;r^{\prime\prime}$$
we have, as in algebra, 
$$q^{\prime}=r^{\prime},\;\;\;\;q^{\prime\prime}=r^{\prime\prime}$$
so that an equation between biquaternions involves in general 
{\sl eight} equations between scalars. Compare \S 80. 

{\bf 131}. We have obviously, since $\sqrt{-1}$ is a scalar, 
$$S(q^{\prime}+\sqrt{-1}\;q^{\prime\prime})=
Sq^{\prime}+\sqrt{-1}\;Sq^{\prime\prime}$$
$$V(q^{\prime}+\sqrt{-1}\;q^{\prime\prime})=
Vq^{\prime}+\sqrt{-1}\;Vq^{\prime\prime}$$
Hence (\S 103) 
$$\{T(q^{\prime}+\sqrt{-1}\;q^{\prime\prime})\}^2$$
$$
=(Sq^{\prime}+\sqrt{-1}\;Sq^{\prime\prime}+
Vq^{\prime}+\sqrt{-1}\;Vq^{\prime\prime})
(Sq^{\prime}+\sqrt{-1}\;Sq^{\prime\prime}-Vq^{\prime}-
\sqrt{-1}\;Vq^{\prime\prime})
$$
$$=(Sq^{\prime}+\sqrt{-1}\;Sq^{\prime\prime})^2-
(Vq^{\prime}+\sqrt{-1}\;Vq^{\prime\prime})^2$$
$$=(Tq^{\prime})^2-(Tq^{\prime\prime})^2+
2\sqrt{-1}\;S.q^{\prime}Kq^{\prime\prime}$$

The only remark which need be made on such formulae is this, that 
{\sl the tensor of a biquaternion may vanish while both of the component 
quaternions are finite}. 

Thus, if 
$$Tq^{\prime}=Tq^{\prime\prime}$$
and
$$S.q^{\prime}Kq^{\prime\prime}=0$$
the above formula gives 
$$T(q^{\prime}+\sqrt{-1}\;q^{\prime\prime})=0$$
The condition 
$$S.q^{\prime}Kq^{\prime\prime}=0$$
may be written 
$$
Kq^{\prime\prime}=q^{'-1}\alpha,\;\;\;\textrm{ or }\;\;\;
q^{\prime\prime}=-\alpha Kq^{'-1}=-\frac{\alpha q^{\prime}}{(Tq^{\prime})^2}
$$
where $\alpha$ is any vector whatever. 

Hence 
$$Tq^{\prime}=Tq^{\prime\prime}=TKq^{\prime\prime}=
\frac{T\alpha}{Tq^{\prime\prime}}$$
and therefore 
$$
Tq^{\prime}(Uq^{\prime}-\sqrt{-1}\;U\alpha . Uq^{\prime})=
(1-\sqrt{-1}\;U\alpha)q^{\prime}
$$
is the general form of a biquaternion whose tensor is zero. 

{\bf 132}. More generally we have, $q$, $r$, $q^{\prime}$, $r^{\prime}$ 
being any four real and non-evanescent quaternions, 
$$
(q+\sqrt{-1}\;q^{\prime})(r+\sqrt{-1}\;r^{\prime})=
qr-q^{\prime}r^{\prime}+\sqrt{-1}\;(qr^{\prime}+q^{\prime}r)
$$
That this product may vanish we must have 
$$qr=q^{\prime}r^{\prime}$$
and
$$qr^{\prime}=-q^{\prime}r$$
Eliminating $r^{\prime}$ we have
$$qq^{'-1}qr=-q^{\prime}r$$
which gives 
$$(q^{'-1}q)^2=-1$$
i.e.
$$q=q^{\prime}\alpha$$
where $\alpha$ is some unit-vector. 

And the two equations now agree in giving 
$$-r=\alpha r^{\prime}$$
so that we have the biquaternion factors in the form 
$$q^{\prime}(\alpha +\sqrt{-1})\;\;\;\textrm{ and }
\;\;\;-(\alpha-\sqrt{-1})r^{\prime}$$
and their product is 
$$-q^{\prime}(\alpha +\sqrt{-1})(\alpha -\sqrt{-1})r^{\prime}$$
which, of course, vanishes. 

[A somewhat simpler investigation of the same proposition 
may be obtained by writing the biquaternions as 
$$
q^{\prime}(q^{'-1}q+\sqrt{-1})\;\;\;\textrm{ and }\;\;\;
(rr^{'-1}+\sqrt{-1})r^{\prime}
$$
or
$$ 
q^{\prime}(q^{\prime\prime}+\sqrt{-1})\;\;\;\textrm{ and }\;\;\;
(r^{\prime\prime}+\sqrt{-1})r^{\prime}
$$
and showing that 
$$q^{\prime\prime}=
-r^{\prime\prime}=\alpha \;\;\;\textrm{ where }\;T\alpha=1]$$

From this it appears that if the product of two {\sl bivectors}
$$
\rho+\sigma\sqrt{-1}\;\;\;\textrm{ and }\;\;\;
\rho^{\prime}+\sigma^{\prime}\sqrt{-1}
$$
is zero, we must have 
$$\sigma^{-1}\rho=-\rho^{\prime}\sigma^{'-1}=U\alpha$$
where $\alpha$ may be any vector whatever. But this result is still more 
easily obtained by means of a direct process. 

{\bf 133}. It may be well to observe here (as we intend to avail our 
selves of them in the succeeding Chapters) that certain abbreviated 
forms of expression may be used when they are not liable to confuse, 
or lead to error. Thus we may write 
$$T^2q\;\;\;\textrm{for}\;\;\;(Tq)^2$$
just as we write
$$\cos^2\theta\;\;\;\textrm{for}\;\;\;(\cos\theta)^2$$
although the true meanings of these expressions are 
$$T(Tq)\;\;\;\textrm{and}\;\;\;\cos(\cos\theta)$$

The former is justifiable, as $T(Tq) = Tq$, and therefore $T^2q$ is not 
required to signify the second tensor (or tensor of the tensor) of $q$. 
But the trigonometrical usage is defensible only on the score of 
convenience, and is habitually violated by the employment of 
$cos^{-1}x$ in its natural and proper sense. 
Similarly we may write 
$$S^2q\;\;\;\textrm{for}\;\;\;(Sq)^2,\;\;\;\textrm{\&c.}$$
but it may be advisable not to use 
$$Sq^2$$
as the equivalent of either of those just written; inasmuch as it 
might be confounded with the (generally) different quantity 
$$S.q^2\;\;\;\textrm{or}\;\;\;S(q^2)$$
although this is rarely written without the point or the brackets. 

The question of the use of points or brackets is one on which 
no very definite rules can be laid down. A beginner ought to use 
them freely, and he will soon learn by trial which of them are 
absolutely necessary to prevent ambiguity. 

In the present work this course has been adopted:-- the 
earlier examples in each part of the subject being treated with 
a free use of points and brackets, while in the later examples 
superfluous marks of the kind are gradually got rid of. 

It may be well to indicate some general principles which 
regulate the omission of these marks. Thus in $S.\alpha\beta$ or
$V.\alpha\beta$
the point is obviously unnecessary:-- because $S\alpha=0$, and 
$V\alpha=\alpha$
so that the $S$ would annihilate the term if it applied to $\alpha$ alone, 
while in the same case the $V$ would be superfluous. But in $S.qr$
and $V.qr$, the point (or an equivalent) is indispensable, for $Sq.r$, 
and $Vq.r$ are usually quite different from the first written 
quantities. In the case of $K$, and of $d$ (used for scalar differentiation),
the {\sl omission} of the point indicates that the operator acts 
{\sl only} on the nearest factor:-- thus 
$$Kqr=(Kq)r=Kq.r,\;\;\;dqr=(dq)r=dq.r$$
Kqr = (Kq) r = Kq . r, dqr = (dq) r=dq.r; 
while, if its action extend farther, we write 
$$K.qr=K(qr),\;\;\;d.qr=d(qr)\;\;\;\textrm{\&c.}$$

In more complex cases we must be ruled by the general 
principle of dropping nothing which is essential. Thus, for 
instance 
$$V(pK(dq)V(Vq.r))$$
may be written without ambiguity as 
$$V(pK(dq)V(Vq.r))$$
but nothing more can be dropped without altering its value. 

Another peculiarity of notation, which will occasionally be 
required, shows {\sl which portions} of a complex product are affected 
by an operator. Thus we write 
$$\nabla S\sigma\tau$$
if $\nabla$ operates on $\sigma$ and also on $\tau$, but 
$$\nabla_1S\sigma\tau_1$$
if it operates on $\tau$ alone. See, in this connection, the last Example 
at the end of Chap. IV. below. 

{\bf 134}. The beginner may expect to be at first a little puzzled 
with this aspect of the notation; but, as he learns more of the 
subject, he will soon see clearly the distinction between such an 
expression as 
$$S.V\alpha\beta V\beta\gamma$$
where we may omit at pleasure either the point or the first V 
without altering the value, and the very different one 
$$S\alpha\beta .V\beta\gamma$$
which admits of no such changes, without alteration of its value. 

All these simplifications of notation are, in fact, merely examples 
of the transformations of quaternion expressions to which part of 
this Chapter has been devoted. Thus, to take a very simple ex 
ample, we easily see that 
$$
\begin{array}{rcl}
S.V\alpha\beta V\beta\gamma
&=&SV\alpha\beta V\beta\gamma
=S.\alpha\beta V\beta\gamma
=S\alpha V.\beta V\beta\gamma
=-S\alpha V.(V\beta\gamma)\beta\\
&=&S\alpha V.(V\gamma\beta)\beta
=S.\alpha V(\gamma\beta)\beta
=S.V(\gamma\beta)\beta\alpha
=SV\gamma\beta V\beta\alpha\\
&=&S.\gamma\beta V\beta\alpha
=S.K(\beta\gamma)V\beta\alpha
=S.\beta\gamma KV\beta\alpha
=-S.\beta\gamma V\beta\alpha\\
&=&S.V\gamma\beta V\beta\alpha,\textrm{\&c., \&c.}
\end{array}
$$

The above group does not nearly exhaust the list of even the simpler 
ways of expressing the given quantity. We recommend it to the 
careful study of the reader. He will find it advisable, at first, to 
use stops and brackets pretty freely; but will gradually learn to 
dispense with those which are not absolutely necessary to prevent 
ambiguity. 

There is, however, one additional point of notation to which 
the reader s attention should be most carefully directed. A very 
simple instance will suffice. Take the expressions 
$$
\frac{\beta}{\gamma}.\frac{\gamma}{\alpha}\;\;\;\;\textrm{and}\;\;\;\;
\frac{\beta\gamma}{\gamma\alpha}
$$

The first of these is 
$$\beta\gamma^{-1}.\gamma\alpha^{-1}=\beta\alpha^{-1}$$
and presents no difficulty. But the second, though at first sight 
it closely resembles the first, is in general totally different in 
value, being in fact equal to 
$$\beta\gamma\alpha^{-1}\gamma^{-1}$$

For the denominator must be treated as {\sl one quaternion}. If, 
then, we write 
$$\frac{\beta\gamma}{\gamma\alpha}=q$$
we have 
$$\beta\gamma=q\gamma\alpha$$
so that, as stated above, 
$$q=\beta\gamma\alpha^{-1}\gamma^{-1}$$
We see therefore that 
$$
\frac{\beta}{\gamma}.\frac{\gamma}{\alpha}=
\frac{\beta}{\alpha}=
\frac{\beta\gamma}{\alpha\gamma};\;\;\;\textrm{but {\sl not}}\;\;=
\frac{\beta\gamma}{\gamma\alpha}
$$

\section{Examples to Chapter 3}

{\bf 1}. Investigate, by quaternions, the requisite formulae for 
changing from any one set of coordinate axes to another ; and 
derive from your general result, and also from special investiga 
tions, the usual expressions for the following cases: 

\begin{itemize}
\item[(a)] Rectangular axes turned about z through any angle. 

\item[(b)] Rectangular axes turned into any new position by rota 
tion about a line equally inclined to the three. 

\item[(c)] Rectangular turned to oblique, one of the new axes 
lying in each of the former coordinate planes. 
\end{itemize}

{\bf 2}. Point out the distinction between 
$$
\left(\frac{\alpha+\beta}{\alpha}\right)^2\;\;\;\textrm{and}\;\;\;
\frac{(\alpha+\beta)^2}{\alpha^2}
$$
and find the value of their difference. 

If
$$
T\beta/\alpha=1\;\;\;\textrm{and}\;\;\;
U\frac{\alpha+\beta}{\alpha}=\left(\frac{\beta}{\alpha}\right)^{\frac{1}{2}}
$$

Show also that 
$$
\frac{\alpha+\beta}{\alpha-\beta}=
\frac{V\alpha\beta}{1+S\alpha\beta^{\prime}}
$$
and 
$$
\frac{\alpha-\beta}{\alpha+\beta}=
-\frac{V\alpha\beta}{1-S\alpha\beta^{\prime}}
$$
provided $\alpha$ and $\beta$ be unit-vectors. If these conditions are not 
fulfilled, what are the true values ? 

{\bf 3}. Show that, whatever quaternion $r$ may be, the expression 
$$\alpha r+r\beta$$
in which $\alpha$ and $\beta$ are any two unit- vectors, is reducible to the 
form 
$$l(\alpha+\beta)+m(\alpha\beta-1)$$
where $l$ and $m$ are scalars. 

{\bf 4}. If $Tp=T\alpha=T\beta=1$, and $S.\alpha\beta\rho=0$
show by direct transformations that 
$$S.U(\rho-\alpha)U(\rho-\beta)=\pm\sqrt{\frac{1}{2}(1-S\alpha\beta)}$$
Interpret this theorem geometrically. 

{\bf 5}. If $S\alpha\beta=0$, $T\alpha=T\beta=1$, show that
$$
(1+\alpha^{m})\beta=
2\cos\frac{m\pi}{4}\alpha^{\frac{m}{2}}\beta=
2S\alpha^{\frac{m}{2}}.\alpha^{\frac{m}{2}}\beta
$$

{\bf 6}. Put in its simplest form the equation 
$$
\rho S.V\alpha\beta V\beta\gamma V\gamma\alpha=
aV.V\gamma\alpha V\alpha\beta+
bV.V\alpha\beta V\beta\gamma+
cV.V\beta\gamma V\gamma\alpha
$$
and show that 
$$a=S.\beta\gamma\rho,\;\;\;\textrm{\&c.}$$

{\bf 7}. Show that any quaternion may in general, in one way only, 
be expressed as a homogeneous linear function of four given 
quaternions. Point out the nature of the exceptional cases. Also 
find the simplest form in which any quaternion may generally be 
expressed in terms of two given quaternions. 

{\bf 8}. Prove the following theorems, and exhibit them as properties 
of determinants : 

\begin{itemize}
\item[(a)] $S.(\alpha+\beta)(\beta+\gamma)(\gamma+\alpha)=
2S.\alpha\beta\gamma$
\item[(b)]$S.V\alpha\beta V\beta\gamma V\gamma\alpha=
-(S.\alpha\beta\gamma)^2$
\item[(c)]$S.V(\alpha+\beta)(\beta+\gamma)V(\beta+\gamma)(\gamma+\alpha)
V(\gamma+\alpha)(\alpha+\beta)=-4(S.\alpha\beta\gamma)^2$
\item[(d)]$S.V(V\alpha\beta V\beta\gamma)V(V\beta\gamma V\gamma\alpha)
V(V\gamma\alpha V\alpha\beta)=-(S.\alpha\beta\gamma)^4$
\item[(e)]$S.\delta\epsilon\zeta=-16(S.\alpha\beta\gamma)^4$\\
where 
$$\delta=V(V(\alpha+\beta)(\beta+\gamma)V(\beta+\gamma)(\gamma+\alpha))$$
$$\epsilon=V(V(\beta+\gamma)(\gamma+\alpha)V(\gamma+\alpha)(\alpha+\beta))$$
$$\zeta=V(V(\gamma+\alpha)(\alpha+\beta)V(\alpha+\beta)(\beta+\gamma))$$
\end{itemize}

{\bf 9}. Prove the common formula for the product of two determinants 
of the third order in the form 
$$
S.\alpha\beta\gamma S.\alpha_1\beta_1\gamma_1=
\left|
\begin{array}{ccc}
S\alpha\alpha_1 & S\beta\alpha_1 & S\gamma\alpha_1\\
S\alpha\beta_1  & S\beta\beta_1  & S\gamma\beta_1\\
S\alpha\gamma_1 & S\beta\gamma_1 & S\gamma\gamma_1
\end{array}
\right|
$$

{\bf 10}. Show that, whatever be the eight vectors involved, 
$$
\left|
\begin{array}{cccc}
S\alpha\alpha_1 & S\alpha\beta_1 & S\alpha\gamma_1 & S\alpha\delta_1\\
S\beta\alpha_1  & S\beta\beta_1  & S\beta\gamma_1  & S\beta\delta_1\\
S\gamma\alpha_1 & S\gamma\beta_1 & S\gamma\gamma_1 & S\gamma\delta_1\\
S\delta\alpha_1 & S\delta\beta_1 & S\delta\gamma_1 & S\delta\delta_1
\end{array}
\right|
=S.\alpha\beta\gamma S. \beta_1\gamma_1\delta_1S\alpha_1(\delta-\delta)=0
$$

If the single term $S\alpha\alpha_1$, be changed to $S\alpha_0\alpha_1$,
the value of the determinant is 
$$S.\beta\gamma\delta S.\beta_1\gamma_1\delta_1 S\alpha_1(\alpha_0-\alpha)$$

State these as propositions in spherical trigonometry. 

Form the corresponding null determinant for any two groups 
of five quaternions : and give its geometrical interpretation. 

{\bf 11}. If, in \S 102, $\alpha$, $\beta$, $\gamma$ be three mutually
perpendicular vectors, can anything be predicated as to $\alpha_1$,
$\beta_1$, $\gamma_1$?  If $\alpha$, $\beta$, $\gamma$ be rectangular
unit-vectors, what of $\alpha_1$, $\beta_1$, $\gamma_1$?

{\bf 12}. If $\alpha$, $\beta$, $\gamma$, $\alpha^{\prime}$, $\beta^{\prime}$,
$\gamma^{\prime}$ be two sets of rectangular unit-vectors, show that 
$$
S\alpha\alpha^{\prime}=
S\gamma\beta^{\prime}S\beta\gamma^{\prime}=
S\beta\beta^{\prime}S\gamma\gamma^{\prime}\;\;\;\textrm{\&c. \&c.}
$$

{\bf 13}. The lines bisecting pairs of opposite sides of a quadrilateral 
(plane or gauche) are perpendicular to each other when the 
diagonals of the quadrilateral are equal. 

{\bf 14}. Show that 
\begin{itemize}
\item [(a)]$S.q^2=2S^2q-T^2q$
\item [(b)]$S.q^3=S^3q-3SqT^2Vq$
\item [(c)]$\alpha^2\beta^2\gamma^2+S^2.\alpha\beta\gamma=
V^2.\alpha\beta\gamma$
\item [(d)]$S(V.\alpha\beta\gamma V.\beta\gamma\alpha V.\gamma\alpha\beta)=
4S\alpha\beta S\beta\gamma S\gamma\alpha S.\alpha\beta\gamma$
\item [(e)]$V.q^3=(2S^2q-T^2Vq)Vq$
\item [(f)]$qUVq^{-1}=-Sq.UVq+TVq$
\end{itemize}

and interpret each as a formula in plane or spherical trigonometry. 

{\bf 15}. If $q$ be an undetermined quaternion, what loci are represented by 
\begin{itemize}
\item[(a)]$(q\alpha^{-1})^2=-a^2$
\item[(b)]$(q\alpha^{-1})^4=a^4$
\item[(c)]$S.(q-\alpha)^2=a^2$
\end{itemize}
where $a$ is any given scalar and $\alpha$ any given vector ? 

{\bf 16}. If $q$ be any quaternion, show that the equation 
$$Q^2=q^2$$
is satisfied, not alone by $Q = \pm q$, but also by 
$$Q=\pm \sqrt{-1}(Sq.UVq-TVq)$$

\begin{flushright}
(Hamilton, {\sl Lectures}, p. 673.)
\end{flushright}

{\bf 17}. Wherein consists the difference between the two equations 
$$
T^2\frac{\rho}{\alpha}=1\;\;\;\textrm{and}\;\;\;
\left(\frac{\rho}{\alpha}\right)^2=-1
$$

What is the full interpretation of each, $\alpha$ being a given, and p an 
undetermined, vector? 

{\bf 18}. Find the {\sl full} consequences of each of the following 
groups of equations, as regards both the unknown vector $\rho$ and 
the given vectors $\alpha$, $\beta$, $\gamma$:
$$
\begin{array}{crcrcr}
 & S.\alpha\beta\rho=0 & & S\alpha\rho=0 & & S\alpha\rho=0\\
(a) &  & (b) & S.\alpha\beta\rho=0 & (c) & S.\alpha\beta\rho=0\\
  & S.\beta\gamma\rho=0 & & S\beta\rho=0 & & S.\alpha\beta\gamma\rho=0
\end{array}
$$

{\bf 19}. From \S\S 74, 110, show that, if $\epsilon$ 
be any unit-vector, and $m$ any scalar, 
$$\epsilon^{m}=\cos\frac{m\pi}{2}+\epsilon\sin\frac{m\pi}{2}$$
Hence show that if $\alpha$, $\beta$, $\gamma$ 
be radii drawn to the corners of a triangle on the unit-sphere, 
whose spherical excess is $m$ right angles, 
$$
\frac{\alpha+\beta}{\beta+\gamma}.
\frac{\gamma+\alpha}{\alpha+\beta}.
\frac{\beta+\gamma}{\gamma+\alpha}=
\alpha^m
$$
Also that, if $A$, $B$, $C$ be the angles of the triangle, we have 
$$
\gamma^{\frac{2C}{\pi}}
\beta^{\frac{2B}{\pi}}
\alpha^{\frac{2A}{\pi}}
=-1
$$

{\bf 20}. Show that for any three vectors $\alpha$, $\beta$, $\gamma$ we have
$$
(U\alpha\beta)^2+(U\beta\gamma)^2+(U\alpha\gamma)^2+(U.\alpha\beta\gamma)^2+
4U\alpha\gamma .SU\alpha\beta SU\beta\gamma=-2
$$

\begin{flushright}
(Hamilton, {\sl Elements}, p. 388.)
\end{flushright}

{\bf 21}. If $a_1$, $a_2$, $a_3$, $x$ be any four scalars, 
and $\rho_1$, $\rho_2$, $\rho_3$ any three vectors, show that 
$$
(S.\rho_1\rho_2\rho_3)^2+
(\sum.a_1V\rho_2\rho_3)^2+
x^2(\sum V\rho_1\rho_2)^2-
$$
$$
x^2(\sum.a_1(\rho_2-\rho_3))^2
+2\prod(x^2+S\rho_1\rho_2+a_1a_2)
$$
$$
=2\prod(x^2+\rho^2)+
2\prod a^2+
$$
$$
\sum\{(x^2+a_1^2+\rho_1^2)((V\rho_2\rho_3)^2+
2a_2a_3(x^2+S\rho_2\rho_3)-x^2(\rho_2-\rho_3)^2)\}
$$
where $\displaystyle \prod a^2=a_1^2a_2^2a_3^2$

Verify this formula by a simple process in the particular case 
$$a_1=a_2=a_3=x=0$$

\begin{flushright}
({\sl Ibid})
\end{flushright}

{\bf 22}. Eliminate $p$ from the equations 
$$V.\beta\rho\alpha\rho=0,\;\;\;S\gamma\rho=0$$
and state the problem and its solution in a geometrical form. 

{\bf 23}. If $p$, $q$, $r$, $s$ be four versors, such that 
$$qp=-sr=\alpha$$
$$rq=-ps=\beta$$
where $\alpha$ and $\beta$ are unit-vectors; show that 
$$S(V.VsVqV.VrVp)=0$$
Interpret this as a property of a spherical quadrilateral. 

{\bf 24}. Show that, if $pq$, $rs$, $pr$, and $qs$ be vectors, we have 
$$S(V.VpVsV.VqVr)=0$$

{\bf 25}. If $\alpha$, $\beta$, $\gamma$ be unit-vectors, 
$$
V\beta\gamma S.\alpha\beta\gamma=
-\alpha(1-S^2\beta\gamma)-
\beta(S\alpha\gamma S\beta r + S\alpha\beta)-
\gamma(S\alpha\beta S\beta\gamma+S\alpha\gamma)
$$

{\bf 26}. If $i$, $j$, $k$, $i^{\prime}$, $j^{\prime}$, $k^{\prime}$,
be two sets of rectangular unit-vectors, show that 
$$
\begin{array}{rcl}
S.Vii^{\prime}Vjj^{\prime}Vkk^{\prime}&=&(Sij^{\prime})^2-(Sji^{\prime})^2\\
                       &=&(Sjk^{\prime})^2-(Skj^{\prime})^2=\textrm{\&c.}
\end{array}
$$
and find the values of the vector of the same product. 

{\bf 27}. If $\alpha$, $\beta$, $\gamma$
be a rectangular unit-vector system, show that, 
whatever be $\lambda$, $\mu$, $\nu$
$$\lambda S^2i\alpha +\mu S^2j\gamma +\nu S^2k\beta$$
$$\lambda S^2k\gamma +\mu S^2i\beta  +\nu S^2j\alpha$$
and
$$\lambda S^2j\beta  +\mu S^2k\alpha +\nu S^2i\gamma$$
are coplanar vectors. What is the connection between this and 
the result of the preceding example ? 

\vfill
\newpage
\section{Axiom Examples}
The basic operation for creating quaternions is {\bf quatern}.
This is a quaternion over the rational numbers.
\boxer{4.6in}{
\spadcommand{q:=quatern(2/11,-8,3/4,1)}
$$
{2 \over {11}} -{8 \  i}+{{3 \over 4} \  j}+k 
$$
\returnType{Type: Quaternion Fraction Integer}
}

This is a quaternion over the integers.
\boxer{4.6in}{
\spadcommand{r:=quatern(1,2,3,4)}
$$
1+{2 \  i}+{3 \  j}+{4 \  k} 
$$
\returnType{Type: Quaternion Integer}
}

We can also construct quaternions with complex components.
First we construct a complex number.
\boxer{4.6in}{
\spadcommand{b:=complex(3,4)}
$$
3+{4 \  i} 
$$
\returnType{Type: Complex Integer}
}
and then we use it as a component in a quaternion.
\boxer{4.6in}{
\spadcommand{s:=quatern(3,1/7,b,2)}
$$
3+{{1 \over 7} \  i}+{{\left( 3+{4 \  i} \right)}\  j}+{2 \  k} 
$$
\returnType{Type: Quaternion Complex Fraction Integer}
}
Notice that the $i$ component of the complex number has no
relation to the $i$ component of the quaternion even though
they use the same symbol by convention.

The four parts of a quaternion are the real part, the $i$ imaginary 
part, the $j$ imaginary part, and the $k$ imaginary part. The
{\bf real} function returns the real part.
\boxer{4.6in}{
\spadcommand{real q}
$$
2 \over {11} 
$$
\returnType{Type: Fraction Integer}
}

The {\bf imagI} function returns the $i$ imaginary part.
\boxer{4.6in}{
\spadcommand{imagI q}
$$
-8 
$$
\returnType{Type: Fraction Integer}
}

The {\bf imagJ} function returns the $j$ imaginary part.
\boxer{4.6in}{
\spadcommand{imagJ q}
$$
3 \over 4 
$$
\returnType{Type: Fraction Integer}
}

The {\bf imagK} function returns the $k$ imaginary part.
\boxer{4.6in}{
\spadcommand{imagK q}
$$
1
$$
\returnType{Type: Fraction Integer}
}

Quaternions satisfy a very fundamental relationship between the parts, 
namely that
$$i^2 = j^2 = k^2 = ijk = -1$$. This is similar to the requirement
in complex numbers of the form $a+bi$ that $i^2 = -1$.

The set of quaternions is denoted by $\mathbb{H}$, whereas the integers
are denoted by $\mathbb{Z}$ and the complex numbers by $\mathbb{C}$.

Quaternions are not commutative which means that in general
$$AB \ne BA$$
for any two quaternions, A and B. So, for instance,
\boxer{4.6in}{
\spadcommand{q*r}
$$
{{437} \over {44}} -{{{84} \over {11}} \  i}+{{{1553} \over {44}} \  j} 
-{{{523} \over {22}} \  k} 
$$
\returnType{Type: Quaternion Fraction Integer}
}

\boxer{4.6in}{
\spadcommand{r*q}
$$
{{437} \over {44}} -{{{84} \over {11}} \  i} -{{{1439} \over {44}} \  
j}+{{{599} \over {22}} \  k} 
$$
\returnType{Type: Quaternion Fraction Integer}
}
and these are clearly not equal.

Complex $2\times2$ matrices form an alternate, equivalent 
representation of quaternions. These matrices have the form:
$$
\left[
\begin{array}{cc}
u & v \\ 
-\overline{v} & \overline{u} 
\end{array}
\right]
$$
=
$$
\left[
\begin{array}{cc}
a+bi & c+di \\ 
-c+di & a-bi
\end{array}
\right]
$$
where $u$ and $v$ are complex, $\overline{u}$ is complex conjugate
of $u$, $\overline{z}$ is the complex conjugate of $z$, and a,b,c,
and d are real.

Within the quaternion each component operator represents a basis
element in $\mathbb{R}^4$ thus:
$$
1 =
\left[
\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 1\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{array}
\right]
$$

$$
i =
\left[
\begin{array}{cccc}
0 & 1 & 0 & 0\\
-1 & 0 & 0 & 1\\
0 & 0 & 0 & 1\\
0 & 0 & -1 & 0
\end{array}
\right]
$$

\chapter{Clifford Algebra}

This is quoted from John Fletcher's web page \cite{Flet09} (with permission).

The theory of Clifford Algebra includes a statement that each Clifford
Algebra is isomorphic to a matrix representation. Several authors
discuss this and in particular Ablamowicz \cite{Abla98} gives examples of
derivation of the matrix representation. A matrix will itself satisfy
the characteristic polynomial equation obeyed by its own
eigenvalues. This relationship can be used to calculate the inverse of
a matrix from powers of the matrix itself. It is demonstrated that the
matrix basis of a Clifford number can be used to calculate the inverse
of a Clifford number using the characteristic equation of the matrix
and powers of the Clifford number. Examples are given for the algebras
Clifford(2), Clifford(3) and Clifford(2,2).

\section{Introduction}

Introductory texts on Clifford algebra state that for any chosen
Clifford Algebra there is a matrix representation which is equivalent.
Several authors discuss this in more detail and in particular,
Ablamowicz \cite{Abla98} 
shows that the matrices can be derived for each algebra
from a choice of idempotent, a member of the algebra which when
squared gives itself.  The idea of this paper is that any matrix obeys
the characteristic equation of its own eigenvalues, and that therefore
the equivalent Clifford number will also obey the same characteristic
equation.  This relationship can be exploited to calculate the inverse
of a Clifford number. This result can be used symbolically to find the
general form of the inverse in a particular algebra, and also in
numerical work to calculate the inverse of a particular member.  This
latter approach needs the knowledge of the matrices.  Ablamowicz has
provided a method for generating them in the form of a Maple
implementation. This knowledge is not believed to be new, but the
theory is distributed in the literature and the purpose of this paper
is to make it clear.  The examples have been first developed using a
system of symbolic algebra described in another paper by this
author \cite{Flet01}.  

\section{Clifford Basis Matrix Theory}

The theory of the matrix basis is discussed extensively by
Ablamowicz.  This theory will be illustrated here following the
notation of Ablamowicz by reference to Clifford(2) algebra and can
be applied to other Clifford Algebras. For most Clifford algebras
there is at least one primitive idempotent, such that it squares to
itself. For Clifford (2), which has two basis members $e_1$ and $e_2$, one
such idempotent involves only one of the basis members, $e_1$, i.e.

\[f_1 = f = \frac{1}{2} (1 + e_1)\]

If the idempotent is mutiplied by the other basis function $e_2$, other
functions can be generated:

\[f_2 = e_2 f = \left(\frac{1}{2}-\frac{1}{2}e_1\right)e_2\]

\[f_3 = f e_2 = \left(\frac{1}{2}+\frac{1}{2}e_1\right)e_2\]

\[f_4 = e_2 f e_2 = \frac{1}{2}-\frac{1}{2}e_1\]

Note that $fe_22f = 0$.  These four functions provide a means of
representing any member of the space, so that if a general member c is
given in terms of the basis members of the algebra

\[ c = a_0 + a_1e_1 + a_2e_2 + a_3e_1e_2\]

it can also be represented by a series of terms in the idempotent and
the other functions.

\[
\begin{array}{rcl}
c&=&a_{11}f_1 + a_{21}f_2 + a_{12}f_3 + a_{22}f_4\\
&&\\
 &=&\frac{1}{2}a_{11} + \frac{1}{2}a_{11}e_1 + \frac{1}{2}a_{21}e_2
-\frac{1}{2}a_{21}e_1e_2 +\\
&&\\
&&\frac{1}{2}a_{12}e_2 + \frac{1}{2}a_{12}e_1e_2 + \frac{1}{2}a_{22} 
-\frac{1}{2}a_{22}e_1
\end{array}
\]


Equating coefficients it is clear that the following equations apply.
\[
\begin{array}{rcl}
a_0 &=& \frac{1}{2}a_{11} + \frac{1}{2}a_{22}\\
&&\\
a_1 &=& \frac{1}{2}a_{11} - \frac{1}{2}a_{22}\\
&&\\
a_2 &=& \frac{1}{2}a_{12} + \frac{1}{2}a_{21}\\
&&\\
a_3 &=& \frac{1}{2}a_{12} - \frac{1}{2}a_{21}
\end{array}
\]

The reverse equations can be recovered by multiplying the two forms of
c by different combinations of the functions $f_1$, $f_2$ and $f_3$. 
The equation

\[
\begin{array}{rcl}
f_1cf_1 &=& f_1(a_{11}f_1 + a_{21}f_2 + a_{12}f_3 + a_{22}f_4)f_1\\
&&\\
        &=& f_1(a_0 + a_1e_1 + a_2e_2 + a_3e_1e_2)f_1
\end{array}
\]

reduces to the equation

\[a_{11}f = (a_0 + a_1)f\]

and similar equations can be deduced from other combinations of the
functions as follows.

\[
\begin{array}{rcl}
f_1cf_2 : a_{12}f &=& (a_2 + a_3)f\\
&&\\
f_2cf_1 : a_{21}f &=& (a_2 - a_3)f\\
&&\\
f_3cf_2 : a_{22}f &=& (a_0 - a_1)f
\end{array}
\]

If a matrix is defined as

\[
A = \left(
\begin{array}{cc}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{array}
\right)
\]

so that

\[
Af = \left(
\begin{array}{cc}
a_{11}f & a_{12}f \\
a_{21}f & a_{22}f
\end{array}
\right) 
=
\left(
\begin{array}{cc}
a_0+a_1 & a_2+a_3 \\
a_2-a_3 & a_0-a_1
\end{array}
\right) f
\]

then the expression

\[
\left(
\begin{array}{cc}
1 & e_2
\end{array}
\right)
\left(
\begin{array}{cc}
a_{11}f & a_{12}f \\
a_{21}f & a_{22}f
\end{array}
\right)
\left(
\begin{array}{c}
1\\
e_2
\end{array}
\right)
=
a_{11}f_1 + a_{21}f_2 + a_{12}f_3 + a_{22}f_4 = c
\]

generates the general Clifford object c.  All that remains to form the
basis matrices is to make c each basis member in turn, and named as
shown.

\[
\begin{array}{lrclcr}
c=1: & Af & = & 
\left(
\begin{array}{cc}
f & 0\\
0 & f
\end{array}
\right)
& = & E_0f\\
c=e_1 & Af & = &
\left(
\begin{array}{cc}
f & 0\\
0 & -f
\end{array}
\right)
& = & E_1f\\
c=e_2 & Af & = &
\left(
\begin{array}{cc}
0 & f\\
f & 0
\end{array}
\right)
& = & E_2f\\
c=e_1e_2 & Af & = &
\left(
\begin{array}{cc}
0 & f\\
-f & 0
\end{array}
\right)
& = & E_{12}f
\end{array}
\]

These are the usual basis matrices for Clifford (2) except that they
are multiplied by the idempotent.

This approach provides an explanation for the basis matrices in terms
only of the Clifford Algebra itself.  They are the matrix
representation of the basis objects of the algebra in terms of an
idempotent and an associated vector of basis functions.  This has been
shown for Clifford (2) and it can be extended to other algebras once
the idempotent and the vector of basis functions have been identified.
This has been done in many cases by Ablamowicz.  This will now be
developed to show how the inverse of a Clifford number can be obtained
from the matrix representation.  

\section{Calculation of the inverse of a Clifford number}

The matrix basis demonstrated above can be used to calculate the
inverse of a Clifford number.  In simple cases this can be used to
obtain an algebraic formulation.  For other cases the algebra is too
complex to be clear, but the method can still be used to obtain the
numerical value of the inverse.  To apply the method it is necessary
to know a basis matrix representation of the algebra being used.

The idea of the method is that the matrix representation will have a
characteristic polynomial obeyed by the eigenvalues of the matrix and
also by the matrix itself.  There may also be a minimal polynomial
which is a factor of the characteristic polynomial, which will have
also be satisfied by the matrix.  It is clear from the proceding
section that if $A$ is a matrix representation of $c$ in a Clifford
Algebra then if some function $f(A) = 0$ then the corresponding Clifford
function $f(c) = 0$ must also be zero.  In particular if $f(A) = 0$ is the
characteristic or minimal polynomial of $A$, then $f(c) = 0$ implies that
$c$ also satisfies the same polynomial.  Then if the inverse of the
Clifford number, $c^{-1}$ is to be found, then 

\[c^{-1}f(c)=0\]

provides a relationship for $c^{-1}$ in terms of multiples a small number
of low powers of $c$, with the maximum power one less than the order of
the polynomial. The method suceeds unless the constant term in the
polynomial is zero, which means that the inverse does not exist.  For
cases where the basis matrices are of order two, the inverse will be
shown to be a linear function of $c$.

The method can be summed up as follows.
\begin{enumerate}
\item Find the matrix basis of the Clifford algebra.
\item Find the matrix representation of the Clifford number whose
inverse is required.
\item Compute the characteristic or minimal polynomial.
\item Check for the existence of the inverse.
\item Compute the inverse using the coefficients from the polynomial.
\end{enumerate}

Step 1 need only be done once for any Clifford algebra, and this can
be done using the method in the previous section, where needed.

Step 2 is trivially a matter of accumulation of the correct multiples
of the matrices.

Step 3 may involve the use of a computer algebra system to find the
coefficients of the polynomial, if the matrix size is at all large.

Steps 4 and 5 are then easy once the coefficients are known.

The method will now be demonstrated using some examples.

\subsection{Example 1: Clifford (2)}

In this case the matrix basis for a member of the Clifford algebra

\[c = a_0 + a_1e_1 + a_2e_2 + a_3e_1e_2\]

was developed in the previous section as

\[A=\left(
\begin{array}{cc}
a_0+a_1 & a_2+a_3\\
a_2-a_3 & a_0-a_1
\end{array}
\right)\]

This matrix has the characteristic polynomial

\[X^2 - 2Xa_0 + a^2_0 - a^2_1 - a^2_2 + a^2_3 = 0\]

and therefore

\[X^{-1}(X^2 - 2Xa_0 + a^2_0 - a^2_1 - a^2_2 + a^2_3) = 0\]

and

\[X^{-1} = (2a_0 - X)/(a^2_0 - a^2_1 - a^2_2 + a^2_3) = 0\]

which provides a general solution to the inverse in this algebra.

\[c^{-1} = (2a_0 - c)/(a^2_0 - a^2_1 - a^2_2 + a^2_3) = 0\]

\subsection{Example 2: Clifford (3)}

A set of basis matrices for Clifford (3) as given by Abalmowicz and
deduced are

\[
\begin{array}{cc}
E_0 =
\left(
\begin{array}{cc}
1&0\\
0&1
\end{array}\right) &
E_1 =
\left(
\begin{array}{cc}
1&0\\
0&-1
\end{array}\right) \\
E_2 =
\left(
\begin{array}{cc}
0&1\\
1&0
\end{array}\right) &
E_3 =
\left(
\begin{array}{cc}
0&-j\\
j&0
\end{array}\right) \\
E_1E_2 =
\left(
\begin{array}{cc}
0&1\\
-1&0
\end{array}\right) &
E_1E_3 =
\left(
\begin{array}{cc}
0&-j\\
-j&0
\end{array}\right) \\
E_2E_3 =
\left(
\begin{array}{cc}
j&0\\
0&-j
\end{array}\right) &
E_1E_2E_3 =
\left(
\begin{array}{cc}
j&0\\
0&j
\end{array}\right) \\
\end{array}
\]

for the idempotent 

\[f = \frac{(1 + e_1)}{2}, {\rm\ where\ } j^2 = -1.\]

The general member of the algebra

\[c_3 = a_0 +a_1e_1 + a_2e_2 + a_3e_3 + 
a_{12}e_1e_2 + a_{13}e_1e_3 + a_{23}e_2e_3 + a_{123}e_1e_2e_3\]

has the matrix representation

\[
\begin{array}{rcl}
A_3&=&a_0E_0 + a_1E_1 + a_2E_2 +a_3E_3 + a_{12}E_1E_2\\
&& +a_{13}E_1E_3 + a_{23}E_2E_3 + a_{123}E_1E_2E_3\\
&&\\
&=&\left(
\begin{array}{cc}
a_0 + a_1 + ja_{23} + ja_{123}& a_2 -ja_3 +a_{12} -ja_{13}\\
a_2 + ja_3- a_{12}- ja_{13}& a_0- a_1- ja_{23} + ja_{123}
\end{array}
\right)
\end{array}
\]

This has the characteristic polynomial

\[
\begin{array}{rl}
&a^2_0-a^2_1-a^2_2-a^2_3+a^2_{12}+a^2_{13}+a^2_{23}-a^2_{123}\\
&\\
+&2j(a_0a_{123}-a_1a_{23}-a_{12}a_3+a_{13}a_2)\\
&\\
-&2(a_0+ja_{123})X + X^2=0
\end{array}
\]

and the expression for the inverse is

\[
\begin{array}{rcl}
X^{-1}&=&(2a_0 + 2ja_{123} -X) /\\
&&(a^2_0-a^2_1-a^2_2-a^2_3+a^2_{12}+a^2_{13}+a^2_{23}-a^2_{123}\\
&&+2j(a_0a_{123}-a_1a_{23}-a_{12}a_3+a_{13}a_2))
\end{array}
\]

Complex terms arise in two cases,

\[a_{123} \ne 0\]

and

\[(a_0a_{123}-a_1a_{23}-a_{12}a_3+a_{13}a_2) \ne 0\]

Two simple cases have real minumum polynomials:

Zero and first grade terms only:

\[
\begin{array}{rcl}
A_1&=&a_0E_0 + a_1E_1 + a_2E_2 + a_3E_3\\
&=&\left(
\begin{array}{cc}
a_0+a_1 & a_2-ja_3\\
a_2+ja_3 & a_0-a_1
\end{array}
\right)
\end{array}
\]

which has the minimum polynomial

\[a^2_0-a^2_1-a^2_2-a^2_3-2a_0X+X^2=0\]

which gives

\[X^{-1} = (2a_0- X) / (a^2_0- a^2_1- a^2_2 - a^2_3)\]

Zero and second grade terms only (ie. the even subspace).

\[
\begin{array}{rcl}
A_2&=&a_0E_0 + a_{12}E_1E_2 + a_{13}E_1E_3 + a_{23}E_2E_3\\
&&\left(
\begin{array}{cc}
a_0+ja_{23}     & a_{12}-ja_{13}\\
-a_{12}-ja_{13} & a_0-ja_{23}
\end{array}
\right)
\end{array}
\]

which has minimum polynomial

\[a^2_0+a^2_{23}+a^2_{12}+a^2_{13}-2a_0X+X^2 = 0\]

giving

\[X^{-1} = (2a_0- X) /(a^2_0 + a^2_{23} + a^2_{12} + a^2_{13})\]

This provides a general solution for the inverse together with two
simple cases of wide usefulness.

\subsection{Example 3: Clifford (2,2)}

The following basis matrices are given by Ablamowicz \cite{Abla98}

\[
\begin{array}{cc}
E_1=\left(
\begin{array}{cccc}
0 & 1 & 0 & 0\\
1 & 0 & 0 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 1 & 0
\end{array}
\right)&
E_2=\left(
\begin{array}{cccc}
0 & 0 & 1 & 0\\
0 & 0 & 0 & -1\\
1 & 0 & 0 & 0\\
0 & -1 & 0 & 0
\end{array}
\right)\\
E_3=\left(
\begin{array}{cccc}
0 & -1 & 0 & 0\\
1 & 0 & 0 & 0\\
0 & 0 & 0 & -1\\
0 & 0 & 1 & 0
\end{array}
\right)&
E_4=\left(
\begin{array}{cccc}
0 & 0 & -1 & 0\\
0 & 0 & 0 & 1\\
1 & 0 & 0 & 0\\
0 & -1 & 0 & 0
\end{array}
\right)
\end{array}
\]

for the idempotent 
\[f = \frac{(1 +e_1e_3) (1+ e_1e_3)}{4}.\]

 Note that this implies that the order of the basis members is such
that $e_1$ and $e_2$ have square $+1$ and $e_3$ and $e_4$ have square
$-1$. Other orderings are used by other authors. The remaining basis
matrices can be deduced to be as follows.

Second Grade members

\[
\begin{array}{cc}
E_1E_2 = \left(
\begin{array}{cccc}
0 & 0 & 0 & -1\\
0 & 0 & 1 & 0\\
0 & -1 & 0 & 0\\
1 & 0 & 0 & 0
\end{array}\right)&
E_1E_3 = \left(
\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & -1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & -1
\end{array}\right)\\
E_1E_4 = \left(
\begin{array}{cccc}
0 & 0 & 0 & 1\\
0 & 0 & -1 & 0\\
0 & -1 & 0 & 0\\
1 & 0 & 0 & 0
\end{array}\right)&
E_2E_3 = \left(
\begin{array}{cccc}
0 & 0 & 0 & -1\\
0 & 0 & -1 & 0\\
0 & -1 & 0 & 0\\
-1 & 0 & 0 & 0
\end{array}\right)\\
E_2E_4 = \left(
\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & -1 & 0\\
0 & 0 & 0 & -1
\end{array}\right)&
E_3E_4 = \left(
\begin{array}{cccc}
0 & 0 & 0 & -1\\
0 & 0 & -1 & 0\\
0 & 1 & 0 & 0\\
1 & 0 & 0 & 0
\end{array}\right)\\
\end{array}
\]

Third grade members

\[
\begin{array}{cc}
E_1E_2E_3 = \left(
\begin{array}{cccc}
0 & 0 & -1 & 0\\
0 & 0 & 0 & -1\\
-1 & 0 & 0 & 0\\
0 & -1 & 0 & 0
\end{array}\right)&
E_1E_2E_4 = \left(
\begin{array}{cccc}
0 & 1 & 0 & 0\\
1 & 0 & 0 & 0\\
0 & 0 & 0 & -1\\
0 & 0 & -1 & 0
\end{array}\right)\\
E_1E_3E_4 = \left(
\begin{array}{cccc}
0 & 0 & -1 & 0\\
0 & 0 & 0 & -1\\
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0
\end{array}\right)&
E_2E_3E_4 = \left(
\begin{array}{cccc}
0 & 1 & 0 & 0\\
-1 & 0 & 0 & 0\\
0 & 0 & 0 & -1\\
0 & 0 & 1 & 0
\end{array}\right)
\end{array}
\]

Fourth grade member

\[
E_1E_2E_3E_4 = \left(
\begin{array}{cccc}
-1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & -1
\end{array}
\right)
\]

Zero grade member (identity)

\[
E_0 = \left(
\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{array}
\right)
\]

The general member of the Clifford (2,2) algebra can be written as follows.

\[\begin{array}{rcl}
c_{22}&=& a_0 + a_1e_1 + a_2e_2 + a_3e_3 + a_4e_4 +\\
&&a_{12}e_1e_2+a_{13}e_1e_3+a_{14}e_1e_4+a_{23}e_2e_3+a_{24}e_2e_4+
a_{34}e_3e_4\\
&&+ a_{123}e_1e_2e_3 +a_{124}e_1e_2e_4 +a_{134}e_1e_3e_4 + 
a_{234}e_2e_3e_4 + a_{1234}e_1e_2e_3e_4
\end{array}
\]

This has the following matrix representation.

\[
\left(
\begin{array}{cccc}
a_0+a_{13}+    & a_1-a_3+        & a_2-a_4-        & -a_{12}+a_{14}-\\
a_{24}-a_{1234}& a_{124}+a_{234} & a_{123}-a_{134} & a_{23}-a_{34}\\
&&&\\
a_1+a_3+        & a_0-a_{13}+     & a_{12}-a_{14}- & -a_2+a_4-\\
a_{124}-a_{234} & a_{24}+a_{1234} & a_{23}-a_{34}  & a_{123}-a_{134}\\
&&&\\
a_2+a_4-        & -a_{12}-a_{14}- & a_0+a_{13}-     & a_1-a_3-\\
a_{123}+a_{134} & a_{23}+a_{34}   & a_{24}+a_{1234} & a_{124}-a_{234}\\
&&&\\
a_{12}+a_{14}- & -a_2-a_4-       & a_1+a_3-        & a_0-a_{13}-\\
a_{23}+a_{34}  & a_{123}+a_{134} & a_{124}+a_{234} & a_{24}-a_{1234}
\end{array}
\right)
\]

In this case it is possible to generate the characteristic equation
using computer algebra. However, it is too complex to be of practical
use. Instead here are numerical examples of the use of the method to
calculate the inverse. For the case where

\[n1 = 1+ e_1 + e_2 + e_3 + e_4\]

then the matrix representation is

\[N_1 = E_0 +E_1 + E_2 + E_3 + E_4 = 
\left(
\begin{array}{cccc}
1 & 0 & 0 & 0\\
2 & 1 & 0 & 0\\
2 & 0 & 1 & 0\\
0 & -2 & 2 & 1
\end{array}
\right)
\]

This has the minimum polynomial

\[X^2 - 2X + 1 = 0\]

so that

\[X^{-1} = 2- X\]

and

\[n^{-1}_1= 2 - n_1 = 1 - e_1 - e_2- e_3- e_4\]

For

\[n_2 = 1+ e_1 + e_2 + e_3 + e_4 +e_1e_2\]

the matrix representation is

\[N_2 = I + E_1 + E_2 + E_3 +E_4 + E_1E_2 = 
\left(
\begin{array}{cccc}
1 & 0 & 0 & -1\\
2 & 1 & 1 & 0\\
2 & -1 & 1 & 0\\
1 & -2 & 2 & 1
\end{array}
\right)
\]

This has the minimum polynomial

\[X^4 - 4X^3 + 8X^2 - 8X - 4 = 0\]

so that

\[X^{-1} = \frac{X^3 - 4X^2 + 8X - 8}{4}\]

and

\[n^{-1}_2 = \frac{n^3_2- 4n^2_2 + 8n_2 - 8}{4}\]

This expression can be evaluated easily using a computer algebra
system for Clifford algebra such as described in Fletcher \cite{Flet01}. 
The result is

\[
\begin{array}{rcl}
n^{-1}_2 &=& -0.5 + 0.5e_1 + 0.5e_2 - 0.5e_1e_2 - 0.5e_1e_3\\
&& - 0.5e_1e_4 + 0.5e_2e_3 + 0.5e_2e_4 - 0.5e_1e_2e_3 - 0.5e_1e_2e_4
\end{array}
\]


Note that in some cases the inverse is linear in the original Clifford
number, and in others it is nonlinear.

\subsection{Conclusion}

The paper has demonstrated a method for the calculation of inverses of
Clifford numbers by means of the matrix representation of the
corresponding Clifford algebra.  The method depends upon the
calculation of the basis matrices for the algebra.  This can be done
from an idempotent for the algebra if the matrices are not already
available.  The method provides an easy check on the existence of the
inverse.  For simple systems a general algebraic solution can be found
and for more complex systems the algebra of the inverse can be
generated and evaluated numerically for a particular example, given a
system of computer algebra for Clifford algebra.

\chapter{Package for Algebraic Function Fields}

PAFF is a Package for Algebraic Function Fields in one variable
by Ga\'etan Hach\'e

PAFF is a package written in Axiom and one of its many purpose is to
construct geometric Goppa codes (also called algebraic geometric codes
or AG-codes). This package was written as part of Ga\'etan's doctorate
thesis on ``Effective construction of geometric codes'': this thesis was
done at Inria in Rocquencourt at project CODES and under the direction
of Dominique LeBrigand at Universit Pierre et Marie Curie (Paris
6). Here is a r\'esum\'e of the thesis.

It is well known that the most difficult part in constructing AG-code
is the computation of a basis of the vector space ``L(D)'' where D is a
divisor of the function field of an irreducible curve. To compute such
a basis, PAFF used the Brill-Noether algorithm which was generalized
to any plane curve by D. LeBrigand and J.J. Risler \cite{LeBr88}. In 
\cite{Hach96}
you will find more details about the algorithmic aspect of the
Brill-Noether algorithm. Also, if you prefer, as I do, a strictly
algebraic approach, see \cite{Hach95}. This is the approach I used in my thesis
(\cite{Hach96}) 
and of course this is where you will find complete details about
the implementation of the algorithm. The algebraic approach use the
theory of algebraic function field in one variable : you will find in
\cite{Stic93} a very good introduction to this theory and AG-codes.

It is important to notice that PAFF can be used for most computation
related to the function field of an irreducible plane curve. For
example, you can compute the genus, find all places above all the
singular points, compute the adjunction divisor and of course compute
a basis of the vector space L(D) for any divisor D of the function
field of the curve.

There is also the package PAFFFF which is especially designed to be
used over finite fields. This package is essentially the same as PAFF,
except that the computation are done over ``dynamic extensions'' of the
ground field. For this, I used a simplify version of the notion of
dynamic algebraic closure as proposed by D. Duval \cite{Duva95}.

Example 1

This example compute the genus of the projective plane curve defined by:
\begin{verbatim}
       5    2 3      4
      X  + Y Z  + Y Z  = 0
\end{verbatim}
over the field GF(2).

First we define the field GF(2).
\begin{verbatim}
K:=PF 2
R:=DMP([X,Y,Z],K)
P:=PAFF(K,[X,Y,Z],BLQT)
\end{verbatim}

We defined the polynomial of the curve.
\begin{verbatim}
C:R:=X**5 + Y**2*Z**3+Y*Z**4
\end{verbatim}

We give it to the package PAFF(K,[X,Y,Z]) which was assigned to the
variable $P$.

\begin{verbatim}
setCurve(C)$P
\end{verbatim}

\chapter{Interpolation Formulas}
{\center{\includegraphics[scale=0.80]{ps/lozenge2.eps}}}

The lozenge diagram is a device for showing that a large number of
formulas which appear to be different are really all the same. The
notation for the binomial coefficients
\[C(u+k,n) = \frac{(u+k)(u+k-1)(u+k-2)\cdots{}(u+k-n+1)}{n!}\]
There are $n$ factors in the numerator and $n$ in the denominator.
Viewed as a function of $u$, $C(u+k,n)$ is a polynomial of degree $n$.

The figure above, Hamming \cite{Hamm62}
calls a lozenge diagram. A line starting at
a point on the left edge and following some path across the page
defines an interpolation formula if the following rules are used.
\begin{itemize}
\item[{\bf 1a}] For a left-to-right step, {\sl add}
\item[{\bf 1b}] For a right-to-left, {\sl subtract}
\item[{\bf 2a}] If the {\sl slope} of the step is {\sl positive}, 
use the product of the difference crossed times the factor 
immediately {\sl below}.
\item[{\bf 2b}] If the {\sl slope} of the step is {\sl negative},
use the product of the difference crossed times the factor
immediately {\sl above}
\item[{\bf 3a}] If the step is {\sl horizontal} and passes through a
{\sl difference}, use the product of the difference times the 
{\sl average} of the factors {\sl above} and {\sl below}.
\item[{\bf 3b}] If the step is {\sl horizontal} and passes through a
{\sl factor}, use the product of the factor times the {\sl average}
of the differences {\sl above} and {\sl below}.
\end{itemize}

As an example of rules {\bf 1a} and {\bf 2a}, consider starting at
$y(0)$ and going down to the right. We get, term by term,
\[y(u)=y(0)+C(u,1)\Delta{}y(0)+C(u,2)\Delta^2y(0)+C(u,3)\Delta^3y(0)+\cdots\]
\[=y(0)+u\Delta{}y(0)+\frac{u(u-1)}{2}\Delta^2y(0)+
\frac{u(u-1)(y-2)}{3!}\Delta^3y(0)+\cdots\]
which is Newton's formula.

Had we gone up and to the right, we would have used {\bf 1a} and {\bf 2a}
to get Newton's backward formula:
\[y(u)=y(0)+C(u,1)\Delta{}y(-1)+C(u+1,2)\Delta^2y(-2)+
C(u+2,3)\Delta^3y(-3)+\cdots\]
\[=y(0)+u\Delta{}y(-1)+\frac{(u+1)u}{2}\Delta^2y(-2)+
\frac{(u+2)(u+1)u}{3!}\Delta^3y(-3)+\cdots\]

To get Stirling's formula, we start at $y(0)$ and go horizontally to
the right, using rules {\bf 3a} and {\bf 3b}:
\[y(u)=y(0)
+u\frac{\Delta{}y_0+\Delta{}y_{-1}}{2}
+\frac{C(u+1,2)+C(u,2)}{2}\Delta^2y_{-1}\\
+C(u+1,3)\frac{\Delta^3y_{-2}+\Delta^3y_{-1}}{2}+\cdots\]
\[=y_0+u\frac{\Delta{}y_0+\Delta{}y_{-1}}{2}
+\frac{u^2}{2}\Delta^2{}y_{-1}
+\frac{u(u^2-1)}{3!}\frac{\Delta^3y_{-2}+\Delta^3y_{-1}}{2}+\cdots\]

If we start midway between $y(0)$ and $y(1)$, we get Bessel's formula:
\[y(u)=1\frac{y_0+y_1}{2}+\frac{C(u,1)+C(u-1,1)}{2}\Delta{}y_0
+C(u,2)\frac{\Delta^2y_{-1}+\Delta^2y_0}{2}+\cdots\]
\[=\frac{y_0+y_1}{2}+(u-\frac{1}{2})\Delta{}y_0+
\frac{u(u-1)}{2}\frac{\Delta^2y_{-1}+\Delta^2y_0}{2}+\cdots\]

If we zigzag properly, we can get Gauss' formula for interpolation:
\[y(u)=y_0+u\Delta{}y_0+\frac{u(u-1)}{2}\Delta^2y(-1)+
\frac{u(u^2-1)}{3!}\Delta^3y(-1)+\cdots\]

\chapter[Type Systems]{Type Systems for Computer Algebra by Andreas Weber}

This chapter is based on a PhD thesis by Andreas Weber\cite{Webe93b}.
Changes have been made to integrate it.

We study type systems for computer algebra systems, which frequently
correspond to the ``pragmatically developed'' typing constructs used
in {\sf Axiom}.

A central concept is that of {\em type classes} which correspond to
{\sf Axiom} categories.  We will show that types can be syntactically
described as terms of a regular order-sorted signature if no type
parameters are allowed.  Using results obtained for the functional
programming language {\sf Haskell} we will show that the problem of
{\em type inference} is decidable.  This result still holds if
higher-order functions are present and {\em parametric polymorphism}
is used.  These additional typing constructs are useful for further
extensions of existing computer algebra systems: These typing concepts
can be used to implement category theoretic constructs and there are
many well known constructive interactions between category theory and
algebra.
 
On the one hand we will show that there are well known techniques to
specify many important type classes algebraically, and we will also
show that a formal and algorithmically Feasible treatment of the
interactions of algebraically specified data types and type classes is
possible.  On the other hand we will prove that there are quite
elementary examples arising in computer algebra which need very
``strong'' formalisms to be specified and are thus hard to handle
algorithmically.

We will show that it is necessary to distinguish between types and
elements as parameters of parameterized type classes.  The type
inference problem for the former remains decidable whereas for the
latter it becomes undecidable. We will also show that such a
distinction can be made quite naturally.

Type classes are second-order types.  Although we will show that there
are constructions used in mathematics which imply that type classes
have to become first-order types in order to model the examples
naturally, we will also argue that this does not seem to be the case
in areas currently accessible for an algebra system.  We will only
sketch some systems that have been developed during the last years in
which the concept of type classes as first-order types can be
expressed.  For some of these systems the type inference problem was
proven to be undecidable.

Another fundamental concept for a type system of a computer algebra
system --- at least for the purpose of a user interface --- are {\em
coercions}.  We will show that there are cases which can be modeled by
coercions but not by an ``inheritance mechanism'', i.\,e.\ the concept
of coercions is not only orthogonal to the one of type classes but
also to more general formalisms as are used in object-oriented
languages.  We will define certain classes of coercions and impose
conditions on important classes of coercions which will imply that the
meaning of an expression is independent of the particular coercions
that are used in order to type it.


We shall also impose some conditions on the interaction between
polymorphic operations defined in type classes and coercions that will
yield a unique meaning of an expression independent of the type which
is assigned to it --- if coercions are present there will very
frequently be several possibilities to assign types to expressions.

Often it is not only possible to coerce one type into another but it
will be the case that two types are actually {\em isomorphic}.  We
will show that isomorphic types have properties that cannot be deduced
from the properties of coercions and will shortly discuss other
possibilities to model type isomorphisms.  There are natural examples
of type isomorphisms occurring in the area of computer algebra that
have a ``problematic'' behavior.  So we will prove for a certain
example that the type isomorphisms cannot be captured by a finite set
of coercions by proving that the naturally associated equational
theory is not finitely axiomatizable.

Up to now few results are known that would give a clear dividing line
between classes of coercions which have a decidable type inference
problem and classes for which type inference becomes undecidable.  We
will give a type inference algorithm for some important classes of
coercions.

Other typing constructs which are again quite orthogonal to the
previous ones are those of {\em partial functions} and of {\em types
depending on elements}.  We will link the treatment of {\em partial
functions} in {\sf Axiom} to the one used in order-sorted algebras and
will show some problems which arise if a seemingly more expressive
solution were used.  There are important cases in which {\em types
depending on elements} arise naturally.  We will show that not only
type inference but even type checking is undecidable for relevant
cases occurring in computer algebra.

Types have played an extremely important role in the development and
study of programming languages.  They have become so prevalent that
type theory is now recognized as an area of its own within computer
science.  The benefits which can be derived from the presence of types
in a language are manifold.  Through type checking many errors can be
caught before a program is ever run, thus leading to more reliable
programs.  Types form also an expressive basis for module systems,
since they prescribe a machine-verifiable interface for the code
encapsulated within a module.  Furthermore, they may be used to
improve performance of code generated by a compiler.


However, most computer algebra systems are based on untyped languages.
Nevertheless, at least in the description and specification of many
algorithms a terminology is used which can be seen as attributing
``types'' to the computational objects.  In {\sf Maple~V}
\cite{Char91} and in {\sf Mathematica} \cite{Wolf91}, which
are both based on untyped languages, it is even possible to attach
``tags'' to data structures which describe types corresponding to the
mathematical structures the data are supposed to represent.

In the area of computer algebra, the problem of finding appropriate
type systems which are supported by the language is that on the one
hand, the type system has to consider the requirements of a computer
system and on the other, it should allow for the mathematical
structures a system is dealing with to have corresponding types.

The development of {\sf Axiom} \cite{Jenk84b}, \cite{Suto87},
\cite{Jenk92} is certainly a break-through since the language
itself is typed with types corresponding to the mathematical
structures the system deals with.

However, the typing constructs used in {\sf Axiom} have been
``pragmatically developed.''  Some are not even formally defined and
only very few studies on formal properties of such a system have been
undertaken.  Even if other approaches to a type system in this area
are considered --- such as the ``object-oriented'' one used for {\sf
VIEWS} \cite{Abda86} --- we have found relatively few formal
studies of type systems suited for the purpose of computer algebra
systems in the literature, although a formal treatment of some typing
constructs occurring in computer algebra was already given almost
twenty years ago in \cite{Loos74}.

So the situation is different from the one in other areas of computer
science in which untyped languages are prevalent.  For instance, most
logic programming languages are untyped.  This is a consequence of the
fact that logic programming has its roots in first-order logic, which
is essentially untyped.  Nevertheless, the progress of type theory in
the last decade has allowed the development of several type systems
for logic programming languages.  Moreover, the formal properties of
these type systems have been studied extensively (see e.\,g.\
\cite{Smol89a}, \cite{Frue91}, \cite{Kife91}, and the
articles in the collection \cite{Pfen92}, in which also a
comprehensive bibliography on the topic is given).

We will not design a typed computer algebra language in this thesis in
which the mathematical structures a program deals with have a
correspondence in the type system.  It does not seem possible to
design and implement a language of similar power as {\sf Axiom} within
a PhD-project.  There are several proposals of languages for computer
algebra systems\footnote{The author knows of Foderaro's {\sf NEWSPEAK}
\cite{Fode83}, Coolsaet's {\sf MIKE} \cite{Cool92}, and
Dalmas' {\sf XFun} \cite{Dalm92}.}  which are designed and partly
implemented as part of a PhD-project that incorporate some typing
concepts, but which can be seen --- more or less --- as subsets of the
typing constructs of {\sf Axiom}.

Instead we will treat typing constructs which are similar in power to
the ones of {\sf Axiom}.  We will define type systems of various
strength and will investigate their properties.  Discussing a variety
of examples we will show their relevance for a computer algebra
system.  We will also discuss some examples which are not implemented
in a system as yet in order to give some estimates about the
extendability of a system based on such typing principles.  This is
one of the shortcomings of many other investigations in which very
often only examples that can be modeled are discussed.  We hope that
our discussion of a variety of examples will help to obtain
characterization theorems of mathematical structures which can be
modeled by certain typing constructs.  This would be the best
solution.  However, it seems to be a large-scale task to obtain such
characterization theorems in many cases.  A problem in this connection
is certainly that one has to define precisely a class of mathematical
structures a program is dealing with at all.  Current computer algebra
programs sometimes deal with objects of universal algebra, sometimes
with those of higher-order universal algebra, sometimes with those of
first-order model theory, or sometimes with those of category theory,
to mention only some possibilities.

We will prove several properties of such type systems.  A very
important feature is the possibility of {\em type inference}.  Given
an expression the system should be able to infer a correct type for it
whenever possible and reject it otherwise.  Since the interpretation
of an expression written in the standard mathematical notation
requires a kind of type inference very frequently the possibility of
type inference improves considerably the usefulness of a system for a
user.  Thus we will investigate the problems connected with type
inference extensively and will also give some results on the
computational complexity of various type inference problems.  Another
important problem we shall investigate in various, precisely defined
ways is a possible ambiguity of a type system.

Some of the results we give are contained in some form in the
literature, especially in papers on type systems for functional
languages.  Nevertheless, it seems to have escaped prior notice that
these results are applicable to the typing problems arising in
computer algebra.

On the one hand it is useful to have a system which can handle as many
mathematical structures as possible.  For many mathematicians a
computer algebra system would be a very valuable tool if it allowed
some computations in rather complicated mathematical structures.
Since many of those computations would be fairly basic it would
suffice for these users to have a system in which they could model
those structures easily, even if that modeling was not very efficient.
Among the existing systems {\sf Axiom} is one of the few which gives
the possibility for such work.\footnote{The new version of 
{\sf Cayley} \cite{Butl90} allows similar possibilities but fewer
structures have been implemented as yet.}  So it seems to be necessary
to have a safe foundation for the constructs found in such a universal
system as {\sf Axiom}.

On the other hand many computations that have to be performed reach
the limits of existing computing power.  So the algorithms should be
as efficient as possible in order to be useful.  Since it seems to be
impossible to have a general system that is always as efficient as a
more special one --- and this thesis will contain some results which
can be viewed as a proof of this claim --- we will not only develop a
framework for a general computer algebra system and discuss its
properties but will also discuss the properties of some subsystems.
The author hopes that some of these results will be useful for the
design of symbolic manipulation systems or the design of user
interfaces for such systems.

The organization of the thesis will be as follows.

In Sec.~\ref{chprelude} we will collect some definitions and facts
which will be needed later.  Most of the material in this chapter can
be found scattered in the literature.  Moreover, we will fix the
notation and will give some discussion on the terminology used in this
thesis as compared to the one found in the literature.

A central concept is that of {\em type classes} which correspond to
{\sf Axiom} categories and will be the subject of
Sec.~\ref{chtycla}.\footnote{They are similar to the {\em varieties}
of {\sf Cayley}, if a {\sf Cayley} {\em class} is interpreted as a
type, which can be done using the concept of {\em types depending on
elements} (see below).  They are also similar to {\em container
classes} used in object-oriented programming. However, we will not
give a systematic treatment of constructs of object-oriented
programming in this thesis.}  We will show that types can be
syntactically described as terms of a regular order-sorted signature
if no type parameters are allowed.  Using results obtained for the
functional programming language {\sf Haskell} we will show that the
problem of {\em type inference} is decidable.  This result still holds
if higher-order functions are present and {\em parametric
polymorphism} is used.  These additional typing constructs are useful
for further extensions of existing computer algebra systems: These
typing concepts can be used to implement category theoretic constructs
and there are many well known constructive interactions between
category theory and algebra.

On the one hand we will show that there are well known techniques to
specify many important type classes algebraically, and we will also
show that a formal treatment of the interactions of algebraically
specified data types and type classes is possible.  On the other hand
we will prove that there are quite elementary examples arising in
computer algebra which need very ``strong'' formalisms to be
specified.

We will show that it is necessary to distinguish between types and
elements as parameters of parameterized type classes.  The type
inference problem for the former remains decidable whereas for the
latter it becomes undecidable. We will also show that such a
distinction can be made quite naturally.

Type classes are second-order types.  Although we will show that there
are constructions used in mathematics which imply that type classes
have to become first-order types in order to model the examples
naturally, we will also argue that this does not seem to be the case
in areas currently accessible for an algebra system.  We will only
sketch some systems that have been developed during the last years in
which the concept of type classes as first-order types can be
expressed.  For some of these systems the type inference problem was
proven to be undecidable, thus showing one of the drawbacks of
stronger formalisms.

In Sec.~\ref{chapcoer} we will treat the concept of {\em coercions}
which is another fundamental concept for a type system of a computer
algebra system, at least for the purpose of a user interface.  We will
show that there are cases which can be modeled by coercions but not by
an ``inheritance mechanism'', i.\,e.\ the concept of coercions is not
only orthogonal to the one of type classes but also to formalisms
extending type classes.  We will define certain classes of coercions
and impose conditions on important classes of coercions which will
imply that the meaning of an expression is independent of the
particular coercions that are used in order to type it.  These results
will also appear in \cite{Webe95}.

We shall also impose some conditions on the interaction between
polymorphic operations defined in type classes and coercions that will
yield a unique meaning of an expression independent of the type which
is assigned to it --- if coercions are present there will very
frequently be several possibilities to assign types to expressions.

Often it is not only possible to coerce one type into another but it
will be the case that two types are actually {\em isomorphic}.  We
will show that isomorphic types have properties that cannot be deduced
from the properties of coercions and will shortly discuss other
possibilities to model type isomorphisms.

Unfortunately, there are natural examples of type isomorphisms
occurring in the area of computer algebra that have a ``problematic''
behavior.  For a major example of types having type isomorphisms that
cannot be captured by a finite set of coercions, we will provide a
proof that no such finite set can be given by proving that the
naturally associated equational theory is not finitely axiomatizable.
This example and the given proof are published by the author in
\cite{Webe05}.

We will give a semi-decision procedure for type inference for a system
having type classes and coercions and a decision procedure for a
subsystem which covers many important cases occurring in computer
algebra.  Up to now few results are known that would give a clear
dividing line between classes of coercions which have a decidable type
inference problem and classes for which type inference becomes
undecidable.  However, even in decidable cases the type inference
problem in the presence of coercions is a hard problem.  Even in cases
in which the possible coercions are rather restricted the type
inference problem was proven to be NP-hard for functional languages.

Two typing constructs which are again quite orthogonal to the previous
ones are treated in Sec.~\ref{chapothtyc}.  We will link the treatment
of {\em partial functions} in {\sf Axiom} to the one used in
order-sorted algebras and will show some problems which arise if a
seemingly more expressive solution were used.  Nevertheless, some
information is lost by the used solution and we sketch a proposal how
the lost information could be regained in certain cases.

There are important cases in which {\em types depending on elements}
arise naturally.  Unfortunately, not only type inference but even type
checking are undecidable for relevant cases occurring in computer
algebra, i.\,e.\ static type checking is not possible.  On the one
hand we will show that already types which have to be given to the
objects in standard algorithms of almost any general purpose computer
algebra program will prohibit static type checking.  On the other hand
it might be possible to restrict the types depending on elements
available to a user of a high-level user interface to classes which
have decidable type checking or even type inference problems.  We will
show that several formalisms have been developed during the last years
which might be relevant in this respect.
 
\section{Prelude}
\label{chprelude}

We will recall some definitions and facts which will be needed later.
 All of this material can be found scattered in the literature.
Moreover, we will fix the notation and will  give some
discussions of the terminology used in this thesis in
comparison to the one found in the literature.

\subsection{Terminology}

\subsubsection{Abstract Data Types}


The term {\em data type} has many informal usages in programming and
programming methodology. For instance, Gries lists seven
interpretations in \cite{Grie78}.

In this thesis we will deal with different meanings of the term {\em
abstract data type} (ADT).  On the one hand there is the meaning used
in the context of algebraic specifications as it is used e.\,g.\ in
the survey of Wirsing \cite{Wirs91}.  In this context an abstract
datatype given by a specification is a class of certain many-sorted
(or order-sorted) algebras which ``satisfy'' the specification.

On the other hand there is the usage of this term for data types whose
representation is hidden.  For instance, in the report on the language
{\sf Haskell} \cite{Huda92} the authors state ``the
characteristic feature of an ADT is that the {\em representation type
is hidden}; all operations on the ADT are done at an abstract level
which does not depend on the representation''.  The explanation given
in the glossary of the book on {\sf Axiom} \cite{Jenk92} is
quite similar:

\begin{quote}
{\bf abstract datatype} \\ a programming language principle used in
{\sf Axiom} where a datatype definition has defined in two parts: (1)
a {\em public} part describing a set of {\em exports}, principally
operations that apply to objects of that type, and (2) a {\em private}
part describing the implementation of the datatype usually in terms of
a {\em representation} for objects of the type.  Programs that create
and otherwise manipulate objects of the type may only do so through
its exports.  The representation and other implementation information
is specifically hidden.
\end{quote}

Usually the purpose of abstract data types in the sense of algebraic
specifications is for the specification of abstract data types in the
sense of the quotations given above.  However, as we will show in this
thesis, the abstract data types in the former sense can also be used
for the specification of other classes of computational objects than
abstract data types in the latter sense.

\subsubsection{Polymorphism}

Although the term {\em polymorphic function} is used in the
literature, there are usually no definitions given.

In the glossary of \cite{Jenk92} only examples of polymorphic
functions are given but no definition.  Also in the book by Aho,
et~al.\ \cite[p.~364]{Ahox86}, the term is explained by
giving examples of polymorphic functions.

In the recent survey of Mitchell \cite{Mitc91a} the author states
explicitly that he does not want to give a definition of {\em
polymorphism}, but that he will only give definitions of some
``polymorphic lambda-calculi''.

There is a distinction between {\em parametric polymorphism} and {\em
ad hoc polymorphism} which seems to go back to Strachey \cite{Stra00}
(cited after \cite{Gogu89}):

\begin{quotation}

In {\em ad hoc} polymorphism there is no simple systematic way of
determining the type of the result from the type of the
arguments. There may be several rules of limited extent which reduce
the number of cases, but these are themselves {\em ad hoc} both in
scope and in content.  All the ordinary arithmetic operations and
functions come into this category. It seems, moreover, that the
automatic insertion of transfer functions by the compiling system is
limited to this class.

Parametric polymorphism is more regular and may be illustrated by an
example. Suppose f is a function whose arguments is of type $\alpha$
and whose result is of type $\beta$ (so that the type of f might be
written $\alpha \longrightarrow \beta$, and that L is a list whose
elements are all of type $\alpha$ (so that the type of L is
$\alpha{\bf list}$).  We can imagine a function, say Map, which
applies f in turn to each member of L and makes a list of the
results. Thus Map[f,L] will produce a $\beta{\bf list}$.  We would
like Map to work on all types of list provided f was a suitable
function, so that Map would have to be polymorphic. However its
polymorphism is of a particularly simple parametric type which could
be written $(\alpha \longrightarrow \beta, \alpha{\bf list})
\longrightarrow \beta{\bf list}$, where $\alpha$ and $\beta$ stand for
any types.  

\end{quotation}

A widely accepted approach to parametric polymorphism is
the Hindley-Milner type system \cite{Hind69},
\cite{Miln78}, \cite{Dama82},
which is used in Standard ML \cite{Miln90}, \cite{Miln91},
Miranda \cite{Turn85}, \cite{Turn86} and other languages.

We will use the term parametric polymorphism in this sense.

There is no widely accepted approach to ad-hoc polymorphism.  In its
general form, we will use the word ad-hoc polymorphism and overloading
quite synonymously indicating that no restriction is imposed on the
possibility to overload an operator symbol.

However, there is a third form of polymorphism which will play a
central role in this thesis and for which an appropriate name is
missing.  It is the polymorphism which occurs when {\em categories} in
the {\sf Axiom}-terminology resp.\ {\em type classes} in the {\sf
Haskell}-terminology are used.  In \cite{Wadl88} the nice
negative formulation ``How to make {\em ad-hoc} polymorphism less {\em
ad-hoc}'' is used but no proposal for a positive name is given. When
necessary we will call the polymorphism encountered by type classes
simply {\em type-class polymorphism}.\footnote{A term like {\em
categorical polymorphism} seems to be misleading, especially since we
prefer the word type class instead of category.}

Sometimes a distinction is made between {\em polymorphic functions}
and {\em generic function calls}.  The intended meaning --- e.\,g.\ in
\cite{Fode83} --- is that {\em polymorphic} refers to functions in
which the same algorithm works on a wide range of data types, whereas
{\em generic} refers to function declarations in the language which
are resolved by different pieces of code.

However, a clear distinction can only be made if there is an untyped
language to which the typed language is reduced.\footnote{This is the
case for typed-functional programming languages which are usually
translated into the untyped lambda-calculus.  It can also be put in a
precise form that the lambda-calculus is untyped.}  On the other-hand
if typing information is used by the run-time system it does not seem
to be possible to have such a distinction.  So in the book by Aho,
et~al.\ \cite{Ahox86} no distinction is made between these terms.

Nevertheless, we will sometimes use these terms with the flavor as is
given in \cite{Fode83} when it will be clear how the language
constructs in discussion can be translated into untyped ones.

\subsubsection{Coercions}

We will assume that we have a mechanism in the language to declare
some functions between types to be {\em coercions}, i.\,e.\ conversion
functions which are automatically inserted by the system if necessary.

The usage of this terminology seems to be more or less standard, as
the definition in the book by Aho, et~al.\ \cite[p.~359]{Ahox86} shows:

\begin{quote}
Conversion from one type to another is said to be {\it implicit} if it
is to be done automatically by the compiler.  Implicit type
conversions, also called {\it coercions}, are limited in many
languages to situations where no information is lost in principle;
\end{quote}

The definitions in the glossary of the book on {\sf Axiom}
\cite{Jenk92} is quite similar:

\begin{quote}
{\bf coercion}\\ an automatic transformation of an object of one {\it
type} to an object of a similar or desired target type.  In the
interpreter, coercions and {\it retractions} are done automatically by
the interpreter when a type mismatch occurs.  Compare {\bf conversion}.
\end{quote}

\begin{quote}
{\bf conversion}\\ the transformation of an object of one {\it type}
to one of another type. Conversions that can be performed
automatically by the interpreter are called {\it coercions}. These
happen when the interpreter encounters a type mismatch and a similar
or declared target type is needed.  In general, the user must use the
infix operation ``::'' to cause this transformation.
\end{quote}

However, there are some issues which have to be clarified.  In the
following a {\em coercion\/} will always be a {\em total function}.
Although we will see that it is desirable to have injective coercions
(``no information is lost in principle'') we will not require that
coercions are injective by the definition of the term.

We will use the term {\em retraction} for non-total conversion
functions.  Our usage of this term is more general than the one in
{\sf Axiom}:
\begin{quote}
{\bf retraction} \\ to move an object in a parameterized domain back
to the underlying domain, for example to move the object 7 from a
``fraction of integers'' (domain {\sf Fraction Integer}) to ``the
integers'' (domain {\sf Integer}).
\end{quote}

In several papers --- e.\,g.\ \cite{Fuhx90}, \cite{Mitc91} --- the
term {\em subtype} is used if there is a coercion from one type (the
``subtype'') into another type (the ``supertype'').  Since the term
``subtype'' has several other meanings in the literature, we will
avoid it.  Only in our notation we will be close to that terminology
and will write $t_1 \subtype t_2$ if there is a coercion $\phi: t_1
\longrightarrow t_2$.

\subsection{General Notation}

As usual we will use {\em ``iff''} for {\em ``if, and only
if''}.\index{iff|ii}

The non-negative integers will be denoted by $\NN$. \index{
N@$\NN$|ii} The integers will be denoted by $\ZZ$ and the rationals by
$\QQ$. For $n \in \NN$ we will denote the integers modulo $n$ by
$\ZZ_n$.  We will use these symbols both for the algebras (of the
usual signatures) and the underlying sets.  Since we use these
ambiguous notations only in parts exclusively written for human beings
and not for machines, there will not be any problems.  Nevertheless, a
major part of this thesis will deal with problems which arise from
ambiguities which mathematicians usually can resolve easily.  We will
show how some of them can be treated in a clean formal way accessible
to machines, sometimes causing computationally hard problems.

The set of strings over a set $L$ --- i.\,e.\ the set of finite
sequences of elements of $L$ --- will be $L^*$,\index{ Lstar@$L^*$|ii}
where $\varepsilon$ is the empty string. \index{
epsilon@$\varepsilon$|ii}

The length of a string $s \in L^*$ will be denoted by $|s|$. \index{
"| "|@$"|\cdot"|$|ii} We will also denote the cardinality of a set $A$
by $|A|$.\index{ "| "|@$"|\cdot"|$|ii}

\subsection{Partial Orders and Quasi-Lattices}

{\bf Definition 1. (Preorder)}.
\index{preorder|ii}
\index{order!preorder|ii}
{\sl A binary relation which is reflexive and
transitive is a {\em preorder}.}

A  preorder which is also antisymmetric is a partial order.
\index{partial order|ii}
\index{order!partial|ii}

{\bf Definition 2.}
{\sl Let $\langle M ,\leq \rangle$ be a partially ordered set.
Then  $c \in M$ is a {\em common lower bound\/} of $a$ and $b$
($a,b \in M$) if  $c \leq a$ and $c \leq b$.}

{\sl Moreover, $c \in M$ is a {\em common upper bound\/} of $a$ and $b$
if  $a \leq c$ and $b \leq c$.}

{\bf Definition 3.}
{\sl Let $\langle M ,\leq \rangle$ be a partially ordered set.
Then  $c \in M$ is called the {\em infimum} of $a$ and $b$
($a,b \in M$) if $c$ is a lower bound of $a$ and $b$ and}
$$\forall d \in M : d \leq a \mbox{ and } d \leq b \Longrightarrow d \leq c.$$

{\sl Furthermore, $c$ is called the {\em supremum} of
$a$ and $b$ if it is an upper bound of $a$ and $b$ and}
$$\forall d \in M : a \leq d \mbox{ and } b \leq d \Longrightarrow c \leq d.$$

It is easy to verify that infima and suprema are unique if they exist.
By induction the infimum and the supremum of any finite subset
of a partially ordered set $\langle M ,\leq \rangle$ can be defined.

{\bf Definition 4.}
{\sl A partially ordered set $ \langle M ,\leq \rangle$ is
a {\em lower quasi-lattice\/} if  for any $a, b \in M$
$a$ and $b$ have an infimum whenever they have a common lower bound.
It is a {\em lower semi-lattice\/} if any $a,b \in M$ have an infimum.}

{\sl A partially ordered set $ \langle M ,\leq \rangle$ is
an {\em upper quasi-lattice\/} if for for any $a, b \in M$
$a$ and $b$ have a supremum whenever they have a common upper bound.
It is an {\em upper semi-lattice\/} if any $a,b \in M$ have a supremum.}

{\sl A partially ordered set $ \langle M ,\leq \rangle$ is
a {\em  quasi-lattice} if it is an upper and a lower quasi-lattice.
It is a {\em lattice} if it is both a upper and a lower semi-lattice.}

{\bf Definition 5. (Free Lower Semi-Lattices)}
\label{freesemilat}
Let $\langle M ,\leq \rangle$ be a
partially ordered set.
The {\em free lower semi-lattice on $\langle M ,\leq \rangle$}
is the following  partially ordered set $\langle F, \preceq \rangle$:
\begin{enumerate}
\item $F$ is the set of all non-empty subsets of $M$
whose elements are pairwise incomparable with respect to $\leq$.
\item If $S_1, S_2 \in F$ then
      $$ S_1 \preceq S_2 \quad \Longleftrightarrow \quad
          \forall s_2 \in S_2 \: \exists s_1 \in S_1 \, . \,
           s_1 \leq s_2.$$
\end{enumerate}

{\bf Lemma 1. (Free Lower Semi-Lattices)}
\label{lefrelosela}
{\sl Let $\langle M ,\leq \rangle$ be a partially ordered set. 
Then the free lower semi-lattice on $\langle M ,\leq \rangle$
is a lower semi-lattice.}

\begin{proof}
{\sl Let $S_1, S_2 \in F$ be arbitrary. Since $S_1 \in F$ and 
$S_2 \in F$ the chains in $S_1 \cup S_2$ with respect to $\leq$ have 
length at most 2. Let}
$$H = \{ d \in S_1 \cup S_2 \mid \exists s \in S_1 \cup S_2 \, . \,
s < d \}$$
{\sl and}
$$\overline{S}= (S_1 \cup S_2) - H.$$

Since $\overline{S}$ is not empty and contains only incomparable
elements by construction we have $\overline{S} \in F$.

We claim that $\overline{S}$ is the infimum of $S_1$ and $S_2$.

We have $\overline{S} \preceq S_1$ because for any $s \in S_1$
either $s \in \overline{S}$ or there is a $s' \in
\overline{S}$ such that $s' < s$. Similarly $\overline{S} \preceq S_2$.

Let $L \in F$ be a common lower bound of $S_1$ and $S_2$
with respect to $\preceq$, i.\,e.\ $L \preceq S_1$ and
$L \preceq S_2$.
Then for any $s \in S_1 \cup S_2$  there is an
$l \in L$ such that $l \leq s$.
Since $$\overline{S} \subseteq S_1 \cup S_2$$ 
we thus have $L \preceq \overline{S}$ by the definition
of $\preceq$.
\qed
\end{proof}

\begin{remark}
The statement given in \cite[p.~9]{Nipk91} that the union of
two sets of incomparable elements is a set of incomparable elements
and is the infimum of these sets with respect to the ordering given in
Def. 5 is false in general.  The proof of Lemma~1 shows the correct 
construction.
\end{remark}

\begin{remark}
If we define semi-lattices algebraically (see e.\,g.\
\cite[\S~6]{Grae79}),
then the free lower semi-lattice on $\langle M ,\leq \rangle$
is indeed a free  semi-lattice.
\end{remark}

{\bf Lemma 2.}
{\sl Let $\langle M ,\leq \rangle$ be a  {\em finite}
partially ordered set. Then  $\langle M ,\leq \rangle$
is a lower quasi-lattice iff it is an upper quasi-lattice.}

\begin{proof}
Let $ \langle M ,\leq \rangle$ be a lower quasi-lattice
in which $a$ and $b$ have a common upper bound. We have
to show that $a$ and $b$ have a supremum. Since the set
$$I= \{c \in M : a \leq c \mbox{ and } b \leq c \}$$
is nonempty and finite and $\langle M ,\leq \rangle$ is a lower 
quasi-lattice the infimum $c$ of $I$ exists. Clearly $c$ is the supremum
of $a$ and $b$.

The other direction is shown analogously. \qed
\end{proof}

{\bf Lemma 3.}
\label{lenolat}
{\sl Let $ \langle M ,\leq \rangle$ be a finite partially ordered set. Then
$ \langle M ,\leq \rangle$ is not a quasi-lattice
iff there are $a,b,c,d \in M$ such that
\begin{enumerate}
\item $a \leq c$ and $a \leq d$,
\item $b \leq c$ and $b \leq d$,
\item $a \not \leq b$ and $b \not \leq a$,
\item $c \not \leq d$ and $d \not \leq c$,
\item \label{lacolenolat} there is no $e \in M$ which is a common upper bound
of $a$ and $b$ and a common lower bound of $c$ and $d$.
\end{enumerate}}

\begin{figure}

\begin{center}
\unitlength=1mm
\thinlines
\begin{picture}(45.00,50.00)
\put(13.00,10.00){\makebox(0,0)[cc]{$a$}}
\put(37.00,10.00){\makebox(0,0)[cc]{$b$}}
\put(13.00,40.00){\makebox(0,0)[cc]{$c$}}
\put(37.00,40.00){\makebox(0,0)[cc]{$d$}}
\put(13.00,13.00){\vector(0,1){24.00}}
\put(13.00,13.00){\vector(1,1){24.00}}
\put(37.00,13.00){\vector(0,1){24.00}}
\put(37.00,13.00){\vector(-1,1){24.00}}
\put(9.00,25.00){\makebox(0,0)[cc]{$\leq$}}
\put(41.00,25.00){\makebox(0,0)[cc]{$\leq$}}
\put(22.00,20.00){\makebox(0,0)[cc]{$\leq$}}
\put(28.00,20.00){\makebox(0,0)[cc]{$\leq$}}
\end{picture}
\end{center}

\caption{Ad Lemma 3}
\end{figure}

\begin{proof}
Assume $\langle M, \leq \rangle$ is  a finite partially ordered set having 
elements $a,b,c,d \in M$ that satisfy the conditions of the lemma.
Since $a$ and $b$ have a common upper bound, we are done
if we can show that
they do not have a supremum. Assume towards a contradiction they had
a supremum $e$. Since $c$ and $d$ are common upper bounds
of $a$ and $b$ and $e$ is the supremum of $a$ and $b$,
we had
$e \leq c$ and $e \leq d$, a contradiction  to condition~\ref{lacolenolat} 
of the lemma.

Now let $\langle M, \leq \rangle$ be  a finite partially ordered set
which is not a quasi-lattice.
Then there are $a,b \in M$ which have a common upper bound $c$
but not a supremum. Since $M$ is finite we can assume w.\,l.\,o.\,g.\
that there is no $c' \leq c$ which is also a common upper bound
of $a$ and $b$ (if there is one, take $c'$ instead of $c$).
Since $a$ and $b$ have no supremum, there is a common upper bound
$d$ of $a$ and $b$ such that $d \not \leq c$ and $c \not \leq d$.
These elements $a, b, c, d$ satisfy the conditions of the lemma.\qed
\end{proof}

\subsection{Order-Sorted Algebras}

There is a growing literature on order-sorted algebras.  Some
comprehensive sources are the thesis of Schmidt-Schau{\ss}
\cite{Schm89}, the survey by Smolka, et~al.\ \cite{Smol89}, and the
articles by Goguen \& Meseguer \cite{Gogu89} and by Waldmann
\cite{Wald92}.  In \cite{Como90} Comon shows that an order-sorted
signature can be viewed as a finite bottom-up tree automaton.

{\bf Definition 6. (Order-Sorted Signature)}
{\sl An {\em order-sorted signature}  
\index{order-sorted signature|ii}
\index{signature!order-sorted|ii}
is a triple
$(S, \leq, \Sigma)$, where $S$
is a set of sorts, $\leq$ a preorder on \index{preorder}
$S$, and $\Sigma$ a family}
$$\{ \Sigma_{\omega,\sigma} \mid \omega \in S^*, \: \sigma \in S \}$$
{\sl of not necessarily disjoint sets of operator symbols.}

{\sl If $S$ and $\Sigma$ are finite, the signature is called finite.}

{\sl For notational convenience, we often write $f: (\omega)\sigma $
instead of $f \in \Sigma_{\omega,\sigma}$; $(\omega)\sigma$ is called
an {\em arity} \index{arity|ii} and $f: (\omega)\sigma$ a {\em
declaration}. \index{declaration|ii} The signature $(S, \leq, \Sigma)$
is often identified with $\Sigma$.  If $|\omega|=n$ then $f$ is called
a $n$-ary operator symbol.  \index{operator symbol|ii} $0$-ary
operator symbols are {\em constant symbols}.  \index{constant
symbols|ii} As in \cite{Smo89} we will assume in the following
that for any $f$ there is only a single $n \in \NN$ such that $f$ is a
$n$-ary operator symbol.}

{\sl An $\sigma$-sorted variable set is a family}
$$V= \{V_\sigma \mid \sigma \in S\}$$
{\sl of disjoint, nonempty sets. For $x \in V_\sigma$ we also 
write $x:\sigma$ or $x_\sigma$.}
\index{variable set!s-sorted@$\sigma$-sorted|ii}

In \cite{Gogu89} the following monotonicity condition
must be fulfilled by any order-sorted signature.

{\bf Definition 7.}
{\sl An order-sorted signature $(S, \leq, \Sigma)$  fulfills the
{\em monotonicity condition}, 
\index{monotonicity condition|ii}
if}
$$f \in \Sigma_{\omega_1,\sigma_1} \cap \Sigma_{\omega_2,\sigma_2}
\mbox{ and } \omega_1 \leq \omega_2 \mbox{ imply } \sigma_1 \leq \sigma_2.$$

Notice that the monotonicity condition excludes multiple declarations
of constants.  This is one of the reasons why we will not assume in
general that the order-sorted signatures we will deal with will
fulfill the monotonicity condition.

{\bf Definition 8. (Order-Sorted Terms)}
\label{defordsortterm}
{\sl The set of {\em order-sorted terms} 
\index{order-sorted terms!set of|ii}
of sort $\sigma$ freely  generated by $V$,
$T_\Sigma(V)_\sigma$, is the least set satisfying}
\begin{itemize}
\item if $x \in V_{\sigma'}$ and $\sigma' \leq \sigma$,
then $x \in T_\Sigma(V)_\sigma$
\item if $f \in \Sigma_{\omega,\sigma'}$,  $\omega = \sigma_1 \ldots \sigma_n$,
 $\sigma' \leq \sigma$ and $t_i \in T_\Sigma(V)_{\sigma_i}$ then
$f(t_1, \ldots, t_n) \in T_\Sigma(V)_\sigma$.
\end{itemize}

If $t \in T_\Sigma(V)_\sigma$ we will also write $t:\sigma$.

In contrast to sort-free terms and variables, order-sorted variables
and terms always have a sort.  Terms must be sort-correct, that is,
subterms of a compound term must be of an appropriate sort as required
by the arities of the term's operator symbol.

Note that an operator symbol may have not just one arity (as in
classical homogeneous or heterogeneous term algebras), but may have
{\em several} arities.  As a consequence, each term may have several
sorts.

The set of all order-sorted terms over $\Sigma$
freely generated by $V$ will be denoted by
$$T_\Sigma(V) := \bigcup_{\sigma \in S} T_\Sigma(V)_\sigma .$$
The set of all 
{\em ground terms} over $\Sigma$ is
$T_\Sigma := T_\Sigma(\{\})$.
\index{ground term|ii}\index{term!ground|ii}

{\bf Definition 9. (Regularity)}
{\sl A signature is {\em regular}, 
\index{signature!order-sorted!regular|see{signature!regular}}
\index{signature!regular|ii}
if the subsort preorder
of $\Sigma$ is antisymmetric, and if each term
$t \in T_\Sigma(V)$ has a least sort.}

The following theorem shows that it is decidable for finite signatures
whose subsort preorders are anti-symmetric if a signature is regular.

{\bf Theorem 1.}
\label{thregulcon}
{\sl A signature  $(S, \leq, \Sigma)$ whose subsort preorder
is anti-symmetric is regular iff for 
every $f \in \Sigma$ and $\omega \in S^*$ the set}
$$\{\sigma \mid \exists \omega' \geq \omega :
 f\in \Sigma_{\omega',\sigma} \}$$
{\sl either is empty or contains a least element.}

\begin{proof}
See \cite{Smol89}. \qed
\end{proof}

As an  example  of a simple non-regular signature, consider
$$(\{ \sigma_0, \sigma_1, \sigma_2 \}, \;
 \{ \sigma_1 \leq \sigma_0, \,  \sigma_2 \leq \sigma_0\}, \;
\Sigma_{\varepsilon,\sigma_1}={a}, \Sigma_{\varepsilon,\sigma_2}={a}).$$
The constant $a$ has two sorts which are incomparable, hence it does
not have a minimal sort.

{\bf Definition 10.}
{\sl The {\em complexity\/}
\index{complexity of a term|ii}
\index{term!complexity|ii}
 of a term $t\in T_\Sigma(V)$, $\com(t)$ is inductively
defined as follows:
\begin{itemize}
\item $\com(t)=1$, if $t \in V_\sigma$ or $t \in \Sigma_{\epsilon,\sigma}$
for some $\sigma \in S$,
\item if $f \in \Sigma_{\omega,\sigma'}$,  $\omega = \sigma_1 \cdots \sigma_n$,
and $t_i \in T_\Sigma(V)_{\sigma_i}$ then
$$\com(f(t_1,\ldots,t_n))= \max(\com(t_1), \ldots, \com(t_n)) + 1.$$
\end{itemize}}

{\bf Definition 11.}
{\sl A {\em substitution} $\theta$ \index{substitution|ii}
from a variable set $Y$ into the term algebra $T_\Sigma(V)$ is a mapping from
$Y$ to $T_\Sigma(V)$, which additionally satisfies
$\theta(x) \in T_\Sigma(V)_\sigma$ if $x \in V_\sigma$
(that is, substitutions must be sort-correct). As usual,
substitutions are extended canonically to
$T_\Sigma(V)$.
If $Y=\{x_1, \ldots, x_n\}$ we write
$\theta=\{x_1 \mapsto t_1, \ldots, x_n \mapsto t_n\}$.
If $\theta=\{x_1 \mapsto t_1\}$ and 
$t \in T_\Sigma(V)$, then we will write $t[t_1/x_1]$ for
$\theta(t)$.
If, for $t, t' \in T_\Sigma(V)$, there is a substitution
$\theta$ such that
$t'=\theta(t)$, then $t'$ is called
an {\em instance} of $t$.\index{instance|ii}
Similarly, a substitution $\theta'$ is called an
instance of a substitution
$\theta$ with respect to a set of variables $W$, written
$\theta \succeq \theta'[W]$, if there is a substitution
$\gamma$ such that 
$\theta'(x)=\gamma(\theta(x))$ for all
$x \in W$.}

{\bf Definition 12.}
{\sl A {\em unifier}\index{unifier|ii} of a set of equations
$\Gamma$ is a substitution $\theta$ such that
$\theta(s)=\theta(t)$ for all equations
$s=^{\rm ?} t$ in $\Gamma$. 
A set of unifiers $U$ of $\Gamma$ is called {\em complete}
\index{complete set of unifiers|ii}\index{unifier!complete set|ii}
(and denoted by CSU),\index{CSU|ii} if for every unifier
$\theta'$ of $\Gamma$ there exists $\theta \in U$ such that
$\theta'$ is an instance of
$\theta$ with respect to the variables in 
$\Gamma$. As usual, a signature is called
{\em unitary (unifying)},
\index{unitary unifying|ii}\index{signature!unitary unifying|ii}
\index{unifying!unitary|ii}
if for all equation sets $\Gamma$ there is a complete set of
unifiers containing at most one element;
it is called {\em finitary (unifying)},
\index{finitary unifying|ii}\index{signature!finitary unifying|ii}
\index{unifying!finitary|ii}
if there is always a finite and complete set of unifiers.}

For non-regular signatures, unifications can be infinitary, 
even if the signature is finite (see e.\,g.\
\cite[p.~326]{Smol89}, \cite[p.~26]{Wald92}).

{\bf Theorem 2. (Schmidt-Schau{\ss})}
{\sl In finite and regular signatures, finite sets of equations have
finite, complete, and effectively computable sets of unifiers.}

\begin{proof}
See \cite[Theorem~15]{Smol89}.\qed
\end{proof}

{\bf Definition 13.}
{\sl A signature $(S, \leq, \Sigma)$ is {\em downward complete}
\index{downward complete|ii}
\index{signature!downward complete|ii}
if any two sorts have either no lower bound or an infimum,
and {\em coregular}
\index{coregular|ii}
\index{signature!coregular|ii}
if for any operator symbol $f$ and any sort $\sigma \in S$
the set
$$D(f,\sigma)=\{ \omega \mid \exists  \sigma'  \in S \:.\;
f: (\omega)\sigma' \wedge \sigma' \leq \sigma\}$$
either is empty or has a greatest element.}

{\bf Definition 14.}
{\sl Let $(S, \leq, \Sigma)$  be an order-sorted signature.
It is  {\em injective} if for any operator symbol $f$
the following condition holds:
$$f: (\omega) \sigma \mbox{ and } f: (\omega') \sigma
\quad \mbox{ imply }\quad \omega = \omega'.$$

It is {\em subsort reflecting} if for any operator  symbol $f$
the following condition holds:}
$$f: (\omega') \sigma' \mbox{ and } \sigma' \leq \sigma
\quad \mbox{ imply } \quad f: (\omega) \sigma \mbox{ for some }
\omega \geq \omega'.$$

{\bf Theorem 3.}
{\sl Every finite, regular, coregular, and downward complete signature is
unitary unifying.}

\begin{proof}
See \cite[Theorem~17]{Smol89}.\qed
\end{proof}

{\bf Corollary 3A.}
{\sl Every finite, regular, downward complete, injective and subsort
reflecting signature is unitary unifying.}

\subsection{Category Theory}

We will recall some basic definitions from category theory which can
be found in many books on the topic.  Some classical textbooks are
\cite{Macl91}, \cite{Schu72}.  A more recent
textbook is \cite{Frey90}.  In \cite{Ryde88}
computational aspects are elaborated.  The basic concepts of category
theory can also be found in several books which use category theory as
a tool for computer science, e.\,g.\ in \cite{Ehri85}.

{\bf Definition 15. (Category)} 
\index{category|ii}
{\sl A {\em category} $\cat{C}$  consists of a class of
{\em objects\/} \index{objects|ii}
$\obj{\cat{C}}$, for each pair
$(A,B) \in \obj{\cat{C}} \times \obj{\cat{C}}$ a  set
$\mor{\cat{C}}{A,B}$ of {\em morphisms} (or {\em arrows}), written
$f: A \longrightarrow B$ for 
$f \in \mor{\cat{C}}{A,B}$,
and a {\em  composition}}
\index{composition|ii}
\index{  composition@$\circ$|ii}
\index{arrows!composition of|ii}
$$\begin{array}{l}
\circ: \mor{C}{A,B} \times \mor{C}{B,C} \longrightarrow \mor{C}{A,C}\\
(f: A \longrightarrow B, \; g: B \longrightarrow C)
\mapsto (g \circ f : A \longrightarrow C)
\end{array}
\index{arrows|ii} \index{morphisms|ii} \index{objects|ii}
$$
{\sl (more precisely a family of functions
$\circ_{A,B,C}$ for all objects $A,B,C$) such that the following
axioms are satisfied:
\begin{enumerate}
\item $(h \circ g ) \circ f = h \circ (g \circ f)$
\hfill {\em (associativity)} \\
for all morphisms $f,g,h$, if at least one side is defined.
\item For each object $A \in \obj{C}$ there is a morphism
${\rm id}_A \in \mor{C}{A,A}$, called 
{\em identity of $A$}, such that we have for all
$f: A \longrightarrow B$ and
$g: C \longrightarrow A$ with $B,C \in \obj{C}$\\
$f \circ {\rm id}_A = f \mbox{ and } {\rm id}_A \circ g =g$ 
\hfill {\em (identity)}.
\index{identity|ii}\index{category!identity|ii}
\index{ id@${\rm id}_A$|ii}
\end{enumerate}}

{\sl Frequently we will write}
$$A \stackrel{f}{\longrightarrow} B$$
{\sl instead of}
$$f: A \longrightarrow B .$$

{\bf Definition 16. (Opposite Category)}
\index{dual category|ii}\index{category!opposite|ii}
\index{opposite category|ii}\index{category!dual|ii}
{\sl Let $\cat{C}$ be a category. Then
$\cat{C}^{\rm op}$, the {\em opposite category} of
$\cat{C}$, is the category which is
defined by 
\begin{enumerate}
\item $\obj{C^{\rm op}}= \obj{C}$,
\item $\mor{C^{\rm op}}{A,B} =\mor{C}{B,A}$.
\end{enumerate}
Sometimes we will call $\cat{C}^{\rm op}$ the
{\em dual category} of $\cat{C}$.}

Clearly, $\cat{(C^{\rm op})^{\rm op}}= \cat{C}$.

For any categories $\cat{C}$ and $\cat{D}$ 
we will write
$\cat{C} \times \cat{D}$\index{  product@$\times$|ii}
for the category which is defined by
\begin{enumerate}
\item $\obj{C \times D}= \obj{C} \times \obj{D}$,
\item $\mor{C\times D}{(A,A'),\,(B,B')} =\mor{C}{A,B} \times 
\mor{D}{A',B'}$,
\end{enumerate}
where the symbol $\times$ on the right hand side of the equations
denotes the usual set theoretic Cartesian product.

Since $\times$ is associative, we will write
unambiguously 
$\cat{C}_1 \times \cdots \times \cat{C}_n$
for  an $n$-fold iteration.
Moreover,
$$\cat{C}^n= \underbrace{\cat{C} \times \cdots \times \cat{C}}_{n}.$$  

{\bf Definition 17. (Functors)}
\index{functor|ii}
{\sl Let $\cat{C}$ and $\cat{D}$ be categories.
A mapping $F: \cat{C} \longrightarrow \cat{D}$ is
called {\em functor}, if
$F$ assigns to each object $A$ in $\cat{C}$
an object $F(A)$ in $\cat{D}$ and to each morphism
$f: A \longrightarrow B$ in $\cat{C}$ a morphism
$F(f): F(A) \longrightarrow F(B)$ in
$\cat{D}$ such that the following axioms are satisfied:
\begin{enumerate}
\item $F(g \circ f) = F(g) \circ F(f)$ for all $g \circ f$ in
$\cat{C}$,
\item $F({\rm id}_A) = {\rm id}_{F(A)}$ for all objects $A$ 
in $\cat{C}$.
\end{enumerate}}

{\sl The composition of two functors
$F: \cat{C} \longrightarrow \cat{D}$ and
$G: \cat{D} \longrightarrow \cat{E}$ 
is defined by }
$$G \circ F (A) = G(F(A))$$
{\sl and}
$$G \circ F (f) = G(F(f))$$
{\sl for objects and morphisms respectively
leading to the {\em composite functor}
$G \circ F : \cat{C} \longrightarrow \cat{E}$.}

{\sl The {\em identical functor}
${\rm ID}_{\cat{C}}: \cat{C} \longrightarrow \cat{C}$ is
defined by 
${\rm ID}_{\cat{C}}(A) = A$ and
${\rm ID}_{\cat{C}}(f) = f$.\index{ ID@${\rm ID}_{\cat{C}}$|ii}}

{\sl A functor $F: \cat{C} \longrightarrow \cat{D}$ is
also called a {\em covariant functor}
from $\cat{C}$ into $\cat{D}$.\index{covariant!functor|ii}
\index{functor!covariant|ii}
A functor $F: \cat{C}^{\rm op} \longrightarrow \cat{D}$ is
 called a {\em contravariant functor} from 
$\cat{C}$ into $\cat{D}$.}
\index{contravariant!functor|ii}
\index{functor!contravariant|ii}

{\bf Definition 18. (Natural Transformations)}
\index{natural transformation|ii}
\index{transformation!natural|ii}
{\sl Let $S, T :\cat{C} \longrightarrow \cat{D}$
be functors.
A {\em natural transformation} $\tau: S \longrightarrow T$ 
is a mapping which assigns to any
object $A$ in $\cat{C}$ an arrow
$\tau_{A}=\tau (A) : S(A) \longrightarrow T(A)$
such that for any arrow
$f: A \longrightarrow B$ in $\cat{C}$ the diagram}
\begin{center}
\square[S(A)`T(A)`S(B)`T(B);\tau(A)`S(f)`T(f)`\tau(B)]
\end{center}
{\sl is commutative.}

{\bf Defintion 19. (Initial Objects)}
{\sl Let $\cat{C}$ be a category.
An object $I \in \obj{C}$ is {\em initial in $\cat{C}$}
if for any object $A \in \cat{C}$ there is
a unique morphism $f \in \mor{C}{I,A}$.
If the category $\cat{C}$ is clear from the
context, then it is simply said that
$I$ is an {\em initial object}.}

If an initial object exists in a category, it is uniquely determined
up to isomorphism.

\subsection{The Type System of Axiom}

The type system of {\sf Axiom} consists of three levels:

\begin{enumerate}
\item elements,
\item domains,
\item categories.
\end{enumerate}

The elements belong to domains, which correspond to types in the
traditional programming terminology.

Domains are built by {\em domain constructors}, which are functions
having the following sort of parameters: elements, or domains of a
certain category.  Any domain constructor has a {\em category
assertion} part which asserts that for any possible parameters of the
domain constructor the constructed domain belongs to the categories
given in it.

\subsubsection{Categories}

Also categories are built by category constructors which are functions
having elements or domains as parameters.

An important subclass is built by the categories which are built by
category constructors having no parameters.  They are called the {\em
basic algebra hierarchy} in \cite{Jenk92} and consist up to now of 46
categories.

As is stated in \cite[p.~524]{Jenk92} the case of elements as
parameters of category constructors is rare.

In the definition of a category there is always a part in which the
categories are given the defined category extends.\footnote{The
category which is extended by all other categories and which does not
extend any other category is predefined and is called {\tt Type}.}

An important component of the definition of a category is the
documentation. There is even a special syntax for comments serving as
a documentation in contrast to other kinds of comments.  The
importance of having a language support for the documentation as well
as for the implementation of an algorithm is also clearly elaborated
in the design of the algorithm description language {\sf ALDES}
\cite{Loos72}, \cite{Loos76}, \cite{Loos92}, and in the
implementation of the SAC-2 library (see e.\,g.\ \cite{Coll90},
\cite{Buen91}).

The {\em axioms} which a member of a category has to fulfill are
stated in the comment only and there is no mechanism for an automated
verification provided yet.  There is a mechanism to declare some
equationally definable axioms as so called {\em attributes} which can
be used explicitly in the language.  However, the attributes can be
used only directly.  A theorem proving component is not included in
the system.

Some operations in a category definition can have {\em default}
declarations, i.\,e.\ algorithms for algorithmically definable
operations can be given. These default declarations can be overwritten
by special algorithms in any instance of a category.


There are two syntactic declarations which reduce the number of
category declarations which have to be given considerably.

Using the keyword {\tt Join} a category is defined which has all
operations and properties of the categories given as arguments to {\tt
Join}.

Instead of defining different categories ${\cal C}_1$ and ${\cal C}_2$
and to declare that ${\cal C}_2$ extends ${\cal C}_1$ it is possible
to define ${\cal C}_1$ and to use the so called {\em conditional
phrase} {\tt has} in the definition of ${\cal C}_1$ to give the
additional properties of ${\cal C}_2$.

\begin{figure}
\rule{\textwidth}{0.1pt}
%\begin{progverb}
\begin{footnotesize}
\begin{verbatim}
++ the class of all multiplicative semigroups
++ Axioms
++ .  associative("*":($,$)->$)            ++ (x*y)*z = x*(y*z)
++ Common Additional Axioms
++ .  commutative("*":($,$)->$)            ++ x*y = y*x
SemiGroup(): Category == SetCategory with
    --operations
      "*": ($,$) -> $
      "**": ($,PositiveInteger) -> $

    add
      import RepeatedSquaring($)
      x:$ ** n:PositiveInteger == expt(x,n)
\end{verbatim}
\end{footnotesize}
%\end{progverb}
\rule{\textwidth}{0.1pt}

\caption{An example of a category definition in {\sf Axiom}}
\label{figaxsegcat}
\end{figure}

\subsubsection{Coercions}

In {\sf Axiom} it is possible to have coercions between domains.
Syntactically, an overloaded operator symbol {\tt coerce} is used for
the definition of the coercion functions.  There seems to be no
restriction on the functions which can be coercions. So also partial
functions can be coercions in {\sf Axiom} in contrary to the usage of
the term in this chapter.

\section{Type Classes}
\label{chtycla}

In the main part of this section we will deal with language constructs
which correspond to {\em categories} of {\sf Axiom} obeying the
restriction of having no parameters.  In Sec.~\ref{chparamtycl} we
will discuss the case of categories with parameters.

The momentarily occurring examples of such categories are given as the
``basic algebra hierarchy'' on the inner cover page of the book on
{\sf Axiom} \cite{Jenk92}.  They consist of 46 categories.  The
maximal length of a chain in the induced partial order is 15.

These categories correspond to type classes of {\sf Haskell}, cf.\
Fig.~\ref{figaxord} and Fig.~\ref{fighasord}.  We will often use the
term {\em type class} --- which seems to be preferable --- instead of
non-parameterized category.

In \cite[Appendix~A]{Webe92b} the author has shown that almost all of
the examples of types occurring in the specifications of the {\sf
SAC-2} library (see e.\,g.\ \cite{Coll90}, \cite{Buch93}) can be
structured by using the language construct of type classes.

We will also assume that all domain constructors have only domains as
parameters, and not elements of other domains.  We will discuss the
extension of having elements of domains as parameters in
Sec.~\ref{chtydeel}.

\begin{figure}[h]
\rule{\textwidth}{0.1pt}
%\begin{progverb}
\begin{footnotesize}
\begin{verbatim}
class  (Eq a) => Ord a  where
  (<), (<=), (>=), (>):: a -> a -> Bool
  max, min            :: a -> a -> a

  x <  y              =  x <= y && x /= y
  x >= y              =  y <= x
  x >  y              =  y <  x

  -- The following default methods are appropriate for partial orders.
  -- Note that the second guards in each function can be replaced
  -- by "otherwise" and the error cases, eliminated for total orders.
  max x y | x >= y    =  x
          | y >= x    =  y
          |otherwise  =  error "max{PreludeCore}: no ordering relation"
  min x y | x <= y    =  x
          | y <= x    =  y
          |otherwise  =  error "min{PreludeCore}: no ordering relation"
\end{verbatim}
\end{footnotesize}
%\end{progverb}
\rule{\textwidth}{0.1pt}

\caption{Definition of partially ordered sets in the
{\sf Haskell} standard prelude}
\label{fighasord}
\end{figure}

\begin{figure}[h]
\rule{\textwidth}{0.1pt}
%\begin{progverb}
\begin{footnotesize}
\begin{verbatim}
++ Totally ordered sets
++ Axioms
++ . a<b or a=b or b<a (and only one of these!)
++ . a<b and b<c => a<c
OrderedSet(): Category == SetCategory with
  --operations
    "<": ($,$) -> Boolean    ++ The (strict) comparison operator
    max: ($,$) -> $          ++ The maximum of two objects
    min: ($,$) -> $          ++ The minimum of two objects
  add
  --declarations
    x,y: $
  --definitions
  -- These really ought to become some sort of macro
    max(x,y) ==
      x > y => x
      y
    min(x,y) ==
      x > y => y
      x
\end{verbatim}
\end{footnotesize}
%\end{progverb}
\rule{\textwidth}{0.1pt}

\caption{Definition of totally ordered sets
in {\sf Axiom}}
\label{figaxord}
\end{figure}

\subsection{Types as Terms of an Order-Sorted Signature}

The idea of describing the types of a computer algebra system as terms
of an order-sorted signature can also be found in the work of Rector
\cite{Rect89} and Comon,\ et~al.\ \cite{Como91}.  The idea of
describing the type system of {\sf Haskell} using order-sorted terms
is due to Nipkow and Snelting \cite{Nipk91}.

However, the combination of ideas found in these papers is new and
gives a solution to an important class of type inference problems
occurring in computer algebra.

In the following a {\em type} will just be an element of the set of
all order-sorted terms over a signature $(S, \leq,\Sigma)$ freely
generated by some family of infinite sets
$V=\{ V_\sigma \mid \sigma \in S\}$.

The sorts correspond to the non-parameterized categories, the basic
algebra hierarchy.  The order on the sorts reflects the inheritance
mechanism of categories.

The sets $V_\sigma$ are  the sets of {\em type variables}.

A type denoted be a ground term is called a {\em ground type}, a
non-ground type is called a {\em polymorphic type}.  Polymorphic types
correspond to the {\em modemaps} of {\sf Axiom}.

A type denoted by a constant symbol will be called a {\em base
type}. So base types correspond to domains built by domain
constructors without parameters.  (Typical examples are \tf{integer},
\tf{boolean}, \ldots)

The non-constant operator symbols are called {\em type constructors}.
The domain constructors of {\sf Axiom} which have only domains as
parameters can be described by type constructors.

We will use
$$\begin{array}{l}
\tf{list}: (\cf{any}) \cf{any}\\
\tf{list}: (\cf{ordered\_set}) \cf{ordered\_set}\\
\tf{UP}: (\cf{commutative\_ring} \; \cf{symbol}) \cf{commutative\_ring} \\
\tf{UP}: (\cf{integral\_domain} \; \cf{symbol}) \cf{integral\_domain}\\
\tf{FF}: (\cf{integral\_domain}) \cf{field}
\end{array}$$
as typical examples, where $\tf{UP}$ builds univariate polynomials in
a specified indeterminate of a commutative ring, and $\tf{FF}$ the
field of fractions of an integral domain.

Notice the use of multiple declarations, which can be achieved in
{\sf  Axiom} using the conditional phrase {\tt has}.

In the following we will sometimes assume that we have a {\em
semantics} for the {\em ground types} which satisfies the following
conditions:
\begin{itemize}
\item The ground types correspond to mathematical objects
in the sense of universal algebra or model theory
(A comprehensive reference for universal algebra is \cite{Grae79},
for model theory \cite{Chan90}).
\item Functions between ground types are set theoretical
functions. If we say that two functions $f,g: t_1 \longrightarrow t_2$
are equal ($f=g$) we mean equality between them as set theoretic
objects.
\end{itemize}

Since we only need a set theoretic semantics for {\em ground types}
and functions between ground types, the obvious interpretations of the
types as set theoretic objects will do.\footnote{All objects
corresponding to ground types one is interested in computer algebra
can be given such a set theoretic interpretation.  In other areas,
e.\,g.\ in the context of the lambda calculus \cite{Bare84} this
is not always the case.  Nevertheless, this is not a real problem for
our work, since our approach is primarily concerned with the situation
arising in computer algebra.}

Of course, equality between two functions will be in general an
undecidable property, but this will not be of importance in the
following discussion, since we will always give some particular
reasoning for the equality of two functions between two types.

We will also deal with polymorphic types in the following.  However,
it will not be necessary to have a formal semantics for the
polymorphic types in the cases we will use them.  Giving a semantics
to polymorphic types can be quite difficult.  So the one given in
\cite{Como91} applies to fewer cases than the ones we are
interested in.  In general, it is possible that no ``set-theoretic
semantics'' can be given to polymorphic types, as was shown by
Reynolds \cite{Reyn84} for the objects of the second-order
polymorphic lambda-calculus.

\subsubsection{Properties of the Order-Sorted Signature of Types}
\label{secproossty}

The possibility to have multiple declarations of type constructors is
used in {\sf Axiom} frequently.  Syntactically it is achieved by a
conditional phrase involving {\tt has}.\footnote{In {\sf Axiom}
conditional phrases are used also for other purposes. So it might be
useful to use different syntactic concepts instead of one.}

Also constant symbols, i.\,e.\ base types, have usually multiple
declarations, e.\,g.\ it is useful to declare $\tf{integer}$ to be an
$\cf{integral\_domain}$ and an $\cf{ordered\_set}$.  So the
monotonicity condition cannot be assumed in general.  However, for the
purposes of type inference (see below) this condition is not needed.

As is shown in \cite[Sec.~5]{Nipk91} it can be assumed that
the signature is regular\footnote{At least if the signature is
finite.} and downward complete if one allows to form the
``conjunction'' $\sigma_1 \wedge \sigma_2$ of sorts $\sigma_1$ and
$\sigma_2$.  This conjunction has to fulfill the following conditions:

\begin{enumerate}
\item $\sigma_1 \wedge \sigma_2$ has to be the meet of $\sigma_1$ and
$\sigma_2$ in the free lower semi-lattice on the partially ordered set
$\langle S, \leq \rangle$ (cf.\ Def. 5).
\item If a type constructor $\chi$ has declarations $\chi: (\gamma_1
\cdots \gamma_n) \gamma$ and $\chi: (\delta_1 \cdots \delta_n) \delta$
then it also has a declaration $$\chi: (\gamma_1 \wedge \delta_1 \:
\cdots \gamma_n \wedge \delta_n) \gamma \wedge \delta.$$
\end{enumerate}

Using {\tt Join} there is a possibility to form such conjunctions of
sort having the required properties in {\sf Axiom}.

\begin{remark} Maybe the choice of the name {\tt Join} in {\sf Axiom}
is somewhat misleading.  Although the {\tt Join} of two categories
gives a category having the union of their operations, this category
is nevertheless corresponding to the {\em meet} of the corresponding
sorts in the lower semi-lattice of sorts of the order-sorted signature
of types.  We cannot simply reverse the order on the sorts.  If a type
belongs to the join of two categories ${\cal A}$ and ${\cal B}$ we can
conclude that it belongs to ${\cal A}$ (or ${\cal B}$) but not vice versa!  
\end{remark}

For the purpose of type inference it would be nice if the signature is
unitary unifying.  This is the case for regular and downward complete
signatures if they are also {\em coregular}.  However, we do not know
whether a restriction implying coregularity is reasonable in the
context of a computer algebra system.

Nipkow and Snelting \cite{Nipk91} have argued that {\sf Haskell}
enforces that the order-sorted signatures are injective and subsort
reflecting which also imply that the signature is unitary unifying.

%These assumptions seem to be problematic in the context
%of computer algebra as the following example shows.
%Consider the
%type constructor
%$\tf{FF}$  building the field of fractions of an integral domain.
%Then the following declarations --- which reflect certain
%mathematical facts --- 
%would contradict the assumption that the
%signature is {\em injective}:

An example of a declaration which would prohibit that the signature is
{\em injective} is the following.  Consider the type constructor
$\tf{FF}$ building the field of fractions of an integral domain.  Then
the declarations
$$\begin{array}{l}
\tf{FF}: (\cf{integral\_domain}) \cf{field}\\
\tf{FF}: (\cf{field}) \cf{field}
\end{array}$$
correctly reflect certain mathematical facts.  Although it does not
seem to be necessary in this example to have the second declaration we
do not know whether there is an ``algebraic'' reason which implies
that declarations violating injectivity are not necessary.  So this
point might deserve further investigations.

\subsubsection{Definition of Overloaded Functions}

The formalism developed above is well suited to express the
overloading which can be performed by category definitions.

A declaration such as
%\begin{progverb}
\begin{footnotesize}
\begin{verbatim}
AbelianSemiGroup(): Category == SetCategory with
    --operations
      "+": ($,$) -> $            ++ x+y computes the sum of x and y
      "*": (PositiveInteger,$) -> $
\end{verbatim}
\end{footnotesize}
%\end{progverb}
would translate into 
$$\begin{array}{l}
\tf{+}: \forall t_{\cf{AbelianSemiGroup}} \, .\,
 t_{\cf{AbelianSemiGroup}} \times t_{\cf{AbelianSemiGroup}}
\longrightarrow t_{\cf{AbelianSemiGroup}},\\
\tf{*}: \forall t_{\cf{AbelianSemiGroup}} \, .\,
 \tf{PositiveInteger} \times t_{\cf{AbelianSemiGroup}}
\longrightarrow t_{\cf{AbelianSemiGroup}},
\end{array}
$$
where $t_{\cf{AbelianSemiGroup}}$ is a type variable of sort
$\cf{AbelianSemiGroup}$.  It is bounded by the universal quantifier
which has to be read that $t_{\cf{AbelianSemiGroup}}$ may be
instantiated by an arbitrary type of sort $\cf{AbelianSemiGroup}$.
This is just what we want.  So the definition of categories resp.\
type classes can be seen as a syntactic mechanism to give such
declarations of overloaded operators.  The mechanism to declare that a
category extends others can be simply modeled by the order relation on
the sorts in the order-sorted algebra of types --- if there are no
parameters in category definitions.\footnote{The inheritance mechanism
is certainly convenient for such a large system as {\sf Axiom} --- as
we have mentioned before, even the basic algebra hierarchy consists of
46 categories with chains of maximal length of 15 ---, although it can
be questioned whether it is really necessary, cf.\
\cite{Chen92}.}

An advantage of the syntactic form of type classes declarations is
certainly that the general declaration of the overloaded operators and
possible {\em default declarations} are collected in one piece of
code.  This collection improves readability and makes clear which
operators can have defaults and which cannot.

The value of default declarations may not be underestimated. They are
a good way to support rapid prototyping and will become more important
the bigger a system grows. They support the possibility to obtain
algorithms over new structures quite easily.  Since it is always
possible to ``overwrite'' a default operation by a more special and
efficient one their existence does not contradict the goal of having
algorithms which are as efficient as possible.

\newsavebox{\fodauxa}
\newsavebox{\fodauxb}
\newsavebox{\fodauxc}
\newsavebox{\fodauxd}
\newsavebox{\fodauxe}
\newsavebox{\fodauxf}
\newsavebox{\fodauxg}
\newsavebox{\fodauxh}
\sbox{\fodauxa}{
\begin{minipage}[l]{0.2\textwidth}
\hfil \large \sf plus \\
\it integer \\
polynomial \\
matrix 
\end{minipage}
}
\sbox{\fodauxb}{
\begin{minipage}[l]{0.2\textwidth}
\hfil \large \sf minus \\
\it integer \\
polynomial \\
matrix 
\end{minipage}
}
\sbox{\fodauxc}{
\begin{minipage}[l]{0.2\textwidth}
\hfil \large \sf times \\
\it integer \\
polynomial \\
matrix 
\end{minipage}
}
\sbox{\fodauxd}{
\begin{minipage}[l]{0.2\textwidth}
\hfil \large \sf divide \\
\it integer \\
polynomial \\
matrix 
\end{minipage}
}
\sbox{\fodauxe}{
\begin{minipage}[l]{0.2\textwidth}
\hfil \large \sf integer \\
\it plus \\
minus \\
times \\
divide 
\end{minipage}
}
\sbox{\fodauxf}{
\begin{minipage}[l]{0.2\textwidth}
\hfil \large \sf polynomial \\
\it plus \\
minus \\
times \\
divide 
\end{minipage}
}
\sbox{\fodauxg}{
\begin{minipage}[l]{0.2\textwidth}
\hfil \large \sf matrix \\
\it plus \\
minus \\
times \\
divide 
\end{minipage}
}

\sbox{\fodauxh}{
\begin{minipage}[l]{0.99\textwidth}
\begin{center}
\fbox{\usebox{\fodauxa}}\quad\quad\fbox{\usebox{\fodauxb}}\\
\vspace{1cm}
\fbox{\usebox{\fodauxc}}\quad\quad\fbox{\usebox{\fodauxd}}\\
\vspace{1cm}
{\large\it Operation-Centered}\\
\vspace{2cm}
\fbox{\usebox{\fodauxe}}\quad \fbox{\usebox{\fodauxf}}\quad
 \fbox{\usebox{\fodauxg}}\\
\vspace{1cm}
{\large\it Type-Centered}
\end{center}
\end{minipage}
}

\begin{figure}
\framebox[\textwidth][l]{
\hfil\usebox{\fodauxh}
}

\caption{Some terminology from Foderaro's thesis}
\label{figfodterm}
\end{figure}

\label{oopfod}

In his thesis \cite{Fode83} Foderaro distinguishes between an
``operation centered'' method and a ``type centered'' or
``object-oriented'' view of organizing data (cf.\
Fig.~\ref{figfodterm}) and argues why the type-centered approach has
to be preferred.

However, in our formalism these two views are essentially equivalent.
There is a translations of a declaration of a type class --- say
$\cf{Ring}$ --- and an instantiation of it --- say with $\tf{integer}$
--- with operations $\tf{integer\_plus}$ and $\tf{integer\_times}$
into declarations
$$\begin{array}{l}
\tf{+}: \forall t_{\cf{Ring}} \, .\,
 t_{\cf{Ring}} \times t_{\cf{Ring}}
\longrightarrow t_{\cf{Ring}},\\
\tf{*}: \forall t_{\cf{Ring}} \, .\,
t_{\cf{Ring}}  \times t_{\cf{Ring}}
\longrightarrow t_{\cf{AbelianSemiGroup}},
\end{array}
$$
whereas it can be deduced by a type inference algorithm that
$\tf{integer\_plus}$ has to be used for $\tf{+}$ if $t_{\cf{Ring}}$ is
instantiated with the type constant $\tf{integer}$.  We will present
this inference algorithm in the next section.

\subsection{Type Inference}
\label{sectytycl}

In the following we will show that the type inference problem is
decidable.  We will sketch the proof which is due to Nipkow and
Snelting \cite{Nipk91} because of its importance also for
computer algebra.

In Sec.~\ref{secaxhasex} we will give some examples in which the {\sf
Axiom} type inference mechanism fails whereas in {\sf Haskell} a type
can be deduced.

\subsubsection{Type Inference Rules of Mini-Haskell}

In Fig.~\ref{figtyns} the type inference rules for the language
Mini-Haskell of Nipkow and Snelting \cite{Nipk91} are given.  This
language includes the central typing concepts of {\sf Haskell} but is
well suited for theoretical investigations since it is very small.
Many useful properties of an actual programming language can be seen
as ``syntactic sugar'' for the purpose of the type inference problem.

Mini-Haskell can only handle unary functions.  However, in this
assumption there is no loss in generality.  Since Mini-Haskell has
higher order-functions, a function of type
$$\tau_1 \times \tau_2 \longrightarrow \tau_3$$
can be  expressed by a function having type
$$\tau_1 \longrightarrow (\tau_2 \longrightarrow \tau_3),$$
a technique usually called {\em currying}.\footnote{After Haskell
B. Curry who has used this technique in his work on Combinatory
logic. Historically, already Sch\"onfinkel has used it in
\cite{Scho24}.}

The language does not have explicit recursion or pattern matching.
Although these are important properties of a programming language,
there is no loss in generality in the type inference problem if we
exclude them from the language.  There are well known translations of
pattern matching into expressions of the lambda-calculus, see e.\,g.\
\cite{Jone87}.  In principle, recursion can be expressed using
fixpoint combinators which only requires to have certain appropriately
typed functional constants (see e.\,g.\ \cite{Leis87}).

\begin{remark}
Having explicit recursion and some special typing rules for recursion
gives the possibility to assign typing to some recursive programs
which would be ill-typed otherwise (see e.\,g.\ \cite{Kfou88},
\cite{Tiur90}). However, in some of these systems type inference
becomes undecidable \cite{Tiur90}, \cite{Kfou93}.
\end{remark}

\begin{remark}
The so called ``anonymous functions'' in {\sf Axiom}
 \cite[Sec.~6.17]{Jenk92}  can simply be seen as
$\lambda$-abstracted expressions.
Since recursion can be expressed by the use of fixpoint combinators,
also $\lambda$-expressions without names can be
recursive,\footnote{Their ``names'' are bound variables!}  in contrast
to the remark in \cite[p.~168]{Jenk92}: ``An anonymous function cannot
be recursive: since it does not have a name, you cannot even call it
within itself!''
\end{remark}

In the following we will use the notation of Nipkow and Snelting
\cite{Nipk91} which has some syntactic differences to our standard
notation but should be clear from the context.  Since the type of
functions between $\tau$ and $\tau'$ has a special role in the
following there is a special notation for it and it is written as
$\tau \longrightarrow \tau'$.  The meta-variable $\chi$ ranges over
type constructors, where it is assumed that a finite set of them is
given (e.\,g.\ having $\tf{int}$, $\tf{float}$, $\tf{list}(\alpha)$,
$\tf{pair}(\alpha,\beta)$ as members, as in \cite{Nipk91}).

Formally, a typing hypothesis $A$ is a mapping from a finite set of
variables to types.  We will write
$$A+[x\mapsto \tau]$$ 
for the mapping which assigns $\tau$ to $x$ and is equal to $A$ on
${\rm dom}(A) - \{x\}$.\footnote{If $x \in {\rm dom}(A)$ its value
will be ``overwritten''.}  For signatures, the notation
$$\Sigma + \chi: (\overline{\gamma_n})\gamma$$ 
just means that a declaration  $\chi: (\overline{\gamma_n})\gamma$ 
is added to $\Sigma$.

\begin{figure}[t]
\newsavebox{\fignsaux}
\sbox{\fignsaux}{
\begin{minipage}[l]{0.995\textwidth}
$$\begin{array}{ll}
{\rm TAUT} & \frac{\displaystyle A(x) \succeq_\Sigma \tau}{\displaystyle (A, \Sigma) \vdash
                       x: \tau} \\
\\
{\rm APP}           & \frac{\displaystyle (A, \Sigma) \vdash  e_0 :
\tau \longrightarrow \tau' \quad (A, \Sigma) \vdash  e_1 }{\displaystyle (A, \Sigma) \vdash 
\tau' }\\
\\
{\rm ABS}           & \frac{\displaystyle (A+[x\mapsto \tau], \Sigma) 
\vdash  e : \tau}{\displaystyle
 (A, \Sigma) \vdash \lambda x.e: \tau \longrightarrow \tau}\\
\\
{\rm LET} & \frac{\displaystyle (A, \Sigma) \vdash  e_0 : \tau \: 
FV(\tau,A)=\{ \alpha_{\gamma_1}, \ldots , \alpha_{\gamma_k} \} \: 
(A+[x\mapsto \forall \overline{\alpha_{\gamma_k}}.\tau], \Sigma)
\vdash e_1 : \tau'}{\displaystyle (A, \Sigma)\vdash {\bf let} \: x= e_0 \: {\bf in} \:
e_1 : \tau} \\
\\
{\rm CLASS} &\begin{array}{l} 
(A, \Sigma) \vdash {\bf class} \: \gamma \leq \gamma_1, \ldots, \gamma_n \:
{\bf where} \: x_1: \forall \alpha_\gamma .\tau_1, \ldots, x_k: \forall
\alpha_\gamma . \tau_k : \\
\quad\quad (A + [x \mapsto \forall \alpha_\gamma .\tau_i \mid i=1..k],
\Sigma + \{\gamma \leq \gamma_j \mid j= 1..n\} ) 
\end{array} \\
\\
{\rm INST} & \frac{\displaystyle 
A(x_i) = \forall \alpha_\gamma . \tau_i \quad\quad 
(A, \Sigma) \vdash e_i: \tau_i[\chi(
\overline{\alpha_{\gamma_n}})/\alpha_\gamma] \quad\quad i=1..k
}{\displaystyle (A, \Sigma)\vdash
 {\bf inst} \: \chi : (\overline{\gamma_n})\gamma
\: {\bf where} \: x_1 = e_1, \ldots, x_k = e_k : (A, \Sigma +
 \chi: (\overline{\gamma_n})\gamma)}\\
\\
{\rm PROG} & \frac{\displaystyle 
(A_{i-1}, \Sigma_{i-1}) \vdash d_i: (A_i, \Sigma_i) \quad i=1..n
\quad\quad (A_n,\Sigma_n) \vdash e : \tau}{\displaystyle
(A_0,\Sigma_0) \vdash d_1; \ldots : d_n; e : \tau}
\end{array}$$
\end{minipage}
}

\framebox[\textwidth][l]{\usebox{\fignsaux}
}

\caption{The type inference rules for Mini-Haskell of
Nipkow \& Snelting}
\label{figtyns}
\end{figure}

In Fig.~\ref{figtyns} the following conventions are used.
$\overline{\alpha_{\gamma_n}}$ denotes the list
$\alpha_{\gamma_1}, \ldots, \alpha_{\gamma_n}$, 
with the understanding that the
$\alpha_{\gamma_i}$ are distinct type variables.  The first four rules
in the type inference system in Fig.~\ref{figtyns} are almost
identical to the rules of Damas and Milner for {\sf ML} typing
\cite{Dama82}.  There are two differences: all inferences
depend on the signature $\Sigma$ of the type algebra as well as the
set of type assumptions $A$.  Furthermore, generic instantiation in
rule {\rm TAUT} must respect $\Sigma$.  This is written
$\sigma \succeq_\Sigma \tau$ 
meaning that $\sigma$ has the form
$\forall \overline{\alpha_{\gamma_n}}. \tau_0$, 
there are $\tau_i$ of sort
$\gamma_i$ and $\tau=\tau_0[\tau_1/\alpha_{\gamma_1}, \ldots
,\tau_n/\alpha_{\gamma_n}]$.  The notation $FV(\tau)$ denotes the set
of free type variables in $\tau$; $FV(\tau,A)$ denotes $FV(\tau) - FV(A)$.

If no class and instance declarations are present, every type
constructor has the topmost sort as arity.

For a detailed discussion of the rules we refer to \cite{Nipk91}.
Notice that rule CLASS has no premises.  The symbol ``$:$'' has two
different meanings.  On the one hand it assigns a type to an
expression or a program. On the other hand it assigns a pair
consisting of a typing hypothesis and a signature to a {\bf class}- or
{\bf inst}-declaration.

We have presented the simpler form of the type inference system as can
be found in \cite{Nipk91}.  A problem is that the obtained
order-sorted signature $\Sigma$ need not be regular. However, if we
allow the formation of the conjunction of two sorts ---- which
corresponds to the {\tt join} of two categories in {\sf Axiom} ---
then the signature can be made regular (and downward complete).  So we
can assume w.\,l.\,o.\,g. that the signature is regular, omitting for
simplicity the slightly more complicated type inference rules for the
system handling these conjunctions of sorts.  For more details we
refer to \cite{Nipk91}.

The main result of \cite{Nipk91} can be stated in the following form.

{\bf Theorem 4. (Nipkow and Snelting)}
\label{decMH}
The type inference problem for Mini-Haskell can be 
effectively reduced to the computation of order-sorted unifiers
for a regular signature.
It is thus decidable and there is a finite set
of principal typings. If the signature is unitary unifying, then
there is a unique principal type.

%One of the main points is rule
%(ABS) in which {\bf declaration} ....
%only overloaded functions as instances of type classes

\subsubsection{Types of Functions}
\label{secaxhasex}

In this section we want to show that the above results on the type
system for {\sf Haskell} would allow an extension of the type system
of {\sf Axiom}.

\begin{figure}[t]
\rule{\textwidth}{0.1pt}
%\begin{progverb}
\begin{footnotesize}
\begin{verbatim}
->fac n == if n < 3 then n else n * fac(n-1)
                                                           Type: Void
->fac 10
   (2)  3628800
                                                Type: PositiveInteger
->g x == x + 1
                                                           Type: Void
->g 9
   Compiling function g with type PositiveInteger -> PositiveInteger
   (7)  10

                                                 Type: PositiveInteger
->g (2/3)
        5
   (8)  -
        3

                                                Type: Fraction Integer
 
->mersenne i== 2**i - 1
                                                            Type: Void
->mersenne

                       i
   (2)  mersenne i == 2  - 1
                                         Type: FunctionCalled mersenne
->mersenne 3
   Compiling function mersenne with type PositiveInteger -> Integer 

   (3)  7
                                                 Type: PositiveInteger
->addx  x == ((y :Integer): Integer +-> x + y)
                                                            Type: Void
>g:=addx 10
   Compiling function addx with type PositiveInteger -> (Integer -> 
   Integer) 

   (10)  theMap(*1;anonymousFunction;0;G1048;internal,502)
                                            Type: (Integer -> Integer)
\end{verbatim}
\end{footnotesize}
%\end{progverb}
\rule{\textwidth}{0.1pt}

\caption{Typing of some user-defined functions in {\sf Axiom}}
\label{figusdeffuax}
\end{figure}

\begin{figure}[t]
\rule{\textwidth}{0.1pt}
%\begin{progverb}
\begin{footnotesize}
\begin{verbatim}
fact 0 = 1
fact (n+1) = (n+1)*fact n
Phase TYPE:
fact :: Integral m => m -> m
\end{verbatim}
\end{footnotesize}
%\end{progverb}

\bigskip
%\begin{progverb}
\begin{footnotesize}
\begin{verbatim}
square x = x * x
Phase TYPE:
square :: Num t => t -> t
\end{verbatim}
\end{footnotesize}
%\end{progverb}

\bigskip
%\begin{progverb}
\begin{footnotesize}
\begin{verbatim}
mersenne i = 2^ i - 1
addx x    =  \y -> y+x
z::Integer
z=10
g = addx  z
h = g 3

Phase TYPE:
mersenne :: (Num tv57, Integral tv58) => tv58 -> tv57
addx :: Num tv59 => tv59 -> tv59 -> tv59
z :: Integer
g :: Integer -> Integer
h :: Integer
\end{verbatim}
\end{footnotesize}
%\end{progverb}
\rule{\textwidth}{0.1pt}

\caption{Corresponding typings in {\sf Haskell}}
\label{figusdeffuha}
\end{figure}

In {\sf Axiom} it is possible to have functions as objects, see
\cite[Sec.~6]{Jenk92} and Fig.~\ref{figusdeffuax}.  Although {\sf
Axiom} has the concept of functions as objects and it can usually
infer the type of objects, it cannot infer the type of functions.

Strictly speaking the inferred types {\tt Void} or
{\tt FunctionCalled mersenne} in Fig.~\ref{figusdeffuax}
are false, since they differ from the types when the functions are
explicitly typed by the user.

The problem seems to be that {\sf Axiom} can only infer ground types
and not polymorphic types.  For most purposes in computer algebra this
might be sufficient. However, the type of functions has to be
polymorphic in many cases.

In Fig.~\ref{figusdeffuha} it is shown that {\sf Haskell} can infer a
type for such functions.  The {\sf Haskell} syntax has to be read as
follows: {\tt Integral} is a type class to which {\tt Integer}
belongs. The typing expression for {\tt fact} has to be read as the
type of {\tt fact} is a function in one argument taking arguments of a
type in type class {\tt Integral} and returning an argument of the
same type; the type variable {\tt m} is bound in the expression and is
chosen arbitrarily.

By Theorem 4 we know that it is decidable whether there is a
typing of an expression and that there are only finitely many most
principal typings in the positive case.  As is discussed in
\cite{Nipk91} the restrictions on typings in {\sf Haskell} even imply
that there is always a single principal type. However, since we do not
know to what extend these assumptions will be justified in the area of
computer algebra, we will not claim the more special result stated in
Theorem 4.

For the purpose of this thesis we can stop at this point, since we are
interested in questions of typability and not in ones of code
generation.  A certain problem in {\sf Haskell} is that of {\em
ambiguity}.  Although all valid typings of an expression are instances
of a most general type (involving type variables) it may happen that
there is not enough information to generate code in an unambiguous
way.  Some discussions and examples of ambiguity can be found e.\,g.\
in \cite{Huda99}, \cite{Faxe02}, \cite{Nipk91}, However, since
this problem arises ``below'' the typing level, some new concepts seem
to be necessary in order to treat this problem formally, and the
author of this thesis does not know of any such formal approaches.

\subsubsection{A Possible Application of
Combining Type Classes and Parametric Polymorphism}
\label{posappcom}

As we have seen, we can extend a type system supporting type classes
with parametric polymorphism and functions as first-class citizens and
the type inference problem still remains decidable.

Such an extension of an {\sf Axiom} like type system seems to be
interesting in the area of computer algebra for several reasons.
First of all lists play an important role in computer algebra and many
typing issues related to lists are connected with parametric polymorphism.

But it seems to be possible to have some much further applications.
As is shown by Rydeheard and Burstall in \cite{Ryde88} it is possible
to encode many concepts of category theory as types in {\sf ML} and to
state several constructive properties of category theory as {\sf ML}
programs.  This encoding uses heavily the concepts of parametric
polymorphism and higher-order functions.  This formalism seems to be
very useful, although there is no perfect correspondence between the
objects of category theory and the types in {\sf ML}.\footnote{For
instance, the well-formedness of composites in a category is not a
matter of type-checking, cf.\ \cite[p.~58]{Ryde88}.  Other examples
can be found in \cite[Sec.~10]{Ryde88}.}

Now there are many well-known interactions between category theoretic
concepts and algebraic concepts, see e.\,g.\
\cite[Sec.~II.7]{Macl92} or \cite{Mane76} for interactions
of equational reasoning and category theory.  Since many concepts in
category theory are constructive, it seems to be possible to use some
of these connections in a computer algebra system.

\subsubsection{Typing of ``Declared Only'' Objects}

Consider the Axiom dialogue:
%\begin{progverb} 
\begin{footnotesize}
\begin{verbatim}
->a:Integer 
                                                            Type: Void 

->a+a a is declared as being in Integer but has not been given a value.
\end{verbatim} 
\end{footnotesize}
%\end{progverb}

Although a corresponding construct leads to a program error in {\sf
Haskell}, it could be typed by the {\sf Haskell} type inference
algorithm, if a declaration such as \verb!a: Integer! would just add
the corresponding typing assumption to the set of typing hypothesis.

Thus if we add a type declaration statement to the syntax of
Mini-Haskell\footnote{We will use {\bf has\_type} as an infix
operation in the object language for the typing declaration instead of
``:'' in order to distinguish between the object and the meta level in
rule {\rm (TYPE-AS)}.}  
$$ x\: {\bf has\_type }\: \tau,$$ 
then we simply need to add the following trivial rule to the ones given in
Fig.~\ref{figtyns}: 
$$\mbox{(TYPE-AS)} \quad\quad {\displaystyle (A,
\Sigma)\vdash x \:{\bf has\_type}\: \tau : (A+[x \mapsto \tau],
\Sigma)} $$

\subsection{Complexity of Type Inference}

\subsubsection{The ML-fragment}

The type inference problem for the simply typed lambda calculus,
i.\,e.\ the {\sf ML} core language without usage of {\tt let}
constructions reduces in linear time to a (syntactic) unification
problem. Using a representation of terms as directed acyclic graphs
(dags) the unification problem is decidable in linear time
\cite{Pate78}, and so is the type inference problem.

In \cite[p.~450]{Kane90} this result is stated in the
following precise form: 
\begin{quote} 
Given a {\tt let}-free expression $M$ of length $n$ (with all bound
variables distinct), there is a linear time algorithm which computes a
dag representation of the principal typing of $M$, if it exists, and
returns {\em untypeable} otherwise.  If it exists, the principal
typing of $M$ has length at most $2^{O(n)}$ and dag size $O(n)$.
\end{quote}

Even if {\tt let}-expressions are used, the type inference problem
remains decidable and can be solved using the Damas-Milner algorithm
\cite{Dama82}.  Unfortunately, the complexity becomes dramatically
worse.  In the worst case, doubly-exponential time is required to
produce a string output of a typing. Using a dag representation the
algorithm can be modified to run in exponential time, which is also
the proven lower (time complexity) bound of the problem (see e.\,g.\
\cite{Kane90}).

Nevertheless, {\sf ML} typing appears to be efficient in practice,
although {\tt let} expressions are frequently used in actual {\sf ML}
programs.\footnote{We refer to \cite{Kane90} for further
discussions of this point.}

\subsubsection{Complexity of Type Inference for the System of Nipkow
and Snelting}

If no {\tt let} expressions are used, then the type inference problem
for the system of Nipkow and Snelting can be reduced to an unification
problem for order-sorted terms.

This reduction is linear, so the inherent complexity of the problem is
the same as the one of corresponding unification problem.

However, the resulting signature need not be regular.  By introducing
``conjunctive sorts'' Nipkow and Snelting show how the signature can
be made regular.  This process consists of building new sorts for any
finite subset of the set of sorts introduced by the {\tt class} and
{\tt inst} declaration.  This construction is thus exponential in the
number of {\tt class} and {\tt inst} declaration of the program.

The unification problem for regular order-sorted signatures is
decidable.  However, in finite and regular signatures, deciding
whether an equation is unifiable is an NP-complete problem (see
\cite[Corollary~10]{Smol89}).

The situations is much better, if the signature is also coregular and
downward complete, since in this case unification has quasi-linear
complexity \cite[Theorem~18]{Smol89}.

Since for many programs of the system the {\tt class} and {\tt inst}
declarations are the same, the type inference problem is of feasible
complexity if the obtained signature is coregular\footnote{By
construction, it is regular and downward complete.} and we view this
signature as pre-computed.

Of course, if {\tt let} statements are used, a lower bound bound for
the complexity is exponentially.  The complexity of various type
systems for {\sf Haskell}-like overloading has been investigated in
\cite{Volp91}.

\subsection{Algebraic Specifications of Type Classes}

Many important classes of objects occurring in computer algebra can be
defined by a finite set of equations, e.\,g.\ monoids, groups, Abelian
groups, or rings.

So the corresponding type class can be specified by an algebraic
specification (see e.\,g.\ \cite{Ehri85}, \cite{Wirs91}) if we use the
class of all models of the specification as the semantics of the
specification, which is usually called the {\em loose semantics}.

\begin{remark} 
Usually, an algebraic specification is thought to specify abstract
data types in the sense {\sf Axiom} or {\sf Haskell}.  So very often
the {\em initial semantics} is used, i.\,e.\ the specified object is
the initial object in the category\footnote{Category in the category
theoretic sense!} of structures being models of the specification.  A
major advantage of this view is that many structures one is interested
in --- e.\,g.\ the rational numbers, stacks, queues, \ldots --- can be
specified by (sorted or order-sorted) equations.  A characterization
of structures which can be specified by the initial semantics can be
found in \cite{Hodg95}.
\end{remark}

So much of the work on algebraic specifications using the loose
semantics are relevant for the specification type classes.  Many
references to such work are given in the survey of
Wirsing~\cite{Wirs91}.

\subsubsection{Some Hard-to-Specify Structures}

Unfortunately, some very basic structures, namely integral domains
(and fields) cannot be specified by equations, even if we allow
equational implications.  This is a consequence of the following
simple fact.

{\bf Lemma 4.}
{\sl The class of integral domains is not closed under the formation of
products.}

\begin{proof} 
Let $A$, $B$ be two arbitrary integral domains (of
cardinality $\geq 2$).  Let $0 \neq a \in A$ and $0 \neq b \in B$.
Then $(a,0) \cdot (0,b) = (0,0)=0_{A \times B}$, i.\,e.\ the product
$A \times B$ has zero divisors.  \qed 
\end{proof}

The following well known theorem shows the problem.

{\bf Theorem 5.}
{\sl A class $V$ of algebras\footnote{Algebra in the sense
of universal algebra.} is definable by equational implications iff $V$
is closed under the formation of isomorphic images, products,
subalgebras, and direct limits.}

\begin{proof} 
See \cite[p.~379]{Grae79}. \qed 
\end{proof}

Combining these results we obtain our claim.

{\bf Corollary 5A}
{\sl The class of integral domains is not definable by equational
implications.}

Since the technique of conditional term rewriting systems handles
reasoning for equational implications (cf.\ \cite[Sec.~11]{Klop90},
\cite{Ders89}) even this powerful technique is to weak to be used as a
mechanical tool for the specification of these examples.\footnote{At
least, if we do not allow some coding of information.}

Clearly, integral domains or fields can be defined by a finite set of
first-order formulas.  Unfortunately, it is not possible to define
them by Horn clauses, which would be one of the next classes of more
powerful specification formalisms which are well known (cf.\
\cite{Wirs91}) and have a much better computational behavior than
arbitrary first-order formulas.\footnote{The success of {\sf PROLOG}
as a programming language is partly due to this fact.}

{\bf Proposition 1.}
{\sl Let ${\cal M}$ be a model-class of a first-order theory.  If ${\cal
M}$ is not closed under products, then the first-order theory of
${\cal M}$ cannot be axiomatized by a set of Horn sentences.}

\begin{proof} 
The claim follows immediately from the fact that Horn
sentences are preserved under direct products (see e.\,g.\
\cite[Prop.~6.2.2]{Chan90}).  \qed 
\end{proof}

Though most of the examples given as the ``Basic Algebra Hierarchy''
in \cite{Jenk92} can be seen as model classes of finite sets of
first-order sentences, there are some which are model classes of a set
of first-order sentences --- even if we allow infinite sets.  An
example is the category $\cf{Finite}$.

{\bf Lemma 5.}
{\sl There is no set of first-order sentences whose model
class is the class of all finite sets.}

\begin{proof} 
If a set of first-order sentences has finite models of
arbitrary large finite cardinality, then it also has an infinite
model.  \qed 
\end{proof}

\begin{remark} 
In \cite{Dave90} it is shown that there are several quite simple
operations in basic classes (such as integral domains) which cannot be
defined constructively although they can be easily specified.  So the
meaning of a certain type class given there is that of a collection of
all domains in which all the specified operations can be interpreted
constructively.  In \cite{Dave91} the technique of introducing classes
in which a operation can be defined constructively is applied to the
problem of factorization of polynomials.
\end{remark}

\subsubsection{Algebraic Theories}

So it seems to be a wise decision in the design of {\sf Axiom} to
distinguish between ``axioms'' which are only stated in comments and
give the intended meaning of an {\sf Axiom} category as a class of
algebraic structures and ``attributes'' that can be ``explicitly
expressed'' \cite[p.~522]{Jenk92}.

The parts which can be explicitly expressed by the {\sf Axiom} system
consists of equational properties only and are even a small subset of
them.  Applying the rich machinery of algebraic specifications
techniques seems to be a possibility to extend the properties that are
``explicitly expressed'' considerably.

Moreover, there are many well known specifications of structures which
are present as domains in {\sf Axiom}.  It seems to be an interesting
field of further research to clarify the interaction between
algebraically specified categories and algebraically specified
domains.

The following extension of the work of Rector \cite{Rect89} is a
first approach in this direction: Assume that only finitely many sorts
and operation symbols are used for the specification ${\cal D}$ of a
certain domain and of the specification ${\cal C}$ of a certain type
class.  We can use different semantics as the initial semantics for
the specification of the domain and the loose semantics for the
specification of the type class.  Then it can be deduced automatically
whether the domain is a member of the type class in the following way:
Generate the finitely many mappings which are potentially a view of
${\cal D}$ as ${\cal C}$ and check algorithmically whether this
mapping is a view.\footnote{We refer to \cite[p.~303]{Rect89} for
the precise definitions of the used terms.}  The possibility of giving
certain specifications an initial semantics and of giving others a
loose semantics is also built in {\sf OBJ} (cf.\ \cite{Wirs91},
\cite{Gogu92}).  The former are called {\em objects}, the
latter {\em theories} and there is the possibility to define certain
mappings as views quite in the sense of above. However, the definition
of views has ``documentation aspect''.  A verification that a given
mapping is a view is not implemented (cf.\ \cite[Sec.~4.3]{Gogu92}).

As we have seen it is not possible to specify all structures used in a
computer algebra system by equations.  Their are several possibilities
to overcome this problem: 
\begin{enumerate} 
\item Use more powerful specification techniques.  
\item Do not specify all structures 
{\em ab initio}, but take some of the structures as given.  
\end{enumerate}

The first possibility is used in \cite{Limo92}.  There the
framework of first-order logic was chosen for the specification of
structures arising in computer algebra.  However, as we have shortly
discussed, even this framework cannot handle all interesting cases.

Moreover, for an efficient system it is necessary that certain parts
of a system have to be implemented by algorithms which are not the
result of a formal specification.  So the combination of taking
certain parts as given and using equational reasoning for the formal
part whose computational behavior is much better than the one of more
powerful techniques seems to be a promising compromise between two
contradicting requirements.

Another advantage of this approach is that already much is known about
mathematical structures which can be specified in this way as e.\,g.\
the book by Manes on ``Algebraic Theories'' \cite{Mane76} shows:
\begin{quote} The program of this book is to define for a ``base
category'' ${\cal K}$ --- a system of mathematical discourse
consisting of objects whose structure we ``take for granted'' ---
categories of ${\cal K}$-objects with ``additional structure,'' to
prove general theorems about such algebraic\footnote{Here
``algebraic'' means equationally definable.}  situations, and to
present examples and applications of the resulting theory in diverse
areas of mathematics.  \end{quote}

\subsubsection{Type Classes with Higher-Order Functions}

Type inference remains decidable for a system with type classes even
if higher-order functions are allowed in the way they are in {\sf
Haskell}.  As we have shown in Sec.~\ref{posappcom} such a combination
is interesting for computer algebra systems.

In order to specify such a system algebraically it is necessary to
extend the concepts of first-order algebraic specifications techniques
with higher-order constructs.  Some investigations of such
combinations are done in \cite{Brea89a} and in \cite{Joua91}.  The
results given there show that such a combination has feasible
properties, e.\,g.\ confluence and termination properties of the
first-order part are preserved when some reasonable conditions are
fulfilled.

\subsection{Parameterized Type Classes} 
\label{chparamtycl}

In {\sf Axiom} categories can be parameterized.  The occurring
examples can be distinguished in several ways.  On the one hand there
is the distinction between domains and elements as parameters.  On the
other hand there are several other distinctions based on more
``semantical'' considerations.

Some parameterized type classes simply arise because the classes of
algebraic objects should be described as being parameterized, e.\,g.\
vector spaces over a field $K$, or more generally, left- or
right-modules over a ring $R$.

An example of a category having an element as a parameter is
%\begin{progverb} 
\begin{footnotesize}
\begin{verbatim} 
PAdicIntegerCategory(p): Category == Definition where
  ++ This is the category of stream-based representations of
  ++ the p-adic integers.
\end{verbatim}
\end{footnotesize}
%\end{progverb} 
It describes all domains implementing the $p$-adic integers for a
given integer $p$.

\label{paramtyiso} 
\sloppy This is an example of a class of categories
used quite frequently in {\sf Axiom}.  The mathematical structures
corresponding to the domains which belong to the category {\tt
PAdicIntegerCategory(p)} are all isomorphic!  The reason for
introducing such a category seems to be the following.  For different
computations it is useful to have different representations of the
$p$-adic integers in a system.  %\fussy

\label{secisomor} 
The occurrence of categories in which all members are isomorphic (seen
as mathematical structures) are not limited to categories having
elements as parameters at all.  Examples of others are

\begin{center} {\tt \begin{tabular}{l} UnivariatePolynomialCategory(R:
Ring) \\ QuotientFieldCategory(D: IntegralDomain)\\
UnivariateTaylorSeriesCategory(Coef)\\
UnivariateLaurentSeriesCategory(Coef)\\
SquareMatrixCategory(ndim,R,Row,Col) \end{tabular} } 
\end{center}

However, the case of elements as parameters for categories --- which
is claimed to be rare in \cite[p.~524]{Jenk92} --- seems to be
restricted to such categories.\footnote{This was the result of an
incomplete check of the source code of {\sf Axiom} by the author.}

It seems to be useful to treat this class of type classes by a new
concept and not only as a special case of the general one of type
classes.  The reason is the following: Formally, these type classes
correspond exactly to the concept of abstract data type in the sense
of algebraic specification as is e.\,g.\ defined by Wirsing
\cite{Wirs91}.  Since the initial and the loose semantics
coincide\footnote{We will assume that there are only at most countable
structures as members of a certain class.  Most properties we are
interested in are still valid if we look at the subclasses of classes
which consist of at most countable structures, cf.\ \cite{Hodg95}.}
the distinction between first-order and second-order types becomes a
problem.  However, such a distinction is very desirable, as we will
show below.

\subsubsection{Sequences}
\label{chapseq}

In {\sf Axiom} the operator $\tf{map}$ is defined by a simple
overloading for several cases, such as matrices, vectors, quotient
fields, \ldots

Using a parameterized type constructor $\tf{sequence}$ as in
\cite{Chen92} this form of ad-hoc polymorphism in {\sf Axiom}
could be changed to a form of type-class polymorphism.  A
parameterized category such as $\tf{HomogeneousAggregate}$ of the
``data structure hierarchy'' of {\sf Axiom} seems to have almost the
same intended meaning as $\tf{sequence}$. So it seems to be possible
even in {\sf Axiom} to define $\tf{map}$ in
$\tf{HomogeneousAggregate}$ and to have the algebraic examples as
instances.  In Sec.~\ref{s43} we will use this view in order to show
that many coercions will fulfill a condition that leads to a coherent
type system.

\subsubsection{Type Inference}

In \cite{Chen92} an extension of the type system of {\sf Haskell}
is given allowing {\em types} as arguments in type classes.  It is
then proved that the type inference problem for parameterized type
classes is decidable.

As we have argued above a restriction of category constructors to have
domains as parameters only in {\sf Axiom} does not seem to be a severe
restriction for the type system of {\sf Axiom}.  In
Sec.~\ref{undetychtydeel} we will show that not only type inference
but even type checking for a system having types depending on elements
is undecidable.  The proof of undecidability given there can be easily
applied to the case of categories having elements as parameters.  So
it seems to be useful not to allow elements as parameters for category
constructors.

A certain problem in the proof given in \cite{Chen92} is that an
entirely new technique is used which cannot be seen as an extension of
the approach of Nipkow and Snelting using order-sorted unification.
However, such an extension would be desirable.  Since we have to add
other typing constructs to the language, it is desirable to have a
well understood theory behind one aspect of the typing problem instead
of using ad-hoc approaches.

Smolka \cite{Smol88}, \cite{Smol89a} extends the framework of
order-sorted algebras by introducing functions having sorts as
parameters.  So if we were looking at category constructors which take
categories as arguments we could directly apply the results of Smolka.
However, it is not clear whether these results are also useful for the
cases we are interested in.

\subsubsection{Algebraic Specifications of Parameterized Type Classes}

As in the case of type classes, any specifications using the loose
approach can be seen as specifications of parameterized type classes.
In the survey of Wirsing \cite{Wirs91} the relevant literature is
cited.  Especially, in \cite{Wirs82} the important {\em pushout
construction} for parameterized specifications has been studied.

\subsection{Type Classes as First-Order Types}

Categories in the type system of {\sf Axiom} resp.\ type classes in
the one of {\sf Haskell} are second-order types.

By our general assumption first-order types have to correspond to
structures in the sense of model theory or universal algebra.

We will briefly discuss to what extend this assumption is justified in
various areas.

\subsubsection{Group Theory} 
\label{sgroupth} 

As the {\sf Axiom} library shows the assumption of types corresponding
to mathematical structures makes good sense for many objects of
computer algebra with the exception of group theory programs.  In a
group theory program many algorithms take certain groups as input and
return other groups --- very often subgroups --- as output.  So it is
reasonable to have the groups an algorithm works on as objects and not
as types in a program.  In this cases it seems to be more natural to
treat certain classes of groups, such as the finitely presented
groups, as a type, and not the groups themselves.  Many of the
algorithms of group theory depend on such a view of groups as objects.
In this way groups are implemented in the group theory program
{\sf GAP} \cite{GAPx17}.

Some group theoretical functions can be found in general purpose
computer algebra programs such as {\sf MAPLE} (see e.\,g.\
\cite[Sec.~4.2]{Char91a}) or {\sf Axiom} (see e.\,g.\
\cite[App.~E]{Jenk92}).  However, these are rather limited in power
and coverage compared to the special group theory programs which have
been developed in the last years ({\sf Cayley} \cite{Butl90},
{\sf GAP} \cite{GAPx17}).

The observation above shows that it is difficult to come up with a
design which can really integrate group theoretical algorithms and the
ones of other areas of computer algebra.  This problem can even be
seen within {\sf Axiom}. For instance, there are domains of
permutation groups defined in {\sf Axiom}. However, these domains are
not members of the {\sf Axiom} category $\cf{group}$!

On the other hand it would be very desirable if some results of such
group theoretic computations can be seen as types for other
computations --- such as the group of integers $\langle \ZZ, +
\rangle$ or the finite cyclic groups $\langle \ZZ_m, + \rangle$.

Of course, if types become objects, then second-order types become
first-order types.  Nevertheless, the problem which has to be solved
is that of the relationship between objects and types, and not that of
the relationship between types and type classes!\footnote{See
Chapter~\ref{chtydeel} of this thesis for further discussions.}

\subsubsection{Requirements of a System}

If types are structures, then the type classes correspond to model
classes of certain theories.  Can we assume that such model classes do
not appear as objects we will deal with?

Of course, as we have shown it makes good sense to view a type class
as an algebraic object, namely the free term-algebra of order-sorted
terms of the sort of the type class.

However, even if we model those order-sorted algebras within our
system there is no need to view type classes as first-order types, as
long as we use ``isomorphic copies'' of them.  So we can even write
e.\,g.\ a compiler or a type inference algorithm in our system using
functions defined for those algebras.

The only thing we cannot model type safe are ``run-time'' interactions
between such a compiler and an algebraic algorithm.  But having
systems which use self-modifying code is anyway contradicting the
software-engineering principles we want to support by a type system.

As we have shown in Sec.~\ref{secisomor} there are several type
classes whose members are all isomorphic.  For reasons of efficiency
it is certainly necessary to distinguish these different members and
to provide different type constructors for them, such as having a type
constructor for univariate polynomials in sparse representation and
another one for univariate polynomials in dense representation.

However, it might be useful on the level of a user interface to have
only a {\em type constructor} ``univariate polynomial'' available for
the user without forcing him to choose a particular
representation.\footnote{Contrary to a person implementing algorithms
a user may be uncertain about the advantages of a particular
representation so that the choice be the system might be better than
the one of the user.}  In this case a {\em category constructor}
univariate polynomial would become a {\em type constructor} inducing
that certain type classes become first-order types.

Nevertheless, this seems to be useful only on the level of a user
interface and seems to be restricted to cases in which the isomorphism
between the types can be implemented in the system. Since such
categories can be seen as (finite) equivalence classes in the coercion
preorder (cf.\ Sec.~\ref{chtyiso}), these equivalence classes could be
easily implemented by a new special concept. Then there would still be
a clear distinction between first-order types (which would include the
constructs describing the equivalence classes) and the second-order
types of type classes.

\subsubsection{Universal Algebra}

In universal algebra, there are constructions which would imply the
view of type classes as first-order objects.  Namely, as in
\cite[Sec.~24]{Monk76}, one can construct for a class {\bf K} of
algebras the class {\bf S\,K} of substructures, or the class {\bf
P\,K} of products or the class {\bf H\,K} of homomorphic images of
{\bf K}.\footnote{More precisely, the class of structures which are
{\em isomorphic} to substructures (or products, or homomorphic images)
of elements of {\bf K}.}  Then many theorems can be stated as an
equation, e.\,g.\ Birkhoff's theorem has the form $$\mbox{{\bf K} is a
variety iff } {\bf K} = {\bf HSP\,K}.$$ Although such a formulation is
certainly elegant, it does not seem to be really necessary.  So the
additional difficulties which arise if one has to allow that type
classes are members of the ``equality type class'' do not seem to be
justified by the practical importance of such a construction.

In model theory the possibility of imposing an algebraic structure ---
e.\,g.\ the Lindenbaum algebra --- or a topological structure on sets
of formulas is used frequently. Via the correspondence between sets of
formulas and model classes such a structure can also be imposed on a
model class making it to an algebra or a topological space. However,
since the properties on the side of the set of formulas are more
useful people work with them and not with the model classes.  Many
books on model theory can serve as references for these remarks, some
comprehensive ones are \cite{Chan90}, \cite{Poiz85}.

\subsubsection{Category Theory}

The situation is different for category theory.  An important tool for
category theory is the possibility to have a category of all
(small)\footnote{Small means that the categories are sets in a set
theory and not proper classes.} categories as objects and the functors
as arrows, or having functor categories, etc.


In this case it is not possible to have a perfect correspondence
between types and type classes in our system and the objects of
category theory.  More generally, it is not possible to have such a
perfect correspondence between the concepts of category theory and a
{\em predicative}\footnote{The word ``predicative'' refers to the fact
that a universe of types is introduced only after all of its members
are introduced.}  type-theory such as Martin-L\"of's type theory
\cite{Mart80}, as is also discussed in \cite[Sec.~10]{Ryde88}.
This is certainly a problem since impredicative
type theories might have unwanted properties.
Impredicative variants of Martin-L\"of's system 
can have an undesirable computational behavior,
as is discussed e.\,g.\ in \cite{Meye86},  
\cite{Howe87}, \cite{Coqu86}.\footnote{This problem
is discussed in the literature under the names
{\em ``Type: Type''} --- referring to the problem
whether the collection of all types is a type ---
or {\em Girard's Paradox}, since Girard has shown in his thesis
\cite{Gira72} that the original version of Martin-L\"of's type theory
allowing such constructs is inconsistent with intuitionistic
mathematics which it was supposed to model.}

So it might be preferable to have a type system which allows some
modeling of category theory but not a perfect correspondence.

\subsubsection{Bounded Polymorphism}

So in the main area of computer algebra there seems to be no need for
a concept of type classes as first-order types. So we will only sketch
some language proposals in which such a concept could be modeled.  The
main idea is to have first-order types as ``bounds'' to polymorphic
constructs.

The notion of {\em bounded quantification} was introduced by Cardelli
and Wegner \cite{Card85} in the language Fun. This proposed
language integrated Girard-Reynolds polymorphism \cite{Gira72},
\cite{Reyn74} with Cardelli's first-order calculus of subtyping
\cite{Card88}.

\begin{remark}
The so called ``second-order polymorphic $\lambda$-calculus'' was
rediscovered independently by Reynolds \cite{Reyn74} as a formalism to
express ``polymorphism'' in programming languages.  Girard has
introduced his system $F$ as a proof theoretic tool to give a
consistency proof for second-order Peano arithmetic along a line of
proof theoretic research which has originated with G\"odel
\cite{Gode58}.  A proof that all $\lambda$-terms typeable in system
$F$ are strongly normalizable and that this theorem implies the
consistency of second-order Peano arithmetic can be found in the book
by Girard, et~al.\ \cite{Gira89}.
\end{remark}

Fun and its relatives have been studied extensively by programming
language theorists and designers.  A slight modification of this
language --- called {\em minimal Bounded Fun} or $F_\leq$ --- by
Curien and Ghelli was extensively studied by Pierce in his thesis
\cite{Pier91}.  Unfortunately, the type checking problem for this
language was proven to be undecidable by Pierce \cite{Pier91},
\cite{Pier91a}.

Syntactically, types can have the form
$$\forall \alpha \leq \sigma_1 \, . \, \sigma_2,$$
where $\alpha$ is a type variable and $\sigma_1$ and $\sigma_2$  are types.
Besides the usual rules asserting reflexivity and transitivity of
$\leq$ the following rule is essential:\footnote{For a detailed
discussion of the rules we refer to the thesis of Pierce
\cite{Pier91}.}
$$\frac{\Gamma \vdash \tau_1 \leq \sigma_1
\quad\quad
\Gamma, \alpha \leq \tau_1 \vdash \sigma_2 \leq \tau_2
}{\Gamma \vdash \forall \alpha \leq \sigma_1 \, . \, \sigma_2 \:
\leq \:
\forall \alpha \leq \tau_1 \, . \, \tau_2
} \eqno\mbox{(\sc Sub-All)}$$
The expressiveness of the language\footnote{Since type checking is
undecidable, it might be too expressive.}  comes from the fact that
first-order types are bounds for type variables.  The rule
$$x \in V_{\sigma'} \mbox{ and }\sigma' \leq \sigma
\: \Longrightarrow \: x \in T_\Sigma(V)_\sigma$$
constituting a part of the definition of order-sorted terms (cf.\
Def. 8) can be seen as a special form of rule ({\sc
Sub-All}) if one would restrict the system $F_\leq$ to cases which
distinguish between two kinds of types where only one kind is allowed
to be a bound.  The typing rules for Mini-Haskell (cf.\
Fig.~\ref{figtyns}) could be simulated by the typing rules for
$F_\leq$ using a similar distinction between types.

We will not develop a formal interpretation of Mini-Haskell in
$F_\leq$ which could be done along the lines sketched above because it
is not clear yet whether the additional expressiveness of $F_\leq$ is
useful for a computer algebra system or an extension by another system
would be more appropriate.

{\bf Relation to Object-Oriented Programming} 

There has been a lot of work in the last years to show how the notions
of {\em object-oriented programming}\footnote{Some books on
object-oriented programming and languages are \cite{Meye88},
\cite{Gold83}, \cite{Kirk89}, \cite{Birt80},
\cite{Stro95}.}  can be modeled in a type safe way by using
$F_\leq$ or a related system like the so called $F$-bounded
polymorphic second-order lambda calculus \cite{Cann89}.  Some
experimental languages based on such principles are {\sf TOOPL}
\cite{Bruc93} and {\sf Quest} \cite{Card91}.

As is argued e.\,g.\ in \cite{Limo92}, \cite{Temp92} and can
be seen by a language for symbolic computation as {\sf VIEWS}
\cite{Abda86} the principles of object-oriented programming
are important tools for the design of a computer algebra system.

However, as we have shown in Sec.~\ref{oopfod} and is discussed in
more detail in \cite{Huda92}, \cite{Berg92} some important principles
of object-oriented programming already come with the use of type classes.

There are some examples --- e.\,g.\ ones related to problems of strict
versus non-strict inheritance (see e.\,g.\ \cite{Limo92},\cite{Temp92}
--- which
cannot be expressed in the type system of {\sf Axiom} and which could
be expressed using more sophisticated techniques of object-oriented
programming.  However, as we will show in Sec.~\ref{coerinstr} there
are properties of a type system which cannot be expressed by
mechanisms of object-oriented programming alone but require an
additional concept.  So it may be preferable to use a system which is
as simple as possible, even if not every example can be expressed in
it.\footnote{There seems to be one single example which is used by
several authors --- e.\,g.\ in \cite{Limo92} and in \cite{Baum95} ---
implying the need of non-strict inheritance in a computer algebra
system!}

\section{Coercions}
\label{chapcoer}

In mathematics the convention to identify an object with its image
under an embedding is used frequently.  It is certainly one of sources
of strength of mathematical notation.  Very often certain structures
are constructed as being of quite different shape and then this
convention is used to identify one with a certain subset of another
one.  Some examples which are explained in many textbooks are the
``subset relationship''
$$\NN\subseteq\ZZ\subseteq\QQ\subseteq\RR\subseteq\CC,$$ embeddings of
elements of $\QQ$ in algebraic extensions of $\QQ$ or in a $p$-adic
completion, or the embeddings of elements of a commutative ring $R$ in
$R[x]$, \ldots

If these mathematical structures correspond to types in a system and
the embeddings are computable functions, then this convention can be
modeled by the use of {\em coercions}.

While the use of implicit conversions instead of explicit conversions
might be debatable for parts of a system in which new efficient
algorithms have to be written, it is certainly necessary for a user
interface.

\subsection{General Remarks}

We will assume that we have a mechanism to declare some functions
between types to be {\em implicit coercions\/} between these types (or
simply {\em coercions}).  If there is a coercion $\phi: t_1
\longrightarrow t_2$ we will write $t_1 \subtype t_2$.

\begin{remark} The requirement of set theoretic ground types and
coercion functions excludes some constructions --- if we gave all
types the ``obvious'' set theoretic interpretation ---, as the one
used in in \cite[Lemma~2]{Mitc91}, which assumes a coercion from the
space of functions $\tf{FS}(D,D)$ over some domain $D$ into this
domain.  Such coercions which correspond to certain constructions of
models of the $\lambda$-calculus (see e.\,g.\ \cite{Bare84}) seem to
be of theoretical interest only.  At least for the purpose of a
computer algebra system the requirement of set theoretic coercion
functions does not seem to be a restriction at all!  
\end{remark}

\subsection{Coherence}
\label{seccoh}

In a larger system, it is possible that there are different ways to
have a coercion from one type into another.  Following \cite{Brea91}
and \cite{Reyn91} we will call a type system {\em coherent}, if the
coercions are independent of the way they are deduced in the
system.\footnote{Notice that the term ``coherence'' is used similarly
in category theory (see e.\,g.\ \cite{Macl91}) but is used quite
differently in connection with order-sorted algebras (e.\,g.\ in
\cite{Wald92}, \cite{Gogu92}, \cite{Rect89}).}

In the following we will look at different kinds of coercions which
occur and we will state some conditions which will yield the coherence
of the system.  Besides the technical proof of the coherence theorem
we will give some informal discussions about the significance of these
conditions.

\subsubsection{Motivating Examples}

Consider the expression $$ \tf{t} - \left( \begin{array}{cc} 1 & 0 \\
3 & \frac{1}{2} \end{array} \right) $$ which --- as a mathematician
would conclude --- denotes a $2 \times 2$-matrix over $\QQ[\tf{t}]$
where $\tf{t}$ is the usual shorthand for $\tf{t}$ times the identity
matrix.  In an {\sf Axiom} like type system, this expression involves
the following types and type constructors: The integral domain \tf{I}
of integers, the unary type constructor \tf{FF} which forms the
quotient field of an integral domain, the binary type constructor
\tf{UP} which forms the ring of univariate polynomials over some ring
in a specified indeterminate, and the type constructor $\tf{M}_{2,2}$
building the $2 \times 2$-matrices over a commutative ring.


In order to type this expression correctly several of the following
coercions have to be used.

\begin{center} \xext=1600 \yext=1200
\begin{picture}(\xext,\yext)(\xoff,\yoff) \resetparms
\setsqparms[1`-1`-1`1;1100`700]
\putsquare(0,0)[\tf{UP}(\tf{I},\tf{\tf{t}})`\tf{UP}(\tf{FF}(\tf{I}),\tf{\tf{t}})
 `\tf{I}`\tf{FF}(\tf{I});```]
\putsquare(400,400)[\tf{M}_{2,2}(\tf{UP}(\tf{I},\tf{\tf{t}}))
`\tf{M}_{2,2}(\tf{UP}(\tf{FF}(\tf{I}),\tf{\tf{t}}))
`\tf{M}_{2,2}(\tf{I})`\tf{M}_{2,2}(\tf{FF}(\tf{I}));```]
\putmorphism(110,110)(1,1)[``]{140}1b
\putmorphism(110,810)(1,1)[``]{140}1b
\putmorphism(1210,110)(1,1)[``]{140}1b
\putmorphism(1210,810)(1,1)[``]{140}1b 
\end{picture} 
\end{center}

There are different ways to coerce $\tf{I}$ to
$\tf{M}_{2,2}(\tf{UP}(\tf{FF}(\tf{I}),\tf{\tf{t}}))$.  Of course one
wants the embedding of $\tf{I}$ in
$\tf{M}_{2,2}(\tf{UP}(\tf{FF}(\tf{I}),\tf{\tf{t}}))$ to be independent
of the particular choice of the coercion functions.

In this example this independence seems to be the case, but how can we
{\em prove} it?  Moreover, not all coercions which would be desirable
for a user share this property.  Consider e.\,g.\ the binary type
constructor ``direct sum'' $\oplus$ defined for Abelian groups.  One
could coerce $A$ into $A \oplus B$ via a coercion $\phi_1$ and $B$
into $A \oplus B$ via a coercion $\phi_2$. But then the image of $A$
in $A \oplus A$ depends on the choice of the coercion function!

\subsubsection{Definition}


Relying on the set theoretic semantics for our types and coercion
functions we can give the following definition of coherence.

{\bf Definition 20. (Coherence)}
\index{coherence|ii} 
\label{defcoh} 
{\sl A type system is {\em coherent} if the following condition is satisfied:

\begin{itemize} \item[] For any ground types $t_1$ and $t_2$ of the
type system, if $\phi,\psi: t_1 \longrightarrow t_2$ are coercions
then $\phi=\psi$.  
\end{itemize}}

\subsubsection{General Assumptions}

It will be convenient to declare each identity function on a type to
be an implicit coercion.

{\bf Assumption 1.}
\label{A1} 
{\sl For any ground type $t$ the identity on
$t$ will be a coercion.  If $\phi: t_1 \longrightarrow t_2$ and $\psi:
t_2 \longrightarrow t_3$ are coercions, then the composition $\phi
\circ \psi : t_1 \longrightarrow t_3$ of $\phi$ and $\psi$ is a
coercion.}

{\bf Lemma 6.}
\label{lem1} 
{\sl If assumption 1 holds, then the set of ground types as objects
together with the coercion functions as arrows form a category.}

\begin{proof} 
Since composition of functions is associative and the identity
function is a coercion, all axioms of a category are fulfilled.\qed
\end{proof}

In the following we will always assume that assumption 1 holds
even if we do not mention it explicitly.

\subsubsection{Base Types}

It is a good instrument for structuring data types to have only as few
types as possible as base types but to construct them by a type
constructor whenever possible.\footnote{As an example consider the
field of rational numbers, which can be constructed as the quotient
field of the integers.}

Since there are only very few coercions between base types the
following assumption seems to be easily satisfiable.

{\bf Assumption 2. (Base Types)}
\label{abasety} 
The subcategory of base types and coercions between base types forms a
preorder, i.\,e.\ if $t_1$ and $t_2$ are base types and $\phi,\psi:
t_1 \longrightarrow t_2$ are coercions then $\phi=\psi$.

\subsubsection{Structural Coercions}

{\bf Definition 21. (Structural Coercions)} 
{\sl The $n$-ary type
constructor ($n \geq 1$) $f$ induces a {\em structural coercion\/}, if
there are sets ${\cal A}_f \subseteq \{1, \ldots, n \}$ and ${\cal
M}_f \subseteq \{1, \ldots, n \}$ such that \index{ Af@${\cal
A}_f$|ii}\index{ Mf@${\cal M}_f$|ii} the following condition is
satisfied:}

{\sl Whenever there are declarations $f: (\sigma_1 \cdots \sigma_n)\sigma$
and $f: (\sigma'_1 \cdots \sigma'_n)\sigma'$ and ground types
$t_1:\sigma_1, \ldots, t_n:\sigma_n$ and $t'_1:\sigma'_1, \ldots,
t'_n:\sigma'_n$ such that $t_i=t'_i$ if $i \notin {\cal A}_f \cup
{\cal M}_f$ and there are coercions $$\begin{array}{ll} \phi_i: t_i
\longrightarrow t'_i, & \mbox{if }i \in {\cal M}_f,\\ \phi_i: t'_i
\longrightarrow t_i, & \mbox{if }i \in {\cal A}_f, \\ \phi_i = {\rm
id}_{t_i} = {\rm id}_{t'_i} , & \mbox{if }i \notin {\cal A}_f \cup
{\cal M}_f, \end{array}$$ then there is a {\em uniquely defined\/}
coercion }
$${\cal F}_f(t_1,\ldots,t_n,t'_1, \ldots, t'_n, \phi_1,
\ldots, \phi_n) : f(t_1,\ldots,t_n) \longrightarrow
f(t'_1,\ldots,t'_n).$$

{\sl The type constructor $f$ is {\em covariant in its $i$-th argument}, if
$i \in {\cal M}_f$.  \index{covariant!type constructor|ii}\index{type
constructor!covariant|ii} It is {\em contravariant in its $i$-th
argument}, if $i \in {\cal A}_f$.  \index{contravariant!type
constructor|ii} \index{type constructor!contravariant|ii}}

Instead of the adjective ``covariant'' we will sometimes use the
adjective ``monotonic'', and instead of ``contravariant'' we will
sometimes use ``antimonotonic'', because both terminologies are used
in the literature and reflect different intuitions which are useful in
different contexts.

{\bf Assumption 3. (Structural Coercions)}
\label{Astruct} 
{\sl Let $f$ be $n$-ary type constructor which induces a structural
coercion and let $f(t_1,\ldots,t_n)$, $f(t'_1,\ldots,t'_n)$, and
$f(t''_1,\ldots,t''_n)$ be ground types.  Assume that}
$$
\begin{array}{ll} t_i \subtype t'_i \subtype t''_i, & \mbox{if }i
\in {\cal M}_f,\\ t''_i \subtype t'_i \subtype t_i, & \mbox{if }i \in
{\cal A}_f, \\ t_i = t'_i=t''_i, & \mbox{if }i \notin {\cal A}_f \cup
{\cal M}_f.  
\end{array}$$ 
{\sl and let $\phi_i : t_i \longrightarrow
t'_i$, $\phi'_i : t'_i \longrightarrow t''_i$ (if $i \in {\cal M}_f$),
and $\phi'_i : t''_i \longrightarrow t'_i$, $\phi_i : t'_i
\longrightarrow t_i$ (if $i \in {\cal A}_f$) be coercion functions.
For $i \notin {\cal A}_f \cup {\cal M}_f$ let $\phi_i$ and $\phi'_i$
be the appropriate identities.}

{\sl Then the following conditions are satisfied: 
\begin{enumerate} 
\item ${\cal F}_f(t_1,\ldots,t_n,t_1, \ldots, t_n, {\rm id}_{t_1}, \ldots,
{\rm id}_{t_n})$ is the identity on $f(t_1,\ldots,t_n)$, 
\item ${\cal F}_f(t_1,\ldots,t_n,t''_1, \ldots, t''_n, \phi_1 \circ \phi'_1,
\ldots, \phi_n \circ \phi'_n) =$ \\ ${\cal F}_f(t_1,\ldots,t_n,t'_1,
\ldots, t'_n, \phi_1, \ldots, \phi_n) \circ {\cal
F}_f(t'_1,\ldots,t'_n,t''_1, \ldots, t''_n, \phi'_1, \ldots, \phi'_n).$ 
\end{enumerate} }

Let $f: (\sigma_1 \cdots \sigma_n) \sigma$ be an $n$-ary type
constructor which induces a structural coercion.  
\label{defcatsigmai}
Let $\cat{C}_{\sigma_i}$ be the category of ground types of sort
$\sigma_i$ as objects and the coercions as arrows, let
$\cat{C}_{\sigma_i}^{\rm op}$ be the dual category of
$\cat{C}_{\sigma_i}$ and let $\cat{C}_{\sigma_i}^{\rm triv}$ be the
discrete subcategory of the objects of $\cat{C}_{\sigma_i}$.  Define
$$ \cat{C}_i=\left\{ \begin{array}{ll} \cat{C}_{\sigma_i}, & \mbox{if
}i \in {\cal M}_f,\\ \cat{C}_{\sigma_i}^{\rm op}, & \mbox{if }i \in
{\cal A}_f,\\ \cat{C}_{\sigma_i}^{\rm triv}, & \mbox{if }i \notin
{\cal A} \cup {\cal M}_f.  \end{array} \right.  $$ Then
assumption 3 means that the mapping assigning $f(t_1,
\ldots, t_n)$ to the $n$-tuple $(t_1, \ldots, t_n)$ and assigning the
coercion $${\cal F}_f(t_1,\ldots,t_n,t'_1, \ldots, t'_n, \phi_1,
\ldots, \phi_n)$$ to the $n$-tuple $(\phi_1, \ldots, \phi_n)$ of
coercions is a {\em functor} from $$\cat{C}_1 \times \cdots \times
\cat{C}_n$$ into $\cat{C}_\sigma$.

\label{s43} 
Typical examples of type constructors which induce a
structural coercion are \tf{list}, \tf{UP}, $\tf{M}_{n,n}$, $\tf{FF}$.
These examples give rise to structural coercions, because the
constructed type can be seen as an instance of the parameterized type
class $\tf{sequence}$ (cf.\ Sec.~\ref{chapseq}).\footnote{The
sequences can be of fixed finite length, as in the case $\tf{FF}$
where it consists of two elements only, the numerator and the
denominator.}  The coercions between the constructed types are then
obtained by {\em mapping} the coercions between the type parameter
into the sequence.  Since a mapping of functions distributes with
function composition, assumption 3 will be satisfied by
these examples.

Although many examples of structural coercions satisfying
assumption 3 can be explained by this mechanism, there are
others, which will satisfy assumption 3 because of another
reason, so that the more general framework we have chosen is
justified.  For instance, it is another mechanism which gives rise to
the structural coercion in the case of the ``function space'' type
constructor, as is well known.\footnote{See e.\,g.\
\cite{Card86}.}  It is contravariant in its first argument and
covariant in its second argument, as the following considerations
show: Let $A$ and $B$ be two types where there is an implicit coercion
$\phi$ from $A$ to $B$.  If $f$ is a function from $B$ into a type
$C$, then $f \circ \phi $ is a function from $A$ into $C$.  Thus any
function from $B$ into $C$ can be coerced into a function from $A$
into $C$.  Thus an implicit coercion from $\tf{FS}(B,C)$ into
$\tf{FS}(A,C)$ can be defined, i.\,e.\ $\tf{FS}(B,C) \subtype
\tf{FS}(A,C)$.  If $C \subtype D$ by an implicit coercion $\psi$, then
$\psi \circ f$ is a function from $A$ into $D$, i.\,e.\ an implicit
coercion from $\tf{FS}(A,C)$ into $\tf{FS}(A,D)$ can be defined.  In
this case assumption 3 is satisfied because of the
associativity of function-composition.

Although many important type constructors arising in computer algebra
are monotonic in all arguments it is not justified to assume that this
property will always hold as was done in \cite{Como91}.  We
have already seen that the type constructor for building ``function
spaces'' is antimonotonic in its first argument.  Constructions like
the fixpoint field of a certain algebraic extension of $\QQ$ under a
group of automorphisms in Galois theory (see e.\,g.\
\cite{Zari75}, \cite{Marc77}, \cite{Lang05}) would give
other --- more algebraic examples --- of type constructors which are
antimonotonic.\footnote{In {\sf GAP} \cite{GAPx17} such constructs
are implemented as functions and not as type constructors, cf.\ the
discussion in Sec.~\ref{sgroupth}.  Nevertheless, the implementation
as type constructors seems to be a reasonably possibility.}

However, an assumption that all type constructors are monotonic or
antimonotonic in all arguments as in \cite{Fuhx90}, \cite{Mitc91}
still seems to be too restrictive for our purposes.

If one allows a type constructor building references (pointers) to
objects of a certain type as is possible in Standard ML or in the
system described by Kaes \cite{Kaes92}, then this type constructor is
neither monotonic nor antimonotonic.

There are also algebraic examples of type constructors which are
neither monotonic nor antimonotonic.  Consider e.\,g.\ the quotient
groups $G/G'$, where $G'$ is the derived subgroup of $G$ (see e.\,g.\
\cite[p.~28]{Robi96}).  Assume that $H$ can be embedded in
$G$. Then in general it is not possible to embed $H/H'$ in $G/G'$ or
vice versa.  Thus if one would have a type constructor building the
type $G/G'$ for a given group $G$, then this type constructor would be
neither monotonic nor antimonotonic.

\begin{remark} 
Of course, one has to restrict the groups in consideration to ones for
which the construction of $G/G'$ can be performed effectively.  One
such class of groups is that of the finite polycyclic groups
(cf. \cite{GAPx17}).
\end{remark}

\subsubsection{Direct Embeddings in Type Constructors}

{\bf Definition 22. (Direct Embeddings)}
\label{defdiem}
\index{direct embedding|ii}
\index{embedding!direct|ii} 
{\sl Let $f:(\sigma_1, \ldots, \sigma_n)\sigma$ be a $n$-ary type
constructor.  If for some ground types $t_1:\sigma_1, \ldots,
t_n:\sigma_n$ there is a coercion
function $$\Phi^{i}_{f,t_1,\ldots,t_n}: t_i \longrightarrow
f(t_1,\ldots,t_n),$$ then we say that {\em $f$ has a direct embedding
at its $i$-th position}.}

{\sl Moreover, let $${\cal D}_f= \{i \mid \mbox{$f$ has a direct embedding
at its $i$-th position}\}$$ \index{ Df@${\cal D}_f$|ii} be the {\em
set of direct embedding positions of $f$}.}

\begin{remark} 
In {\sf Axiom} the inverses of direct embeddings are
called {\em retractions} (cf.\ \cite[p.~713]{Jenk92}) assuming
that the direct embeddings are always injective.  Thus the usage of
the term in {\sf Axiom} is a special case of our usage of that term,
since in our terminology any partial function which is an inverse of
any injective coercion can be a retraction.

On the other hand the {\sf Axiom} terminology shows that the designers
of {\sf Axiom} have seen the importance of direct embeddings, even if
there is no special terminology for direct embeddings themselves but
only for their inverses!  
\end{remark}

\begin{remark} 
In a system, a type constructor represents a
parameterized abstract data type which is usually built uniformly from
its parameters.  So the family of coercion functions
$$\{\Phi^{i}_{f,t_1,\ldots,t_n} \mid t_i \in T_\Sigma(\{\})_{\sigma_i}
\}$$ will very often be just one ({\em polymorphic\/}) function.  In
this respect the situation is similar to the one in Sec.~\ref{s43}.
\end{remark}

{\bf Assumption 4. (Direct Embeddings)}
\label{Aemb} 
{\sl Let $f:(\sigma_1 \cdots \sigma_n)\sigma$ be a $n$-ary type constructor.

Then the following conditions hold: 
\begin{enumerate} 
\item $|{\cal D}_f|\leq 1$.  
\item The coercion functions which give rise to the
direct embedding are unique, i.\,e.\ if $\Phi^{i}_{f,t_1,\ldots,t_n}:
t_i \longrightarrow f(t_1,\ldots,t_n)$ and
$\Psi^{i}_{f,t_1,\ldots,t_n}: t_i \longrightarrow f(t_1,\ldots,t_n)$,
then $$\Phi^{i}_{f,t_1,\ldots,t_n}=\Psi^{i}_{f,t_1,\ldots,t_n}.$$
\end{enumerate} }

Many important type constructors such as $\tf{list}$, $\tf{M}_{n,n}$,
$\tf{FF}$, and in general the ones describing a ``closure'' or a
``completion'' of a structure --- such as the $p$-adic completions or
an algebraic closure of a field --- are unary.  Since for unary type
constructors the condition $|{\cal D}_f| \leq 1$ is trivial and the
second condition in assumption 4 should be always fulfilled,
the assumption holds in this cases.

For $n$-ary type constructors ($n \geq 2$) the requirement $|{\cal
D}_f| \leq 1$ might restrict the possible coercions.  Consider the
``direct sum'' type constructor for Abelian groups which we have
already seen that it could lead to a type system that is not coherent
if we do not restrict the possible coercions.  For a type constructor
$$\oplus: (\cf{Abelian\_group} \; \cf{Abelian\_group})
\cf{Abelian\_group}$$ the requirement $|{\cal D}_f| \leq 1$ means that
it is only possible to have either an embedding at the first position
or at the second position.

In the framework that we have used the types $A \oplus B$ and $B
\oplus A$ will be different.  However, the corresponding mathematical
objects are {\em isomorphic}.  Having a mechanism in a language that
represents certain isomorphic mathematical objects by the same type
(cf.\ Sec.~\ref{chtyiso}) the declaration of both natural embeddings
to be coercions would not lead to an incoherent type system.  Notice
that such an additional mechanism, which corresponds to factoring the
free term-algebra of types we regard by some congruence relation, will
be a conservative extension for a coherent type system. If a type
system was coherent, it will remain coherent. It is only possible that
a type system being incoherent otherwise becomes coherent.

Let $f:(\sigma \sigma')\sigma$ be a binary type constructor with
$\sigma$ and $\sigma'$ incomparable having direct embeddings at the
first and second position, and let $t : \sigma$ and $t' : \sigma'$ be
ground types such that $$t' \subtype f(f(t,t'),t').$$ Then there are
two possibilities to coerce $t'$ into $f(f(t,t'),t')$ which might be
different in general.  In the case of types $\tf{R} : \cf{c\_ring}$
and $\tf{x} : \cf{symbol}$ the coercions of $\tf{x}$ into
$\tf{UP}(\tf{UP}(\tf{R},\tf{x}),\tf{x})$ are unambiguous, if
$\tf{UP}(\tf{UP}(\tf{R},\tf{x}),\tf{x})$ and $\tf{UP}(\tf{R},\tf{x})$
are the same type.  However, it does not seem to be generally possible
to avoid the condition $|{\cal D}_f| \leq 1$ even in cases where a
type constructor is defined for types belonging to incomparable type
classes.

The naturally occurring direct embeddings for types built by the type
constructors $\tf{FF}$ and $\tf{UP}$ show that in the context of
computer algebra there are cases in which a coercion is defined into a
type belonging to an incomparable type class, into a type belonging to
a more general type class, into a type belonging to a less general
type class, or into a type belonging to the same type class.  So
coercions occur quite ``orthogonal'' to the inheritance hierarchy on
the type classes showing an important difference between the coercions
in computer algebra and the ``subtypes'' occurring in object oriented
programming (cf.\ Sec.~\ref{coerinstr}).

The next assumption will guarantee that structural coercions and
direct embeddings will interchange nicely.

{\bf Assumption 5. (Structural Coercions and Embeddings)}
\label{Accemb} 
{\sl Let $f$ be a $n$-ary type constructor which induces a
structural coercion and has a direct embedding at its $i$-th position.
Assume that $f:(\sigma_1 \cdots \sigma_n)\sigma$ and $f:(\sigma'_1
\cdots \sigma'_n)\sigma$, $t_1:\sigma_1, \ldots, t_n:\sigma_n$, and
$t'_1:\sigma'_1, \ldots, t'_n:\sigma'_n$.  If there are coercions
$\psi_i: t_i \longrightarrow t'_i$, if the coercions
$\Phi^{i}_{f,t_1,\ldots,t_n}$ and $\Phi^{i}_{f,t'_1,\ldots,t'_n}$ are
defined, and if $f$ is covariant at its $i$-th argument, then the
following diagram is commutative:}

\begin{center}
\setsqparms[1`1`1`1;2600`600]
\square[t_i`t'_i`f(t_1,\ldots,t_n)`f(t'_1,\ldots,t'_n);
\psi_i`
{\Phi^{i}_{f,t_1,\ldots,t_n}}`
{\Phi^{i}_{f,t'_1,\ldots,t'_n}}`
{{\cal F}_f(t_1,\ldots,t_n,t'_1,\ldots,t'_n,
\psi_1, \ldots, \psi_n)}]
\end{center}

{\sl If $f$ is contravariant at its $i$-th argument,
then  the following diagram is commutative:}
\begin{center}
\setsqparms[1`1`1`-1;2600`600]
\square[t_i`t'_i`f(t_1,\ldots,t_n)`f(t'_1,\ldots,t'_n);
\psi_i`
{\Phi^{i}_{f,t_1,\ldots,t_n}}`
{\Phi^{i}_{f,t'_1,\ldots,t'_n}}`
{{\cal F}_f(t_1,\ldots,t_n,t'_1,\ldots,t'_n,
\psi_1, \ldots, \psi_n)}]
\end{center}

The type constructors \tf{list}, \tf{UP}, $\tf{M}_{n,n}$ may serve as
examples of constructors which induce structural coercions and can
also have direct embeddings: It might be useful to have coercions from
elements into one element lists, from elements of a ring into a
constant polynomial or to identify a scalar with its multiple with the
identity matrix.

As was already discussed
in Sec.~\ref{s43}, in all these examples the
parameterized data types can be seen as sequences  and the
structural coercions ---
i.\,e.\ ${\cal F}_\tf{UP}(\tf{I},\tf{x},
\tf{FF}(\tf{I}),\tf{x},
\psi, {\rm id}_{\tf{x}})$ ---
can be seen as a kind of ``mapping'' operators.

The direct embeddings are  ``inclusions'' of
elements in these sequences.
Since applying a coercion function to such  an elements
and then ``including''  the result in a sequence
will yield the same result as first including
the element in the sequence and then ``mapping'' the
coercion function into the sequence,
assumption 5 will be satisfied by these examples.
For instance,
$${\cal F}_\tf{UP}(\tf{I},\tf{x},\tf{FF}(\tf{I}),\tf{x},
\Phi^{1}_{\tf{FF},\tf{I}}, {\rm id}_{\tf{x}})$$
is  the function which maps the coercion function
$\Phi^{1}_{\tf{FF},\tf{I}}$
to the sequence of elements of $\tf{I}$ in $\tf{UP}(\tf{I},\tf{x})$
which represents the polynomial.

Thus the diagrams
\begin{center}
\resetparms
\setsqparms[1`1`1`1;1000`500]
\square[\tf{I}`\tf{FF}(\tf{I})`
\tf{UP}(\tf{I},\tf{\tf{t}})`\tf{UP}(\tf{FF}(\tf{I}),\tf{\tf{t}});```]
\end{center}
and
\begin{center}
\resetparms
\setsqparms[1`1`1`1;1000`500]
\square[\tf{I}`\tf{UP}(\tf{I},\tf{\tf{t}})`
\tf{M}_{2,2}(\tf{I})`\tf{M}_{2,2}(\tf{UP}(\tf{I},\tf{\tf{t}}));```]
\end{center}
and
\begin{center}
\resetparms
\setsqparms[1`1`1`1;1000`500]
\square[\tf{I}`\tf{FF}(\tf{I})`
\tf{M}_{2,2}(\tf{I})`\tf{M}_{2,2}(\tf{FF}(\tf{I}));```]
\end{center}
which are instances of the diagrams
in assumption 5 are commutative.\footnote{The
first of these diagrams can also be found in
\cite{Fort90}.}

\bigskip
If the mathematical structure
represented by a type $t_i$
in assumption 5 has non-trivial
automorphisms, then it is  possible to
construct the structural coercion
$${{\cal F}_f(t_1,\ldots,t_n,t'_1,\ldots,t'_n,
\psi_1, \ldots, \psi_n)}$$
in a way such that the assumption is
violated: just apply a non-trivial automorphism
to $t_i$!
However, such a construction seems to be artificial.
Moreover, the argument shows that 
a possible violation of assumption 5
``up to an automorphism'' can be avoided by an
appropriate definition of 
$${{\cal F}_f(t_1,\ldots,t_n,t'_1,\ldots,t'_n,
\psi_1, \ldots, \psi_n)}.$$

\subsubsection{A Coherence Theorem}

We are now ready to state the main result of this section.
The assumptions 1, 2, 3, 4, and 5 are
 ``local'' coherence conditions
imposed on the coercions of the type system.
In the following theorem we will prove
that the type system is ``globally'' coherent,
if these local conditions are satisfied.

{\bf Theorem 6. (Coherence)}
\label{thmain}
{\sl Assume  that all coercions between ground types
are only built by one of the following mechanisms:
\begin{enumerate}
\item coercions between base types;
\item coercions induced by structural coercions;
\item direct embeddings in a type constructor;
\item composition of coercions;
\item identity function on ground types as coercions.
\end{enumerate}
If the assumptions 1, 2, 3, 4, and 5 are satisfied,
then the set of  ground types as objects and the coercions
between them as arrows form a category which is a preorder.}

\begin{proof}

By assumption 1 and lemma 6 the
set of  ground types as objects and the coercions
between them as arrows form a category.

For any two ground types $t$ and $t'$ we will
prove by induction on the complexity
of $t'$ that if
$\phi, \psi : t \longrightarrow t'$ are coercions
then $\phi=\psi$ which will establish the theorem.

If $\com(t')=1$ then we  have $\com(t)=1$ because of 
the assumption on the possible mechanisms for building coercions.
Since $\com(t)=1$ and $\com(t')=1$  the claim 
follows from assumption 2.

Now assume  that the induction hypothesis holds
for $k$
and  let $\com(t')=k+1$.
Thus we can assume that
$t'=f(u_1,\ldots,u_n)$ for some $n$-ary type
constructor $f$. 

Let $\phi, \psi : t \longrightarrow t'$ be coercions.


The coercions $\phi$ and $\psi$ are
compositions of coercions
between  base types, direct embeddings in type
constructors and structural coercions.
Because of assumption 3
and the induction hypothesis we
can assume that there are ground
types $s_1$ and $s_2$ and
unique coercions $\psi_1: t \longrightarrow s_1$
and  $\psi_2: t \longrightarrow s_2$
such that
\begin{equation}
\label{e1}
\phi =  {\cal F}_f(\ldots, t, \ldots , s_1,
\ldots , \psi_1, \ldots )
\end{equation}
or
\begin{equation}
\label{e2}
\phi= \psi_1 \circ \Phi^i_{f, \ldots, s_1, \ldots}
\end{equation}
Similarly,
\begin{equation}
\label{e3}
\psi =  {\cal F}_f(\ldots, t, \ldots , s_2,
\ldots , \psi_2, \ldots )
\end{equation}
or
\begin{equation}
\label{e4}
\psi= \psi_2 \circ \Phi^j_{f, \ldots, s_2, \ldots}
\end{equation}
If $\phi$ is  of form \ref{e1} and $\psi$ is
of form \ref{e3}, then $\phi=\psi$
because of assumption 3
and the uniqueness of
${\cal F}_f$.
If $\phi$ is  of form \ref{e2} and $\psi$ is
of form \ref{e3}, then $\phi=\psi$
because of assumption 5.
Analogously for $\phi$ of form \ref{e1}
and $\psi$ of form \ref{e4}.

If $\phi$ is of form \ref{e2}  and $\psi$ is of form \ref{e3}
then assumption 4 implies that
$i=j$ and $s_1=s_2$. Because of the induction
hypothesis we have $\psi_1 = \psi_2$ and
hence $\phi=\psi$ again by assumption 4.
\qed
\end{proof}

\subsection{Type Isomorphisms}
\label{chtyiso}

In several important cases there is
not only a coercion from a type 
$A$ into a type $B$ but also one
from $B$ into $A$.
So there are coercions from univariate polynomials
in sparse representation over some ring
to ones in dense representation and vice versa.
Or we have
$$\tf{FF}(t_\cf{integral\_domain}) \subtype
\tf{FF}(\tf{FF}(t_\cf{integral\_domain}))$$
and
$$\tf{FF}(\tf{FF}(t_\cf{integral\_domain}))
\subtype \tf{FF}(t_\cf{integral\_domain}).$$
Other examples can be found in Sec.~\ref{paramtyiso}.
If $A \subtype B$ and $B \subtype A$ then we will
write $A \typeiso B$.

If we require that for coercions
$$\begin{array}{l}
\phi: A \longrightarrow B, \\
\psi: B \longrightarrow A
\end{array}
$$
the compositions $\phi \circ \psi$ and $\psi \circ \phi$
are the  identities  on $A$ resp.\ $B$, then
the coherence theorem 6 can be extended
to the case of type isomorphisms.\footnote{Obviously,
the conditions that $\phi$ and $\psi$ are true inverses
of each other is also a necessary condition
for coherence.}

So type isomorphisms can be seen as equivalence classes in the
preorder on types induced by the coercions.  However, there are
several reasons to treat type isomorphisms by a new typing construct
independent from the concept of coercions.  As we have shown in
Sec.~\ref{chparamtycl} there is usually the second-order type of a
category present in {\sf Axiom} for a class of equivalent types.  On
the one hand if coercions are present in the system the equivalence
classes in the coercion preorder can be deduced by a system so that it
is not necessary to define them by the programmer.\footnote{In 
{\sf Axiom} the isomorphic types are treated independently of the
 coercions.}  On the other hand --- at least for the purpose of a
user interface --- it seems to be useful to have a class of isomorphic
types present as a first-order type.  Since all equivalence classes in
the coercion preorder are finite --- only finitely many (possibly
polymorphic) functions can be defined to be coercions --- the type of
finite disjoint unions --- variant record types --- can serve as a
well known first-order type for that purpose
(cf.\ \cite[p.~46]{That91}).

Moreover, it is reasonable to assume that type isomorphisms have the
following properties which cannot be deduced from the properties of
general coercion functions.
\begin{enumerate}
\item Isomorphic types belong to the same type class, i.\,e.\ if $t_1
  : \sigma$ and $t_1 \typeiso t_2$ then $t_2 : \sigma$.
\item If $f: (\sigma_1 \cdots \sigma_n)\sigma$ is an $n$-ary type
  constructor, $t_1:\sigma_1, \ldots, t_n:\sigma_n$, $t'_1:\sigma_1,
  \ldots, t'_n:\sigma_n$, such that
$$ t_i \typeiso t'_i \quad \forall i$$ then
$$f(t_1, \ldots, t_n ) \typeiso f(t'_1, \ldots, t'_n ).$$
\end{enumerate}

The second condition is only implied by the rules for structural
coercions if $f$ would be monotonic or antimonotonic in all arguments.
Because of the second condition a {\em congruence relation} is defined
by $\typeiso$ on the term-algebra of types.\footnote{It follows from
the properties of $\subtype$ alone that $\typeiso$ defines an
equivalence relation.}  Thus we can built the factor algebra modulo
this congruence relation.  This factor algebra is isomorphic to the
factor algebra modulo some equational theory, the equational theory
which is obtained if we interpret $\typeiso$ as equality.  We will
call this equational theory {\em the equational theory corresponding
to the type isomorphism.}

For simplicity we will often neglect the sort constraints but will
only write the unsorted part. Since for many examples in consideration
the sort is always the same, these slightly sloppy view can be
justified even formally.

While it is useful to know that certain {\em different types} are
isomorphic --- such as the sparse and dense representations of
polynomials --- there are other cases where it seems to be more
appropriate to have a semantics of the type system implying that
certain types are actually {\em equal}.

So the type system is not coherent if we define all naturally
occurring embedding functions to be coercions and if we regard two
types
$$\tf{direct\_sum}(t_1,t_2) \mbox{ and } \tf{direct\_sum}(t_2,t_1)$$
as being different.  This example would not violate the coherence of
the type system if we had not only two possible coercion functions
implying that these types are isomorphic but if these types are
actually {\em equal\/} in the system.  Notice that an implementation
of this type constructor having these properties is possible.  One
just has to use the same techniques as are used for the representation
of general associative and commutative operators in certain
term-rewriting systems (see e.\,g.\ \cite[Sec.~10]{Bund93},
\cite{Bund93a}), i.\,e.\ a certain ordering on terms has to be
given and the terms have to be represented in a {\em flattened form}.

In Sec.~\ref{scoercprbl} we will give a family of type isomorphisms
whose corresponding equational theory is not finitely axiomatizable.
Thus all of these isomorphisms cannot be modeled by declaring finitely
many functions to be coercions between types (even if we allow
``polymorphic'' coercion functions between polymorphic types).  So
these type isomorphisms could be only modeled in the system by a
direct mechanism implying that certain types are equal.

\subsubsection{Independence of the Coercion Preorder
from the Hierarchy of Type Classes}
\label{coerinstr}

If two types are isomorphic, then they belong to the same type class.

Such a conclusion is not justified if there is only a coercion form
$A$ into $B$.  Consider for instance a field $K$. Its elements can be
coerced to the constant polynomials in $K[x]$. Of course, the ring of
polynomials over some field is no longer a field.

However, it cannot be concluded in general that $A \subtype B$ and $A:
\sigma$ implies $B: \tau$ for some $\sigma \leq \tau$.  Just the
opposite holds for many important examples!

Consider e.\,g.\ the coercion from an integral domain into its field
of fractions which is not only an integral domain but even a field.
Similarly, any field can be embedded in its algebraic closure,
i.\,e.\ in a structure which has additional ``nice'' properties,
namely that it is an algebraically closed field.  The constructions of
the real numbers $\RR$ or of $p$-adic completions of $\QQ$ can be seen
similarly.  The field of rational numbers $\QQ$ can be embedded in
these structures --- and is usually identified with its image under
this embedding --- which are complete metric spaces, a property that
the original structure did not have.

The construction of structures which have additional ``nice''
properties and in which the original structure can be embedded is an
important tool for mathematical reasoning.\footnote{The author could
  easily list several examples of such constructions from the area of
  mathematics he has worked on.  Since this area is non-constructive
  we will omit them.  However, it seems to be possible to find some
  examples in almost {\em any} area of mathematics.}  Usually, the
original structures and their images under this embedding are not
distinguished notationally.

So the possibility to have coercions which induce a preorder on types
that is quite independent on the preorder on types induced by the
inheritance hierarchy on type classes seems to be important.  Notice
that these preorders would still differ even if we had allowed more
sophisticated inheritance possibilities on type classes than the ones
given in {\sf Axiom} or {\sf Haskell}.  There have to be (at least)
two hierarchies.  The one corresponding to some form of
``inheritance'': more special structures (such as a ``rings'') inherit
all properties of more general ones (such as ``groups''), and another
one reflecting possible embeddings of a structure into another that
might have stronger properties.

\begin{remark}
Of course, it is desirable to have some form of control over the
possibilities how coercions behave with respect to the hierarchy on
type classes.  This seems to be possible.

All of the examples given above can be described by an unary type
constructor $F$ such that for any types $A$ and $B$ of an appropriate
sort the following holds:
$$\begin{array}{l} \mbox{If }A \subtype B, \mbox{ then } F(A) \subtype
  F(B),\\ A \subtype F(A), \\ F(F(A)) \typeiso F(A) .
\end{array}$$
Thus --- if we interpret $\subtype$ as $\subseteq$ and $\typeiso$
as equality --- the type constructor $F$ has the properties of a 
{\em closure operator} (see e.\,g.\ \cite{Dave90},\cite{Laue82}).

So the requirement that a type unary constructor which has a direct
embedding and whose constructed type belongs to a type class with
stronger properties then the type parameter has to be a closure
operator in the sense of above would be fulfilled by many important
examples.  On the other hand such a restriction might allow much more
efficient type inference algorithms so that it might be a reasonable
requirement for a system.
\end{remark}

\subsubsection{Some Problematic Examples of Type Isomorphisms}

In this section we will collect some natural examples of type
isomorphisms which arise in the context of computer algebra.  We will
show that their corresponding equational theories are not unitary or
even not finitary unifying or that the unification problem is even
undecidable.

In Sec.~\ref{sectypinfcoer} we will show why these properties of the
corresponding equational theory are problematic in the context of type
inference.

We have already shown that a family of type isomorphisms whose
corresponding equational theory is not finitely axiomatizable cannot
be modeled by means of finitely many coercion functions and thus
requires another concept.  The presentation of a family of type
isomorphisms having this property will be given in the next section
because the proof of this property will need a little technical
machinery.

{\bf Example 1.}
\label{isomac}
As was mentioned above for the type constructor $\tf{direct\_sum}$ on
Abelian groups the type isomorphisms
$$\tf{direct\_sum}(t_1,t_2) \typeiso \tf{direct\_sum}(t_2,t_1),$$ and
$$\tf{direct\_sum}(t_1,\tf{direct\_sum}(t_2,t_3)) \typeiso
\tf{direct\_sum}(\tf{direct\_sum}(t_1,t_2),t_3)$$ hold.

Thus $\tf{direct\_sum}$ would give rise to an equational theory modulo
an associate and commutative operator.  The unification problem for
such an equational theory is decidable, but not unitary unifying.
However, it is finitary unifying (cf.\ \cite{Siek89},
\cite{Joua90}).

{\bf Example 2.}
\label{isomass}
For the binary type constructor $\tf{pair}$ which builds the type of
ordered pairs of elements of arbitrary types the following type
isomorphisms hold:
$$\tf{pair}(\tf{pair}(A,B),C) \typeiso \tf{pair}(A, \tf{pair}(B,C)),$$
i.\,e.\ it corresponds to an associative equational theory.
Unification for such theories is decidable but not finitary unifying
\cite{Siek89}.

{\bf Example 3.}
\label{isounde}
Let $A, B, C$ be vector spaces over some fixed field $K$ and let
$\oplus$ denote the direct sum of vector spaces and $\otimes$ denote
the tensor product of two vector spaces.  Then we have
$$ (A \oplus B) \otimes C \cong (A \otimes C) \oplus (B \otimes C)
$$ (see e.\,g.\ \cite[p.~293]{Kowa63}.)  Thus if we had two binary
type constructors over vector spaces building direct sums and tensor
products respectively, then the ``distributivity law'' gives rise to
type isomorphisms.  Since associativity and commutativity also hold
for the type constructor building direct sums of vector spaces alone
--- any vector space is an Abelian group --- we have the case of an
equational theory having two operators obeying associativity,
commutativity, and distributivity but no other equations.

Unfortunately, unification for such theories is undecidable
\cite{Siek89}, \cite{Szab82}.

\begin{figure}[t]
\begin{center}
\begin{tabular}{|l|r|}
\hline Type isomorphisms whose & Example given \\ corresponding
equational theory & on page \\ \hline \hline is not unitary unifying &
\pageref{isomac} \\ is not finitary unifying & \pageref{isomass}
\\ has an undecidable unification problem & \pageref{isounde} \\ is
not finitely axiomatizable &
\pageref{begincoerpr}--\pageref{endcoerpro}\\ \hline
\end{tabular}
\end{center}
\caption{Some problematic examples of type isomorphisms}
\end{figure}

\subsection{A Type Coercion Problem}
\label{scoercprbl}
\label{begincoerpr}

In this section we want to present an example of a family of types
which allow type-isomorphisms which correspond to an equational theory
that is not finitely axiomatizable. In order to set up the example we
first need a technical result.

\subsubsection{A  Technical Result}
\label{s3}

{\bf Definition 23.}
{\sl Let $f : \{P,F\}^* \longrightarrow \{P,F\}^*$ be the function, which
is defined by the following algorithm:
\begin{itemize}
\item[] If no $F$ is occurring in the input string, then return the
  input string as output string.

Otherwise, remove any $F$ except the leftmost occurrence from the
input string and return the result as output string.
\end{itemize}}

{\sl Let $\equiv$ be the binary relation on $\{P,F\}^*$ which is defined by}
$$\forall v,w \in \{P,F\}^*: \; v \equiv w \iff f(v)=f(w).$$

Obviously, the function $f$ can be computed in linear time and the
relation $\equiv$ is an equivalence relation on $\{P,F\}^*$.

Let $\Sigma$ be the first-order signature consisting of the two unary
function Symbols $F$ and $P$.  We will now lift the equivalence
relation $\equiv$ to a set of equations over $\Sigma$.

{\bf Definition 24.}
\label{deeqe}
{\sl Let ${\cal E}$ be the following set of equations:}
$$\begin{array}{lll} {\cal E} = \{ & S_1(S_2(\cdots S_k(x)\cdots) =
  S_{k+1}(S_{k+2}(\cdots S_r(x)\cdots )) \mid \\ & \;\; S_i \in
  \{F,P\} \:(1 \leq i \leq r) \mbox{ and } S_1 S_2 \cdots S_k \equiv
  S_{k+1} S_{k+2} \cdots S_r & \} \\
\end{array}$$

{\bf Theorem 7.}
\label{thmtr}
${\cal E}$ is not finitely based, i.\,e.\ there is no finite set of
axioms for ${\cal E}$.

\begin{proof} Assume towards a contradiction that there
is such a finite set ${\cal E}_0$.  Let ${\cal M}$ be the free model
of $\aleph_0$ generators over ${\cal E}$ and let ${\cal M}_0$ be the
free model of one generator over ${\cal E}_0$.

Except for a possible renaming of the variable symbol $x$, ${\cal
  E}_0$ has to be a subset of ${\cal E}$.  Otherwise, ${\cal E}_0$
would contain an equation of the form
$$S_1(S_2(\cdots S_k(x)\cdots) = S_{k+1}(S_{k+2}(\cdots S_r(y)\cdots
)), $$ or of the form
$$S_1(S_2(\cdots S_k(x)\cdots) = S_{k+1}(S_{k+2}(\cdots S_r(x)\cdots
)), \;\: S_1 S_2 \cdots S_k \not \equiv S_{k+1} S_{k+2} \cdots S_r.$$
However, none of these equations holds in ${\cal M}$.

Now let $n \in \NN$ be the maximal size of a term in ${\cal E}_0$.
Then the equation
$$F(\underbrace{P(P(\cdots (P}_{n}(x) ) \cdots ))) =
F(P(F(\underbrace{P(P(\cdots (P}_{n-1}(x) ) \cdots )))))$$ holds in
${\cal M}$, but it does not hold in ${\cal M}_0$.  \qed
\end{proof}

\subsubsection{The Problem}
\label{s4}

If $R$ is an integral domain, we can form the field of fractions
$\tf{FF}(R)$.  We can also built the ring of univariate polynomials in
the indeterminate $x$ which we will denote by $\tf{UP}(R,x)$ --- the
ring of polynomials $R[x]$ in the standard mathematical notation ---
which is again an integral domain by a Lemma of Gau{\ss}.  Thus we can
also built the field of fractions of $\tf{UP}(R,x)$,
$\tf{FF}(\tf{UP}(R,x))$ --- the field of rational functions $R(x)$.

Starting from an integral domain $R$ we will always get an integral
domain and can repeatedly built the field of fractions and the ring of
polynomials in a ``new'' indeterminate.


Thus if a computer algebra system has a fixed integral domain $\tf{R}$
and names for symbols $\tf{x}_0, \tf{x}_1, \tf{x}_2 \ldots$, it should
also provide types of the form
\begin{enumerate}
\item $\tf{R}$, \label{l11}
\item $\tf{FF}(\tf{R})$, \label{l12}
\item $\tf{UP}(\tf{R},\tf{x}_0)$, \label{l13}
\item $\tf{UP}(\tf{FF}(\tf{R}),\tf{x}_0)$, \label{l14}
\item $\tf{FF}(\tf{UP}(\tf{R},\tf{x}_0))$, \label{l15}
\item $\tf{UP}(\tf{UP}(\tf{R},\tf{x}_0),\tf{x}_1)$, \label{l16}
\item
  $\tf{UP}(\tf{FF}(\tf{UP}(\tf{R},\tf{x}_0)),\tf{x}_1)$, \label{l17}
\item
  $\tf{FF}(\tf{UP}(\tf{UP}(\tf{R},\tf{x}_0),\tf{x}_1))$, \label{l18}
\item
  $\tf{FF}(\tf{UP}(\tf{FF}(\tf{UP}(\tf{R},\tf{x}_0)),\tf{x}_1)$, \label{l19}
\item
  $\tf{UP}(\tf{UP}(\tf{UP}(\tf{R},\tf{x}_0),\tf{x}_1),\tf{x}_2)$, \label{l1
  10}
\item[] \rule{0mm}{0mm} \vdots
\end{enumerate}

It is convenient to use the same symbols for a mathematical object and
the symbolic expression which denotes the object.  In order to clarify
things we will sometimes use additional $\lsb \cdot \rsb$ for the
mathematical objects.

There are canonical embeddings from an integral domain into its field
of fractions and into the ring of polynomials in one indeterminate (an
element is mapped to the corresponding constant polynomial).

It is common mathematical practice to identify the integral domain
with its image under these embeddings.  Thus the type system should
also provide a coercion between these types, i.\,e.\ if $t$ is a type
variable of sort $\cf{integral\_ domains}$ and $x$ is of sort
$\cf{symbol}$, then
$$t \subtype \tf{FF}(t)$$ and
$$t \subtype \tf{UP}(t,x).$$

However, not all of the types built by the type constructors $\tf{FF}$
and $\tf{UP}$ should be regarded to be different.  If the integral
domain $R$ happens to be a field, then $R$ will be isomorphic to its
field of fractions.  Especially, for any integral domain $R$, $\lsb
\tf{FF}(R)\rsb$ and $\lsb \tf{FF}(\tf{FF}(R)) \rsb$ are isomorphic.

The fact that also $\lsb \tf{FF}(\tf{FF}(R)) \rsb$ can be embedded in
$\lsb \tf{FF}(R) \rsb$ can be expressed by
$$\tf{FF}(\tf{FF}(t)) \tf \subtype{FF}(t),$$ which is one of the
examples given in \cite[p.~354]{Como91}.

But there are more isomorphisms which govern the relations of this
family of types.

If we assume that an application of the type constructor $\tf{UP}$
always uses a ``new'' indeterminate as its second argument, any
application of the type constructor $\tf{FF}$ except the outermost one
application is redundant.

This observation will be captured by the following formal treatment.
In order to avoid the technical difficulty of introducing ``new''
indeterminates, we will use an unary type constructor $\tf{up}$
instead the binary $\tf{UP}$.  The intended meaning of $\tf{up}(t)$ is
$\tf{UP}(t,\tf{x}_n)$, where $\tf{x}_n$ is a new symbol, i.\,e.\ not
occurring in $t$.

{\bf Definition 25.}
{\sl Define a function {\sf trans} from $\{F,P\}^*$ into the set of types
recursively by the following equations.  For $w \in \{F,P\}^*$,
\begin{itemize}
\item ${\sf trans}(\varepsilon) = \tf{R}$,
\item ${\sf trans}(Fw)=\tf{FF}({\sf trans}(w))$,
\item ${\sf trans}(Pw)=\tf{up}({\sf trans}(w))$.
\end{itemize}}

If we take $\lsb \tf{R} \rsb$ to be the ring of integers, the
following lemma will be an exercise in elementary
calculus.\footnote{If we started with the ring of polynomials in
  infinitely many indeterminates over some domain, then there would be
  additional isomorphisms.}

{\bf Lemma 7.}
\label{letrans}
{\sl Let $\lsb \tf{R} \rsb$ be the ring of integers.  For any $v, w \in
\{F,P\}^*$, the integral domains $\lsb {\sf trans}(v) \rsb$ and $\lsb
{\sf trans}(w)\rsb$ are isomorphic iff $v \equiv w$.}

{\sl Moreover, $\lsb {\sf trans}(v)\rsb$ can be embedded in $\lsb {\sf
  trans}(w)\rsb$ and $\lsb {\sf trans}(w)\rsb$ can be embedded in
$\lsb{\sf trans}(v)\rsb$ iff $\lsb{\sf trans}(v)\rsb$ and $\lsb{\sf
  trans}(w)\rsb$ are isomorphic.}

{\bf Theorem 8.}
{\sl Let $\Sigma$ be the signature consisting of the unary function symbols
$\tf{FF}$ and $\tf{up}$ and the constant $\tf{R}$.  Let $\lsb \tf{R}
\rsb$ be the ring of integers.}

{\sl Then there is no finite set of Equations ${\cal E}' $ over $\Sigma$,
such that for ground terms $t_1$ and $t_2$ the following holds.}
$$ {\cal E}' \models \{ t_1 = t_2 \} \iff \mbox{$\lsb t_1\rsb$ and
$\lsb t_2\rsb$ are isomorphic.}$$

\begin{proof}
If $t_1$ and $t_2$ are ground terms, then there are $v, w \in \{F,
P\}^*$ such that $t_1={\sf trans}(v)$ and $t_2={\sf trans}(w)$.  Now
we are done by Lemma 7 and Theorem 7.  \qed
\end{proof}

The problem is that the equational theory which describes the coercion
relations in the example we gave is not finitely based.  Since this
property of an equational theory is {\em equivalence-invariant} in the
sense of \cite[p.~382]{Grae79}, the use of another signature for
describing the types does not help.
\label{endcoerpro}

\subsection{Properties of the Coercion Preorder}
\label{secpropcoerpreord}

If the type system is coherent, then the category of ground types as
objects and the coercions as arrows is a preorder.  Even if the type
system is not coherent, a reflexive and transitive relation on the
ground types (and even on the polymorphic types) is defined by
``$\subtype$'', i.\,e.\ a preorder.\footnote{Notice the difference
between {\em a category which is a preorder} and {\em a relation
which is a preorder}.}

Factoring out the equivalence classes of this reflexive and transitive
relation we will obtain a partial order on the types.

In general this order on the types will not be a lattice if we
consider some typical examples occurring in a computer algebra system.
Take e.\,g.\ the types $\tf{integer}$ and $\tf{boolean}$. There is no
type which can be coerced to both of these types (unless an additional
``empty type'' is present in the system).

For many purposes, especially type inference (see
Sec.~\ref{secaltyincoer}), it would be convenient if this partial
ordering on the types were a quasi-lattice.  In the following we will
show that in general this will not be the case.

{\bf Example 4.}
\label{exnolatds}
Let $\tf{I}$ be the ring of integers and let $\oplus$ denote the
direct sum of two Abelian groups and let the direct embeddings into
the first argument and into the second argument of this type
constructor be present, i.\,e.\ ${\cal D}_{\oplus} = \{ 1, 2 \}$.
Then we have
$$\begin{array}{l} \tf{UP}(\tf{I},\tf{x}) \subtype
  \tf{UP}(\tf{FF}(\tf{I}),\tf{x}), \\ \tf{UP}(\tf{I},\tf{x}) \subtype
  \tf{UP}(\tf{I},\tf{x}) \oplus \tf{FF}(\tf{I}),\\ \tf{FF}(\tf{I})
  \subtype \tf{UP}(\tf{FF}(\tf{I}),\tf{x}), \\ \tf{FF}(\tf{I})
  \subtype \tf{UP}(\tf{I},\tf{x}) \oplus \tf{FF}(\tf{I}),
\end{array}$$
and no other coercions can be defined between these types.  There is
also no type $R$ with $R \neq \tf{UP}(\tf{I},\tf{x})$ and $R \neq
\tf{FF}(\tf{I})$ such that
$$\begin{array}{l} R \subtype \tf{UP}(\tf{FF}(\tf{I}),\tf{x}), \\ R
  \subtype \tf{UP}(\tf{I},\tf{x}) \oplus \tf{FF}(\tf{I})
\end{array}
$$ (cf.\ Fig.~\ref{fignolatmde}).  Thus in this case the partial
ordering given by $\subtype$ is not a quasi-lattice (see also
Lemma 3).

\begin{figure}
\begin{center}
\unitlength=1.2mm
\begin{picture}(45.00,50.00)
\put(13.00,10.00){\makebox(0,0)[cc]{$\tf{UP}(\tf{I},\tf{x})$}}
\put(37.00,10.00){\makebox(0,0)[cc]{$\tf{FF}(\tf{I})$}}
\put(13.00,40.00){\makebox(0,0)[cc]{$\tf{UP}(\tf{FF}(\tf{I}),\tf{x})$}}
\put(37.00,40.00){\makebox(0,0)[cc]{$\tf{UP}(\tf{I},\tf{x}) \oplus
    \tf{FF}(\tf{I})$}} \put(13.00,13.00){\vector(0,1){24.00}}
\put(13.00,13.00){\vector(1,1){24.00}}
\put(37.00,13.00){\vector(0,1){24.00}}
\put(37.00,13.00){\vector(-1,1){24.00}}
\put(9.00,25.00){\makebox(0,0)[cc]{$\subtype$}}
\put(41.00,25.00){\makebox(0,0)[cc]{$\subtype$}}
\put(22.00,20.00){\makebox(0,0)[cc]{$\subtype$}}
\put(28.00,20.00){\makebox(0,0)[cc]{$\subtype$}}
\end{picture}
\end{center}

\caption{Ad Example 4}
\label{fignolatmde}
\end{figure}

Even if we require $|{\cal D}_f| \leq 1$ for all type constructors ---
recall that this requirement is also necessary in order to ensure a
coherent type system --- and we have only direct embeddings and
structural coercions then it is still possible that the partial
ordering on types induced by ''$\subtype$'' is not a quasi-lattice.
Consider for instance two type constructors $f:(\sigma)\sigma$ and $g:
(\sigma)\sigma$ which we assume to be unary for simplicity.  If ${\cal
  D}_f \cap {\cal M}_f \neq \emptyset$ and ${\cal D}_g \cap {\cal M}_g
\neq \emptyset$ and $t: \sigma$, then
$$g(t) \subtype f(g(t)) \quad\quad\mbox{and}\quad\quad g(t) \subtype
g(f(t))$$ and similarly
$$f(t) \subtype g(f(t)) \quad\quad\mbox{and}\quad\quad f(t) \subtype
f(g(t))$$ (cf.\ Fig.~\ref{fignolatoth}).  Having only direct
embeddings and structural coercions the condition imposed in
Lemma 3 with $a = g(t)$, $b=f(t)$, $c=f(g(t))$ and
$d=g(f(t))$ are fulfilled.

\begin{figure}
\begin{center}
\unitlength=1.2mm
\begin{picture}(45.00,50.00)
\put(13.00,10.00){\makebox(0,0)[cc]{$g(t)$}}
\put(37.00,10.00){\makebox(0,0)[cc]{$f(t)$}}
\put(13.00,40.00){\makebox(0,0)[cc]{$g(f(t))$}}
\put(37.00,40.00){\makebox(0,0)[cc]{$f(g(t))$}}
\put(13.00,13.00){\vector(0,1){24.00}}
\put(13.00,13.00){\vector(1,1){24.00}}
\put(37.00,13.00){\vector(0,1){24.00}}
\put(37.00,13.00){\vector(-1,1){24.00}}
\put(9.00,25.00){\makebox(0,0)[cc]{$\subtype$}}
\put(41.00,25.00){\makebox(0,0)[cc]{$\subtype$}}
\put(22.00,20.00){\makebox(0,0)[cc]{$\subtype$}}
\put(28.00,20.00){\makebox(0,0)[cc]{$\subtype$}}
\end{picture}
\end{center}

\caption{Another counter-example for the coercion order}
\label{fignolatoth}
\end{figure}

The type constructors $\tf{FF}$ and $\tf{up}$ have such properties.
However, we can define
$$\tf{up}(\tf{FF}(R)) \subtype \tf{FF}(\tf{up}(R))$$ for any integral
domain $R$ using a coercion which is not a direct embedding nor a
structural coercion.

So in this case some ``ad hoc knowledge'' can be used to avoid that
the partial ordering induced by $\subtype$ is not a quasi-lattice.

In general, it does not seem to be justified to assume that the
partial ordering induced by $\subtype$ is a quasi-lattice.

\subsection{Combining Type Classes and Coercions}
\label{seccomtcco}

Let
$${\rm op} : \overbrace{v_\sigma \times \cdots \times v_\sigma}^{n}
\longrightarrow v_\sigma$$ be an $n$-ary operator defined on a type
class $\sigma$ and let $A \subtype B$ be types belonging to $\sigma$
and let
$$ \phi: A \longrightarrow B$$ be the coercion function.  Moreover,
let ${\rm op}_A$ and ${\rm op}_B$ be the instances of ${\rm op}$ in
$A$ resp.\ $B$.

For $a_1, \ldots, a_n \in A$ the expression
$${\rm op}(a _1, \ldots, a_n)$$ might denote different objects in $B$,
namely
$${\rm op}_B(\phi(a_1), \ldots, \phi(a_n))$$ or
$$\phi({\rm op}_A(a_1, \ldots, a_n)).$$

The requirement of a unique meaning of
$${\rm op}(a _1, \ldots, a_n)$$ just means that $\phi$ has to be a
{\em homomorphism} for $\sigma$ with respect to ${\rm op}$.

The typing of ${\rm op}$ in the example above is only one of several
possibilities.  In general if $\sigma$ is a type class having
$p_{\tau_1}, \ldots, p_{\tau_k}$ as parameters ---
i.\,e.\ $p_{\tau_i}$ is a type variable of sort $\tau_i$ --- then a
$n$-ary first-order operation ${\rm op}$ defined in $\sigma$ can have
the following types.\footnote{For simplicity, we will exclude in the
  following discussion arbitrary polymorphic types different from type
  variables.  Especially, we will not regard higher-order functions,
  which do not play a central role in computer algebra although they
  are useful, cf.\ Sec.~\ref{posappcom}.  For the other relevant cases
  of polymorphic types the following can be generalized easily.}
$${\rm op}: \xi_1 \times \cdots \times \xi_n \longrightarrow
\xi_{n+1},$$ where $\xi_i$, $1 \leq i \leq n+1$, is either $v_\sigma$,
or $p_{\tau_l}$, $l \leq k$, or a ground type $t_m$.

As on page~\pageref{defcatsigmai} let $\cat{C}_{\sigma}$ be the
category of ground types of sort $\sigma$ as objects and the coercions
as arrows.  For a ground type $t$ let $\cat{C}_t$ be the subcategory
which has $t$ as single object and has thus the identity on $t$ as
single arrow.\footnote{If the type system is not coherent this
  subcategory might have more than one arrow.}  Now let
$$ \cat{C}_i=\left\{
\begin{array}{ll}
\cat{C}_{\sigma}, & \mbox{if }\xi_i = v_\sigma,\\ \cat{C}_{\tau_l}, &
\mbox{if }\xi_i = p_{\tau_l},\\ \cat{C}_{t_m}, & \mbox{if }\xi_i = t_m
\mbox{ for a ground type $t_m$}.
\end{array}
\right.
$$

Let $\rtypeasop$ be a functor from $\cat{C}_1 \times \cdots \times
\cat{C}_n$ into $\cat{C}_{n+1}$.  If $(\zeta_1, \ldots, \zeta_n)$ is
an object of $\cat{C}_1 \times \cdots \times \cat{C}_n$, i.\,e.\
$$ \zeta_i=\left\{
\begin{array}{ll}
A_{\sigma}, & \mbox{if }\xi_i = v_\sigma \mbox{ and $A_\sigma$ is a
  ground type belonging to $\sigma$},\\ A_{\tau_l}, & \mbox{if }\xi_i
= p_{\tau_l}\mbox{ and $A_{\tau_l}$ is a ground type belonging to
  $\tau_l$},\\ t_m, & \mbox{if }\xi_i = t_m,
\end{array}
\right.
$$ then $\rtypeasop(\zeta_1, \ldots, \zeta_n)$ is an object of
$\cat{C}_{n+1}$, i.\,e.\ a ground type belonging to $\sigma$
resp.\ $\tau_{l'}$, or is a ground type $t_{m'}$ depending on the
value of $\xi_{n+1}$.

Informally $\rtypeasop$ can be used to specify the type of the range
of an instantiation of ${\rm op}$ if instantiations of $\sigma$ and
the parameters of $\sigma$ are given.  We need a functor $\rtypeasop$
because of the following reason.  Given two instantiations of the type
class which can be described by $(\zeta_1, \ldots, \zeta_n)$ and
$(\zeta'_1, \ldots, \zeta'_n)$ such that
$$\zeta_i \subtype \zeta'_i \quad \forall i \leq n$$ it is necessary
that
$$\rtypeasop(\zeta_1, \ldots, \zeta_n) \subtype \rtypeasop(\zeta'_1,
\ldots, \zeta'_n).$$ Otherwise, if $a_i$ is an object of type
$\zeta_i$, $1 \leq i \leq n$, the expression
$${\rm op}(a_1, \ldots, a_n)$$ has the types $\rtypeasop(\zeta_1,
\ldots, \zeta_n)$ and $\rtypeasop(\zeta'_1, \ldots, \zeta'_n)$ for
which a coercion has to be defined in order to give the expression a
unique meaning.

If $\sigma$ is a non-parameterized type class {\em any} mapping
assigning an appropriate type to a tuple $(\zeta_1, \ldots, \zeta_n)$
can be extended to a functor.  So the requirement that $\rtypeasop$ is
a functor is only a restriction for parameterized type classes.

Since in a coherent type system there are unique coercions between
types, we will omit the names of the coercions in the following and we
will write
$$\rtypeasop(\zeta_1 \subtype \zeta'_1, \ldots, \zeta_n \subtype
\zeta'_n)$$ for the image of the single arrow between the objects
 $$(\zeta_1, \ldots, \zeta_n) \mbox{ and } (\zeta'_1, \ldots,
\zeta'_n)$$ in the category
$$\cat{C}_1 \times \cdots \times \cat{C}_n$$ under the functor
$\rtypeasop$.  Thus $\rtypeasop(\zeta_1 \subtype \zeta'_1, \ldots,
\zeta_n \subtype \zeta'_n)$ is an arrow in $\cat{C}_{n+1}$.

Let $\catofsets$ be the category of all set as objects and functions
as arrows.\footnote{Notice that the category theoretic object
  $\catofsets$ is quite different from the {\sf Axiom} category {\tt
    SetCategory}.}

By the assumption of set theoretic ground types and coercion functions
we can assign to any object of $\cat{C}_\sigma$ an object of
$\catofsets$ and to any arrow in $\cat{C}_\sigma$ an arrow of
$\catofsets$ in a functorial way.  We will write
$\typesetinterpr{\cat{C}_\sigma}$ for the functor defined by this
mapping.

We will use the notation $\zeta_i \subtype \zeta'_i$ to denote the
single arrow between $\zeta_i$ and $\zeta'_i$ in $\cat{C}_i$.  Thus
$$ \typesetinterpr{\cat{C}_1} \times \cdots \times
\typesetinterpr{\cat{C}_n}(\zeta_1 \subtype \zeta'_1, \ldots, \zeta_n
\subtype \zeta'_n)
$$ is an arrow in the category
$$\underbrace{\catofsets \times \cdots \times \catofsets}_{n}.$$ Since
$n$-tuples of sets are sets there is a functor from $\catofsets ^n$
into $\catofsets$ which we will denote by $\flatsetn$.

If $(\zeta_1, \ldots, \zeta_n)$ is an object in $\cat{C}_1 \times
\cdots \times \cat{C}_n$ we are now ready to formalize a requirement
on the instantiation of ${\rm op}$ given by $(\zeta_1, \ldots,
\zeta_n)$.  We will not impose this condition directly on ${\rm
  op}_{(\zeta_1, \ldots, \zeta_n)}$.  It will be convenient to regard
the set-theoretic interpretation
$$\typesetinterpr{\cat{C}_1} \times \cdots \times
\typesetinterpr{\cat{C}_n}(\zeta_1, \ldots, \zeta_n)$$ of $(\zeta_1,
\ldots, \zeta_n)$ instead this $n$-tuple of types itself.  Then the
set-theoretic interpretation of ${\rm op}_{(\zeta_1, \ldots,
  \zeta_n)}$ induces a function between
$$\flatsetn(\typesetinterpr{\cat{C}_1} \times \cdots \times
\typesetinterpr{\cat{C}_n}(\zeta_1, \ldots, \zeta_n))$$ and
$$\typesetinterpr{\cat{C}_{n+1}}(\rtypeasop(\zeta_1, \ldots,
\zeta_n)),$$ which we will denote by $\opinterprset{{\rm op}}(\zeta_1,
\ldots, \zeta_n)$.

Given $(\zeta_1, \ldots, \zeta_n)$ and $(\zeta'_1, \ldots, \zeta'_n)$
such that
$$\zeta_i \subtype \zeta'_i \quad \forall i \leq n$$ we just need that
the following diagram is commutative.

\begin{center}
\resetparms \setsqparms[1`1`1`1;2000`700]
\square[\flatsetn(\typesetinterpr{\cat{C}_1} \times \cdots \times
  \typesetinterpr{\cat{C}_n}(\zeta_1, \ldots, \zeta_n))`
  \typesetinterpr{\cat{C}_{n+1}}(\rtypeasop(\zeta_1, \ldots,
  \zeta_n))` \flatsetn(\typesetinterpr{\cat{C}_1} \times \cdots \times
  \typesetinterpr{\cat{C}_n}(\zeta'_1, \ldots, \zeta'_n))`
  \typesetinterpr{\cat{C}_{n+1}}(\rtypeasop(\zeta'_1, \ldots,
  \zeta'_n)); \opinterprset{{\rm op}}(\zeta_1, \ldots, \zeta_n)` {\rm
    L}` {\rm R}` \opinterprset{{\rm op}}(\zeta'_1, \ldots, \zeta'_n)]
\end{center}

In the diagram above we have set
$${\rm L} =\flatsetn( \typesetinterpr{\cat{C}_1} \times \cdots \times
\typesetinterpr{\cat{C}_n}(\zeta_1 \subtype \zeta'_1, \ldots, \zeta_n
\subtype \zeta'_n))$$ and
$${\rm R} = \typesetinterpr{\cat{C}_{n+1}}(\rtypeasop (\zeta_1
\subtype \zeta'_1, \ldots, \zeta_n \subtype \zeta'_n)).
$$

This requirement on $\opinterprset{{\rm op}}$ can be read that
$\opinterprset{{\rm op}}$ is a {\em natural transformation} between
the functor
$$\flatsetn \circ (\typesetinterpr{\cat{C}_1} \times \cdots \times
\typesetinterpr{\cat{C}_n})$$ and the functor
$$\typesetinterpr{\cat{C}_{n+1}} \circ \rtypeasop .$$

Thus for a $n$-ary first-order operator ${\rm op}$ the requirements
that
\begin{enumerate}
\item the assignments of a range type for an operation given
  instantiations of a type class and its parameters has to be
  ``functorial'' and
\item the instantiation of the operator has to correspond to a natural
  transformation between functors giving the set-theoretic
  interpretations of the ground types and the coercions between them
\end{enumerate}
will guarantee that type classes and coercions interact nicely,
i.\,e.\ give expressions involving ${\rm op}$ a unique meaning.


A brief inspection of the examples of parameterized type classes
occurring in {\sf Axiom} by the author has suggested that there is no
example violating the first requirement which will always hold in
non-parameterized type classes.  Nevertheless, a formal requirement
for a computer algebra language seems to be useful to ensure that no
such violating will occur in future extensions.

The second requirement is formulated as one on the possible
instantiations of operators.  However, it can also be read that given
the instantiations only certain coercions between base types are
allowed, namely only coercions for which the interpretation is a
natural transformation.  We will show below that using this view we
can conclude that only ``injective'' coercion functions are allowed
between most types.\footnote{In the following we will precisely state
  what we mean by ``injective'' and ``most types.''}


\begin{remark}
Our conditions imposed on the combination of type classes and
coercions are an adaptation of the work of Reynolds \cite{Reyn80}
on {\em category-sorted algebras}.  The difference is that Reynolds
allows each operator to be generic, i.\,e.\ that it may be
instantiated with any type in any position.  We allow type-class
polymorphism at some position and do not allow polymorphism at all in
other positions which seems to be the natural way to describe many
important examples.
\end{remark}

\subsubsection{Injective Coercions} 

An important type class is the class of types on which a test for
equality of objects can be performed in the system.\footnote{It is
called {\tt Eq} in {\sf Haskell} and {\tt SetCategory} in 
{\sf Axiom}.}  In this type class the operator symbol
$$= \: : t_{\cf{Eq}} \times t_{\cf{Eq}} \longrightarrow \tf{Boolean}$$
is used to denote the system test for equality.  In order to
distinguish between the ``system equality'' and ``true equality'' we
will use
$${\tt isequal} : t_{\cf{Eq}} \times t_{\cf{Eq}} \longrightarrow
\tf{Boolean}$$ for the system equality in the following.

Then the boolean values of
$${\tt isequal}(a_1, a_2)$$ and
$${\tt isequal}(\phi(a_1),\phi(a_2))$$ have to be the same.
Especially, if the latter evaluates to {\tt true} then the former also
has to evaluate to {\tt true}.  In analogy to the definition of
injective this means that $\phi$ has to be an injective function
``modulo system equality'' (usually, the definition of injective
involves true equality).

Thus coercions between types belonging to the ``equality type class''
have to be ``injective.''

The system equality for a type might very well differ from the
equality defined on a certain data type representing it.  So very
often the rational numbers are just represented as pairs of integers.
Then different pairs of integers can represent the same rational
number, thus the system test for equality of rational numbers is
different from the equality on pairs of integers.

Of course, a non-injective coercion function would not violate our
requirements, if $A$ and $B$ do not use the same operator symbol as a
test for equality.  Thus defining two different type classes
$\cf{Eq1}$ and $\cf{Eq2}$ with operators ${\tt isequal1}$ resp.\ ${\tt
  isequal2}$ as tests for equality and having $A$ of type class
$\cf{Eq1}$ and $B$ of type class $\cf{Eq2}$ would allow to define a
non-injective function to be a coercion between $A$ and $B$.  Defining
such different type classes is also a clear indication for the user
that there are problems.  Exposing a problem seems to be preferable
than hiding it and and hoping that it will not occur.  Although
usually for two elements $a_1$ and $a_2$ of type $A$ the test for
equality in $A$ will be used and not the one in $B$ it might happen
that one of the elements is coerced to $B$. Probably, this will not
happen very frequently which makes the situation even more dangerous,
since the system will wrongly say that two elements are equal only in
situations which are rather complicated so that the behavior of the
system might not be clear for the user.\footnote{For instance, the
  situation described above arises when coercions between (arbitrary
  precision) integers and floating point numbers are defined and the
  same symbol is used as a test for equality. Then two integers $a$
  and $b$ which are not equal might be equal if they are coerced to
  floating point numbers.  Such a coercion is used in many system if
  an expression like ``$a+0.0$'' occurs and can thus happen in
  situations which are quite surprising for the user.}

So the requirement of ``injective'' coercions seems to be absolutely
necessary for a computer algebra system although it is not required by
a system like {\sf Axiom}!\footnote{Since it is an undecidable problem
  to check whether a given recursive function is injective --- which
  can be easily proved by applying Rice's Theorem --- it is not
  possible to enforce by a compiler that coercions are injective if
  functions defined by arbitrary code can be declared to be coercions.
  Nevertheless, it seems to be useful to state this requirement as a
  guideline for a programmer.}

\subsection{Type Inference}
\label{sectypinfcoer}

In Sec.~\ref{sectytycl} we have seen that the type inference problem
for a language having type classes is decidable even if we have a
language with higher-order functions and one allowing parametric
polymorphism.  Moreover, there is a finite set of types for any object
of the language such that any type of the object is a substitution
instance of one of those types.

The type inference problem for a language with coercions is much more
complicated.  So there are objects which have infinitely many types
which are not substitution instances of finitely many (polymorphic)
types.\footnote{Using the results of Sec.~\ref{secaltyincoer} it will
  be possible to assign finitely many types to an object in the
  subsystem described in that section which have ``minimal
  properties'' among all types of the object.}  Consider a type
$\tf{R}$ belonging to a type class $\cf{commutative\_ring}$ and let
$r$ be be an object of type $\tf{R}$.  Given coercions
$$v_{\cf{commutative\_ring}} \subtype
\tf{up}(v_{\cf{commutative\_ring}})$$ then $r$ also has the types
$$\tf{up}(\tf{R}), \tf{up}(\tf{up}(\tf{R})), \ldots$$

In \cite{Mitc91}, \cite{Fuhx89}, \cite{Fuhx90} type systems for
functional languages allowing coercions between base types and
structural coercions are given and type inference algorithms for them.
These systems do not allow type class polymorphism nor parametric
polymorphism.  In \cite{Brea89}, \cite{Brea91} a system having
coercions and parametric polymorphism is given; however, no type
inference for the system is provided.

In \cite{That91} a type inference system for the case of type
isomorphisms induced by coercions is given which allows parametric
polymorphism.  However, as is argued in \cite{That91} if the
equational theory corresponding to the type isomorphisms is not
unitary unifying then the semantics of an expression involving {\tt
  let} may be ambiguous.  Moreover, the type inference problem is
reduced to an unification problem over the equational theory
corresponding to the type isomorphisms.  So in the case of an
undecidable equational unification problem
(cf.\ Example 3) only a semi-decision method is available
for type inference.

Type inference algorithms for a system allowing parametric
polymorphism and records resp.\ variants are given in \cite{Wand87},
\cite{Wand88}, \cite{Wand89}, \cite{Wand91}, \cite{Stan88},
\cite{Leis87}, \cite{Remy89}.  Since variants can be used to model
classes of isomorphic types some of these results can be applied if we
model classes of isomorphic types as variants.

Kaes \cite{Kaes92} gives a system allowing type-class polymorphism
(also parametric type classes can be described) which can handle
coercions between base types and structural coercions according to our
definition.\footnote{In the systems in \cite{Mitc91}, \cite{Fuhx90},
\cite{Fuhx89} all type constructors have to be monotonic or
antimonotonic in all arguments.}  However, direct embeddings are not
allowed.

In \cite{Como91} a type inference system and a semi-decision procedure
for it are described.  However, in that system some assumptions on the
properties on coercions are imposed which are not justified for many
examples occurring in computer algebra.\footnote{The problematic
assumptions are that all type constructors have to be monotonic in
all arguments and that any polymorphic type can be coerced to its
substitution instances.}  In \cite{Como91} a proof is given that the
type inference problem for the described system becomes undecidable if
no restrictions on the coercions are imposed.

Since there are infinitely many ground types in a system usually
infinitely many coercions will be necessary.  However, with the
exception of the example stated in Sec.~\ref{scoercprbl} all examples
of coercions we have given --- such as the direct embeddings and the
structural coercions --- can be described by a finite set of Horn
clauses which will usually have variables.  The formalism of Horn
clauses is strong enough to capture type classes and even parametric
type classes and also polymorphic types can be described.  Then the
typability of an object can be stated as the question whether a
certain clause is the logical consequence of the given set of Horn
clauses.  Thus using a complete Horn clause theorem
prover\footnote{Notice that {\sf PROLOG} is not one because of the
used depth-first search strategy.}  we have a semi-decision
procedure for type inference.  The size of the search space seems to
be a problem for the practical use of this method, but not the fact
that it is only a semi-decision procedure.  If an expression cannot be
typed using certain resources --- i.\,e.\ a typing of the expression
involves too many coercions if it is typeable at all --- it does not
seem to be a practical limitation if a system rejects the expression
as possibly untypeable and asks the user to provide more typing
information if the user thinks that the expression is typeable.

It is not clear which classes of coercions in connection with which
other typing constructs are allowed such that the type inference
problem is decidable.  Coercions between polymorphic types are
certainly a problem. In the following we will shortly discuss to what
extent some restrictions are justified for a computer algebra system.

If type inference has to be performed for user defined functions, then
polymorphic types arise naturally (cf.\ Sec.~\ref{secaxhasex}).  Since
the possibility to type user defined functions is useful for a
computer algebra system but does not play the same central role as for
a functional programming language it might be reasonable to exclude
them from type inference if coercions are present in order to
facilitate the problem.

But there are also other objects than functions that can be
polymorphic. Especially there are naturally occurring examples of 
{\em polymorphic constants}.

In {\sf Haskell} integer constants are polymorphic constants.  If $n$
is a constant denoting an integer then it also denotes the
corresponding objects of the types in the type class {\tt Num}.
Having a language allowing coercions the use of polymorphic constants
can be avoided for the examples used in {\sf Haskell}, because
coercions can be defined between the types belonging to {\tt Num} in
{\sf Haskell}.\footnote{In {\sf Haskell} only explicit conversions but
no implicit coercions are allowed.}

In a computer algebra system there are more types present which have
objects usually denoted by integer constants.  A nice example showing
the use of polymorphic constants in mathematical notation is given by
Rector \cite[p.~304]{Rect89}:
\begin{quote}
Consider
$$\frac{(x+y)^{1+n}+1}{1+nx}$$
where the user wants to work with rational functions
over a finite field of $p$-elements.
This formula  presents the problem of polymorphic constants.
To a mathematician, the types of  each subexpression are immediately clear:
$n$ is an integer variable which must be reduced modulo $p$
in the denominator of the expression,  $x$ and $y$ are finite field
variables, $1$  appearing in the exponent is an integer and
the other $1$'s are the multiplicative identity of the finite field.''
\end{quote}
Since there are no embeddings from $\ZZ$ into 
$\ZZ_m$ nor from $\ZZ_m$ into $\ZZ$ ---
for $n \neq m$ there is not even one from
the ring $\ZZ_m$ into the ring $\ZZ_n$\footnote{If $n =km$
then there is an embedding  of the {\em Abelian group}
$\langle \ZZ_m,+ \rangle$ into the Abelian group
$\langle \ZZ_n,+ \rangle$, namely the one given by the mapping
$i \mapsto ki$. Notice that a declaration of this embedding to
be a coercion between the corresponding types
and to have the elements of $\ZZ$ as polymorphic constants
(in their usual interpretation) in $\langle \ZZ_m,+ \rangle$
and in $\langle \ZZ_n,+ \rangle$ would contradict
the requirements stated in Sec.~\ref{seccomtcco}.} 
the use of polymorphic constants cannot
be avoided by introducing coercions.

\subsubsection{Algorithms for Type Inference}
\label{secaltyincoer}

In the following section we will restrict the types
to the ones which can be expressed as terms
of a finite order-sorted signature.
As we have seen in Sec.~\ref{secproossty} 
we can also assume that the signature is regular.

Let ${\rm op}$ be a $n$-ary operation,
$${\rm op}: \xi_1 \times \cdots \times \xi_n \longrightarrow
           \xi_{n+1},$$
where $\xi_i$, $1 \leq i \leq n+1$,
is either  a type variable $v_{\tau_l}$, $l \leq k$,
or a ground type $\overline{t}_i$.
Given objects
$o_1, \ldots , o_n$ having types
$t_1, \ldots, t_n$ respectively,
the expression
$${\rm op}(o_1, \ldots, o_n)$$
will be well typed having type $\xi_{n+1}$
iff the following conditions are satisfied.
\begin{enumerate}
\item If $\xi=\overline{t}_i$ for some ground type
$\overline{t}_i$ then $t_i \subtype \overline{t}_i$.
\item If $\xi_i = \xi_j = v_{\tau_l}$ for some
$i \neq j$ then there is a type $t : \tau_l$
such that
$t_i \subtype t$ and $t_j \subtype t$.
\item If $\xi_i = v_{\tau_k}$ then there is a type
$t:\tau_k$ such that $t_i \subtype t$.
\end{enumerate}

Notice that if we require that all
objects have ground types then algorithms solving the problems
imposed by the above conditions can be used
to solve the type inference problem using
a bottom-up process.\footnote{Similar ideas can be found
in \cite[Sec.~4]{Como91} and in \cite{Rect89}.}

If we do not restrict the possible coercions
then determining whether for given types
$t_1$ and $t_2$ there is a type $t$ such that
$t_1 \subtype t$ and $t_2 \subtype t$ might be an
undecidable problem (cf.\ \cite{Como91}). 

In the following we will restrict the possible coercions
to coercions between base types,\footnote{By the assumption of
a finite signature there are only finitely many base types and we will
assume that the finitely many
coercions between base types are effectively given.}
  direct embeddings and structural coercions.
In Sec.~\ref{seccoh} we have defined
the coercions only between ground types, because we
have given semantic considerations on coercions 
and it is not clear how to define a semantics
for arbitrary polymorphic types.
The algorithmic problems we are dealing with
in this section can be seen as algorithmic problems
on certain terms of an order-sorted signature where
an additional relation ``$\subtype$'' is given.
It will be convenient to define 
$\subtype$ also for polymorphic types, i.\,e.\ non-ground terms.
It is clear how  the definitions given in
Sec.~\ref{seccoh} for direct embeddings and
structural coercions can be extended to
polymorphic types. 

We will assume that for any type constructor $f$
the set of direct embedding positions ${\cal D}_f$
and the sets ${\cal A}_f$ and ${\cal M}_f$
are well defined, i.\,e.\ independent
of the arguments of $f$.
Moreover, we will assume that
for any types $t_1 \subtype t_2$ and any (sort-correct)
substitution $\theta$ we also have
$\theta(t_1) \subtype \theta(t_2)$.
These assumptions
are satisfied by all examples we gave and
are natural for the formalism of describing
types we use.

The advantage of extending  
the notions of direct embeddings and
structural coercions to polymorphic types is
that there are {\em finitely} many (polymorphic) types
$$t_1^1 \subtype t_1^2 , \ldots, t_r^1 \subtype t_r^2$$
such that for any types
$t_1 \subtype t_2$ there is
a (sort-correct) substitution $\theta$ and
an $1 \leq i \leq r$ such that
$$t_1 = \theta(t_i^1) \quad\mbox{and}\quad t_2 =\theta(t_i^2).$$

{\bf Proposition 2.}
{\sl Assume that the types  are terms of a finite, regular
order-sorted signature and that there are only
coercions between base types, direct embeddings and
structural coercions. Then for any type $t$, the set}
$${\cal S}_t = \{ \sigma \mid \exists t'\, . \, t' : \sigma \mbox{ and }
t \subtype t' \}$$
{\sl is effectively computable.}

\begin{proof}
We claim that the set ${\cal S}_t$
will be computed by ${\sf CSGT}(t)$
(see Fig.~\ref{figalCSGT}).

All computations which are used in 
${\sf CSGT}$ and ${\sf CSBT}$
can be performed effectively.
Since the signature is finite
there are always only finitely many possibilities 
which have to be checked in the existential clauses of
the algorithms and so 
algorithm ${\sf CSBT}$ will terminate and so will
${\sf CSGT}$.
Algorithm ${\sf CSGT}$ 
is correct (i.\,e.\ ${\sf CSGT}(t) \subseteq {\cal S}_t$),
because only types and
the sort of  types  $t$ can be coerced to are computed.
Its completeness
(i.\,e.\ ${\sf CSGT}(t) \supseteq {\cal S}_t$)
 follows from the fact that structural
coercions cannot add new sorts to ${\cal S}_t$.
\qed

\newsavebox{\algcsbt}
\newsavebox{\algcsgt}
\newsavebox{\algcsgtandcsbt}
\sbox{\algcsbt}{\begin{minipage}[l]{0.94\textwidth}
\begin{center}
${\cal S} \leftarrow {\sf CSBT}(t)$.
\end{center}
[Sorts of types a base type $t$ is coercible to.
 ${\cal S}$ is the set
of sorts of types in which $t$ can be coerced to.
Assumes that the signature is finite, only direct embeddings
and structural coercions are present.]
\begin{deflist}{(1)}
\item[(1)] [Initialize.] ${\cal T} 
                 \leftarrow \{ t' \mid t \subtype t' \mbox{ and }
                               t' \mbox{ is a base type}
                         \}$; \\
${\cal S} \leftarrow \{ \sigma' \mid t : \sigma' \}$;
${\cal S}' \leftarrow {\cal S}$; ${\cal T}' \leftarrow {\cal T}$.
\item[(2)] [Compute Direct Embeddings.] 
{\bf for} $\overline{t} \in {\cal T}$ 
       {\bf do}
     $\aldesbegbr$ 
{\bf if} there are
   $\overline{\sigma}$,
   $f :(\sigma_1 \cdots \sigma_n) \sigma'$,
   $i \in \{ 1, \ldots, n \}$ such that
      $\overline{t} : \overline{\sigma}$ 
          and $\sigma_i=\overline{\sigma}$
          and $i \in {\cal D}_f$
          and $\sigma' \notin {\cal S}$
{\bf then}
    $\aldesbegbr$
    ${\cal S}' \leftarrow {\cal S}' \cup \{ \sigma' \}$;
    ${\cal T}' \leftarrow {\cal T}' \cup
             \{ f(v_{\sigma_1}, \ldots, v_{\sigma_n}) \}$
       $\aldesendbr$ $\aldesendbr$.
\item[(3)] [Iterate if something is added.] {\bf if}
             ${\cal S'} \neq {\cal S}$
      {\bf then} $\aldesbegbr$ \\
        ${\cal S} \leftarrow {\cal S}'$;
    ${\cal T} \leftarrow {\cal T}' $;
{\bf goto~(2)} $\aldesendbr$.
\end{deflist}
\end{minipage}
}
\sbox{\algcsgt}{\begin{minipage}[l]{0.94\textwidth}
\begin{center}
${\cal S} \leftarrow {\sf CSGT}(t)$.
\end{center}
[Sorts of types a type $t$ is coercible to.
 ${\cal S}$ is the set
of sorts of types in which $t$ can be coerced to.
Assumes that the signature is finite, only direct embeddings
and structural coercions are present.]
\begin{deflist}{(1)}
\item[(1)] [$t$ base type.] {\bf if} $\com(t)=0$ {\bf then}
                 $\aldesbegbr\,
                   {\cal S} \leftarrow {\sf CSBT}(t)$;
                  {\bf return}$\aldesendbr$. 
\item[(2)] [Recurse.] Let $t=g(t_1, \ldots, t_m)$; \\
           {\bf for} $i = 1, \ldots, m$ 
              {\bf do} ${\cal S}_i \leftarrow {\sf CSGT}(t_i)$; \\
      {\bf for} $(\sigma_1, \ldots, \sigma_m) \in
                   {\cal S}_1 \times \cdots \times {\cal S}_m$
          {\bf do} $\aldesbegbr$ \\
        {\bf if}
               there is 
           $g: (\sigma_1 \cdots \sigma_m) \overline{\sigma}$
               such that
              $\overline{\sigma} \notin {\cal S}$
        {\bf then}
           $\aldesbegbr$ \\
             ${\cal T} \leftarrow 
                {\cal T} \cup \{ g(v_{\sigma_1}, \ldots, v_{\sigma_m}) \}$; 
             ${\cal S} \leftarrow {\cal S} \cup \{ \overline{\sigma} \} $;
            ${\cal S}' \leftarrow {\cal S}$; 
             ${\cal T}' \leftarrow {\cal T}$
               $\aldesendbr$ $\aldesendbr$.
\item[(3)] [Compute Direct Embeddings.]
{\bf for} $\overline{t} \in {\cal T}$
       {\bf do}
     $\aldesbegbr$
{\bf if} there are
   $\overline{\sigma}$,
   $f :(\sigma_1 \cdots \sigma_n) \sigma'$,
   $i \in \{ 1, \ldots, n \}$ such that
      $\overline{t} : \overline{\sigma}$
          and $\sigma_i=\overline{\sigma}$
          and $i \in {\cal D}_f$
          and $\sigma' \notin {\cal S}$
{\bf then}
    $\aldesbegbr$
    ${\cal S}' \leftarrow {\cal S}' \cup \{ \sigma' \}$;
    ${\cal T}' \leftarrow {\cal T}' \cup
             \{ f(v_{\sigma_1}, \ldots, v_{\sigma_n}) \}$
       $\aldesendbr$ $\aldesendbr$.
\item[(4)] [Iterate if something is added.] {\bf if}
             ${\cal S'} \neq {\cal S}$
      {\bf then} $\aldesbegbr$ \\
        ${\cal S} \leftarrow {\cal S}'$;
    ${\cal T} \leftarrow {\cal T}' $;
{\bf goto~(3)} $\aldesendbr$.
\end{deflist}
\end{minipage}
}
\sbox{\algcsgtandcsbt}{
\begin{minipage}[l]{0.96\textwidth}
\begin{center}
\usebox{\algcsgt}
\end{center}
\vspace{2\bigskipamount}
where
\vspace{2\bigskipamount}
\begin{center}
\usebox{\algcsbt}
\end{center}
\end{minipage}
}
\begin{figure}[tbhp]
\hfil\fbox{\usebox{\algcsgtandcsbt}}
\caption{Algorithms computing sorts of 
             types a given type can be coerced to}
\label{figalCSGT} 
\end{figure}


\end{proof}

In the following we will rule out
antimonotonic structural coercions,
i.\,e.\ we will require that
${\cal A}_f = \emptyset$ for all
type constructors $f$. 

Notice that the restriction 
${\cal A}_f = \emptyset$ 
does not exclude type constructors like
$\tf{FS}$ from the framework.
Only the automatic insertion of a coercion
giving rise to the antimonotony is excluded.
For instance, instead of having $\tf{FS}$ as
a type constructor which is
antimonotonic in its first argument and monotonic in
its second, it is one which is only monotonic in its
second argument. Such a restriction does not seem
to cause a loss of too much expressiveness.
This is an important difference to the system
in \cite{Como91}, in which  all type constructors
have to be monotonic in all arguments.
Type constructors which are antimonotonic in some
argument have to be excluded from that system in general,
because it is not  possible that a type constructor being
antimonotonic in some argument can be made monotonic
in that argument without changing the
intended meaning of the type constructor.
Thus our framework is more general in this respect
than the one in \cite{Como91}.
However, direct embeddings are a special form
of the ``rewrite relations'' for coercion considered
in that paper. 
So  the following can be seen as a solution  
for one of the open problems stated in \cite{Como91},
namely finding restrictions on the system of coercions
which will yield a decidable type inference problem.

{\bf Definition 26.}
{\sl If for two types $t_1$ and $t_2$ there is a type $t$
such that $t_1 \subtype t$ and $t_2 \subtype t$
then $t$ is called a {\em common upper bound}
of $t_1$ and $t_2$.}

{\sl A {\em minimal upper bound} $\mub(t_1,t_2)$ of two types
$t_1$ and $t_2$ is a type $t$ satisfying the following
conditions.
\begin{enumerate}
\item The type $t$ is a common upper bound of $t_1$ and $t_2$.
\item If $t'$ is a type which is a common
upper bound of $t_1$ and $t_2$
 such that $t' \subtype t$,
then $t \subtype t'$.
\end{enumerate}
A {\em complete set of minimal upper bounds}
for two types $t_1$ and $t_2$
is a set $\CSMUB(t_1,t_2)$ such that
\begin{enumerate}
\item
 all $t \in \CSMUB(t_1,t_2)$
are a minimal common upper bound of $t_1$ and $t_2$, and
\item for every type $t'$
which is a common upper bound of $t_1$ and $t_2$ there is
a $t \in \CSMUB(t_1,t_2)$ such that $t \subtype t'$.
\end{enumerate}}

If two types $t_1$ and $t_2$ have no minimal
upper bound then the complete sets of minimal upper bounds
are all empty. In this case we will write
$\CSMUB(t_1,t_2)=\emptyset$.
We will write $|\CSMUB(t_1,t_2)|$ to denote the
smallest cardinality
of  a complete set of minimal upper bounds of $t_1$ and $t_2$.

If the partial order induced by $\subtype$ is
a quasi-lattice then
$|\CSMUB(t_1,t_2)| \leq 1$ for all types $t_1$ and
$t_2$.
However, as  we have seen in Sec.~\ref{secpropcoerpreord}
this partial order will not be a quasi-lattice in general.

In the following we will assume that for
any two {\em base types} $t_1^{\rm b}$ and $t_2^{\rm b}$ a
{\em finite}
complete
set of minimal upper bounds can be computed effectively,
say by ${\sf CSMUBBT}(t_1^{\rm b},t_2^{\rm b})$.
We will give an algorithm computing for any
two types $t_1$ and $t_2$ a complete set of minimal
upper bounds and will show that this set is finite.

{\bf Theorem 9.}
{\sl Assume that all coercions are coercions between base types, direct
embeddings and structural coercions.  Moreover, assume that for all
type constructors $f$ there is at most one direct embedding position,
i.\,e.\ $|{\cal D}_f| \leq 1$, and no antimonotonic coercions are
present, i.\,e.\ ${\cal A}_f=\emptyset$, and for any base types
$t_1^{\rm b}$ and $t_2^{\rm b}$ there is a finite complete set of
minimal upper bounds with respect to the set of base types which can
be effectively computed by a function ${\sf CSMUBBT}(t_1^{\rm
b},t_2^{\rm b})$.}

{\sl Then for any two types $t_1$ and $t_2$ there is a finite complete set
of minimal upper bounds which can be effectively computed.}

\begin{proof}
We claim that algorithm ${\sf CSMUBGT}$
(see Fig.~\ref{algCSMUBGT})
 terminates
for any input parameters $t_1$ and $t_2$ and
computes a complete set of minimal upper bounds
which is finite.


\newsavebox{\algcsmubgt}
\newsavebox{\algcsmubbt}
\newsavebox{\algcsmuall}
\sbox{\algcsmubgt}{
\begin{minipage}[l]{0.94\textwidth}
\begin{center}
${\cal U} \leftarrow {\sf CSMUBGT}(t_1,t_2)$
\end{center}
[${\cal U}$ is a complete set of minimal upper bounds
of two types $t_1$ and $t_2$. Requires that 
only direct embeddings and structural coercions are used,
$|{\cal D}_f| \leq 1$
and ${\cal A}_f = \emptyset$ for any type constructor
$f$. Assumes that algorithm {\sf CSMUBBT} returns a finite
set.]
\begin{deflist}{(1)}
\item[(1)] [$t_1$ and $t_2$ base types.]
{\bf if} $\com(t_1)=1$ and $\com(t_2)=1$
     {\bf then }
                $\aldesbegbr$
                 ${\cal U} \leftarrow {\sf CSMUBBT}(t_1,t_2)$;
                  {\bf return}
                $\aldesendbr$.
\item[(2)] [Ensure that $\com(t_1) \leq \com(t_2)$.]
            {\bf if} $\com(t_1) > \com(t_2)$ {\bf then}
                $\aldesbegbr$
                 $h \leftarrow t_1$; $t_1 \leftarrow t_2$;
                $t_2 \leftarrow h$
                $\aldesendbr$.
\item[(3)] [$t_1$ a base type.]
          {\bf if} $\com(t_1) = 1$ {\bf then}
             $\aldesbegbr$ \\
          let $t_2 = f(t_2^1, \ldots, t_2^n)$; \\
          {\bf if} $|{\cal D}_f| = 0$ {\bf then}
            $\aldesbegbr$
             ${\cal U} \leftarrow \emptyset$;
             {\bf return}$\aldesendbr$; \\
               let ${\cal D}_f = \{ i \}$; \\
               ${\cal U}' \leftarrow {\sf CSMUBGT}(t_1,t_2^i)$;
               \begin{deflist}{(3.1)}
               \item[(3.1)] {\bf if} ${\cal U}' = \emptyset$
                  {\bf then} $\aldesbegbr$
                           ${\cal U} \leftarrow \emptyset$;
                           {\bf return}$\aldesendbr$;
                \item[(3.2)] {\bf if} ${\cal U}' \neq \emptyset$
                                    {\bf then} $\aldesbegbr$ 
                       {\bf if} $i \in {\cal M}_f$ {\bf then}
                         $\aldesbegbr$
                             ${\cal U} \leftarrow \emptyset$; \\
                             {\bf for} $t' \in {\cal U}'$
                                     {\bf do} \\
                             ${\cal U} \leftarrow {\cal U} \cup
                                 \{ f(t_2^1, \ldots, t_2^{i-1},
                                       t', t_2^{i+1},\ldots, t_2^n) \}$
                          $\aldesendbr$; \\
                       {\bf if} $i \notin {\cal M}_f$ {\bf then}
                         $\aldesbegbr$
                       {\bf if} $t_2^i \in {\cal U}'$
                     {\bf then} ${\cal U} \leftarrow \{t_2\}$ \\
                     {\bf else} ${\cal U} \leftarrow \emptyset$$\aldesendbr$ 
                           {\bf return}
                              $\aldesendbr$ $\aldesendbr$.
                  \end{deflist}
\item[(4)] [General case.] let $t_1 = g(t_1^1, \ldots, t_1^m)$; 
                           let $t_2 = f(t_2^1, \ldots, t_2^n)$;
               ${\cal U} \leftarrow \emptyset$.
\item[(5)] [Structural coercions.] {\bf if} $f=g$ {\bf then}
                  $\aldesbegbr$ \\
                  {\bf for} $i \in {\cal M}_f$ {\bf do}
                        ${\cal U}_i \leftarrow {\sf CSMUBGT}(t_1^i,t_2^i)$; \\
                 let ${\cal M}_f = \{ j_1, \ldots, j_l \}$; \\
        {\bf if} $t_1^k = t_2^k$
           for all $k \in \{1, \ldots, n \} - {\cal M}_f$
         {\bf then} $\aldesbegbr$ \\
           {\bf for} $(t'_{j_1}, \ldots, t'_{j_l}) \in
                       {\cal U}_{j_1} \times \cdots \times {\cal U}_{j_l}$
                  {\bf do}
                     $\aldesbegbr$ \\
            {\bf for} $k \in \{1, \ldots, n \} - {\cal M}_f$
                 {\bf do} $t'_k \leftarrow t_1^k$; \\
                   ${\cal U} \leftarrow {\cal U} \cup
                         \{ f(t'_1, \ldots, t'_n) \}$
                    $\aldesendbr$ $\aldesendbr$ $\aldesendbr$. 
\item[(6)] [Direct embeddings in $g$.] 
                    {\bf if }
        $|{\cal D}_g| = 1$ {\bf then} $\aldesbegbr$ \\
let ${\cal D}_g = \{ i \}$; 
               ${\cal U}' \leftarrow {\sf CSMUBGT}(t_1^i,t_2)$; \\
                 {\bf if} ${\cal U}' \neq \emptyset$
                                    {\bf then} $\aldesbegbr$ 
                       {\bf if} $i \in {\cal M}_g$ {\bf then}
                         $\aldesbegbr$ 
                             {\bf for} $t' \in {\cal U}'$
                                     {\bf do} 
                             ${\cal U} \leftarrow {\cal U} \cup
                                 \{ g(t_2^1, \ldots, t_2^{i-1},
                                       t', t_2^{i+1},\ldots, t_2^m) \}$
                          $\aldesendbr$; \\
                       {\bf if} $i \notin {\cal M}_g$ 
                       and $t_1^i \in {\cal U}'$
                     {\bf then} ${\cal U} \leftarrow {\cal U} \cup \{t_1\}$
                              $\aldesendbr$ $\aldesendbr$.
\item[(7)] [Direct embeddings in $f$.] {\bf if} 
               $|{\cal D}_f| = 1$ {\bf then} $\aldesbegbr$ \\
let ${\cal D}_f = \{ i \}$; 
               ${\cal U}' \leftarrow {\sf CSMUBGT}(t_1,t_2^i)$; \\
                 {\bf if} ${\cal U}' \neq \emptyset$
                                    {\bf then} $\aldesbegbr$ 
                       {\bf if} $i \in {\cal M}_f$ {\bf then}
                         $\aldesbegbr$  
                             {\bf for} $t' \in {\cal U}'$
                                     {\bf do} 
                             ${\cal U} \leftarrow {\cal U} \cup
                                 \{ f(t_2^1, \ldots, t_2^{i-1},
                                       t', t_2^{i+1},\ldots, t_2^n) \}$
                          $\aldesendbr$; \\
                       {\bf if} $i \notin {\cal M}_f$ and 
                        $t_2^i \in {\cal U}'$
                     {\bf then} ${\cal U} \leftarrow {\cal U} \cup \{t_2\}$
                              $\aldesendbr$ $\aldesendbr$.
\end{deflist}
\end{minipage}
}
\sbox{\algcsmuall}{
\begin{minipage}[l]{0.96\textwidth}
\begin{center}
\usebox{\algcsmubgt}
\end{center}
\end{minipage}
}
\begin{figure}[tbhp]
\hfil\fbox{\usebox{\algcsmuall}}
\caption{An algorithm computing a complete set of
minimal upper bounds}
\label{algCSMUBGT}
\end{figure}



We will prove this claim by
induction on the complexity of $t_1$
and $t_2$ along the steps of the algorithm.

If $t_1$ and $t_2$ are base types, then
${\sf CSMUBBT}(t_1,t_2)$ is also
 a complete set of minimal
 upper bounds of $t_1$ and
$t_2$ with respect to
all types. This subclaim can be proved by
induction on the complexity of possible common upper bounds
of $t_1$ and $t_2$ using the assumption that for
any type constructor $f$ we
have $|{\cal D}_f| \leq 1$.\footnote{Without this assumption
the subclaim is false in general.}

So the algorithm terminates for the case of base types
and
returns a finite set which is a complete set of minimal upper
bounds for $t_1$ and $t_2$.

The algorithm will terminate for all other $t_1$
and $t_2$, too.
Recursive calls of the algorithm are done on arguments
of which at least one has a strictly smaller complexity.
Since any of the recursive calls returns a finite set,
only  finitely many iterations have to be performed
by the algorithm and the returned set is finite.

Since only direct embeddings and
monotonic structural coercions are present,
any element of ${\cal U}$ is a minimal upper
bound of $t_1$ and $t_2$.
The set ${\cal U}$ will be a complete set of
minimal upper bounds, because
$|{\cal D}_f| \leq 1$ for any type constructor
and all other possibilities of minimal upper bounds
for $t_1$ and $t_2$ are covered by the algorithm. 


Since ${\sf CSMUBGT}$ returns a finite set
of types, the existence of a finite set of minimal
upper bounds follows from the correctness of
the algorithm.
\qed
\end{proof}

\begin{remark}
Since algorithm ${\sf CSMUBGT}$ uses the type constructors
given by its arguments and does not have to perform
a search on all type constructors,
it is not necessary that the signature is finite.
It is only necessary that there is an effective algorithm
which computes for any type constructor $f$ the sets
${\cal D}_f$ and ${\cal M}_f$, and that
the conditions imposed on algorithm ${\sf CSMUBBT}$ 
are fulfilled.\footnote{If the signature is finite,
these conditions will  always be fulfilled if the coercions
between the base types are effectively given.}

An example of an infinite signature
with such properties is
a finite signature extended with a
type constructor $\tf{M}_{m,n}$  for any
$m,n \in \NN$ 
with the intended meaning of building the $m\times n$-matrices
over commutative rings.
It is natural to define ${\cal M}_{\tf{M}_{m,n}}=\{1\}$ for all
$m,n \in \NN$ and to have ${\cal D}_{\tf{M}_{m,n}}=\emptyset$ for
$m\neq n$ and ${\cal D}_{\tf{M}_{n,n}}=\{1\}$ for any $n \in \NN$.
\end{remark}

\subsubsection{Complexity of Type Inference}
\label{secomtycoer}

In \cite{Wand89} and \cite{Linc92}
the complexity of type inference for expressions of the $\lambda$-calculus 
which are typed by allowing various possibilities of coercions are
investigated.

In \cite{Linc92} the problem is shown to be NP-hard if the order given
by the coercions is arbitrary but fixed by reducing the following
problem on partial orders called {\sc Pol-Sat} to it.
\begin{quote}
Given a partial order $\langle P, \leq \rangle$ and a set of
inequalities $I$ of the form $p \leq w$, $w \leq w'$, where $w$ and
$w'$ are variables, and $p$ is a constant drawn from $P$, is there an
assignment from variables to members of $P$ that satisfies all
inequalities of $I$?
\end{quote}
{\sc Pol-Sat} is an NP-complete problem.  It is shown to be NP-hard by
reducing the {\sf 3-SAT}-problem to it.\footnote{A proof that 
{\sf 3-SAT} is NP-complete can be found e.\,g.\ in
\cite[p.~347]{Davi94}.}  However, if only lattices are
allowed as partial orders in {\sc Pol-Sat} then the problem is
decidable in linear time.

A quite similar problem on partial orders, called {\sc Po-Sat} is
introduced in \cite{Wand89}, which is reduced in polynomial time to a
type inference problem using polymorphic functions.  The problem {\sc
  Po-Sat} is proven to be NP-complete for arbitrary partial orders but
to be solvable in polynomial time if the partial orders are restricted
to finite quasi-lattices.

A quite systematic study of the complexity of decision problems for
various partial orders which might be relevant for type inference is
given in \cite{Tiur92}.

\section{Other Typing Constructs}
\label{chapothtyc}

\subsection{Partial Functions}
\label{secpartfunc}

Many functions arising in the area of computer algebra are only
partially defined. Some basic examples are

\begin{enumerate}
\item division in a field, which is defined for non-zero elements only;
\item matrices over fields have inverses only, if they are regular;
\item the square-root over the reals exists for non-negative
values only.
\end{enumerate}

We could make partial functions total by introducing new types --- the
type of elements, on which the function is defined.

The following examples, which are taken from \cite{Farm90},
show that there are severe problems  if we were to take this solution.

Let $f$ be the binary functions over the reals defined by
$$f(x,y) = \sqrt{x-y}.$$
The function $f$ cannot be represented as a {\em binary\/} total-function
in a many-sorted algebra since the domain of $f$ is not a set of the
form $D_x \times D_y$, where $D_x$ and $D_y$ are
subsets of the real numbers.

It makes good sense to view division in a field as a partial function
with the second argument having the type of the field.  If in the case
of the rationals we were to restrict the second argument to a type
``non-zero rationals'', we would have made this function total.
However, this solution has a severe drawback.  A term such as
$1/(2-1)$ is no longer well-formed, since ``$-$'' is a function into
the rationals and not into the non-zero rationals only.

The usual solution which is taken in connection with
many-sorted and order-sorted algebras uses the ``opposite'' way.

New elements --- ``error  elements'' --- are introduced
and new types are built by adjoining these error elements.
A partial function is made total by setting the value
of the function to be an error element if it is undefined before,
see e.\,g.\ \cite{Smol89} for a more detailed description
of this construction.

This construction is also used in universal algebra in order
to embed a partial algebra in a full algebra,
see e.\,g.\ \cite[p.~79]{Grae79}.

In the area of computer algebra this approach is taken in
the computer algebra system {\sf Axiom}.

The disadvantage of this approach is that we loose information.
If we consider terms built out of partial functions and
total functions, we have to repeat the construction.
Since the range of the partial function has increased,
a previously total function has become partial, since it is
not defined on the error value.

In the general framework of many-sorted or order-sorted computations,
it might be difficult to regain the lost information.  There are
important examples, where the set of elements on which a partial
function is defined is only recursively enumerable but not recursive
(see e.\,g.\ \cite[p.~342]{Smol89} for an example).

In connection with a computer algebra system, a better solution should
be possible. In most cases, the set of elements a partial function is
defined on can easily be decided; in our examples a simple test for
being non-zero, non-negative or calculating a determinant would have
been sufficient.

Hence, in these cases it is decidable whether a
{\em ground term\/} is well formed, i.\,e. has an error value or not.

Finding conditions and algorithms which tell the (minimal) type of an
arbitrary term is an interesting problem, whose solution would be of
practical significance.

\subsubsection{Retractions}
\label{specialize}

The sum of two polynomials is in general again a polynomial.
However, if we add the polynomials $(-x+5)$ and $(x+2)$, we obtain
the  constant polynomial $3$ as a result.
For future computations it would be useful if we {\em retract}
the type of the result from \tf{integral polynomial} to
\tf{integer}

Since retractions are partially defined implicit conversion functions
the general framework developed for other kinds of partial functions
also applies to retractions.

\subsection{Types Depending on Elements}
\label{chtydeel}

In this section we will discuss typing constructs which correspond to
the case of elements as parameters to domain constructors in {\sf
  Axiom}.  We will use the term ``types depending on elements'' to
describe these types, because it seems to be more or less standard for
type theories including such constructs.

There are some important examples of data structures whose type
depends on a non-negative integer.
\begin{itemize}
\item Elements of $\ZZ_m$.
\item Vectors of dimension $n$.
\item The $m \times n$-matrices.
\end{itemize}
However, the elements a type can depend on are not restricted to
integers.

An algebraic number $\alpha$ over $\QQ$ is usually represented by its
minimal polynomial over the rationals.  Thus, an element of the field
$\QQ[\alpha]$ has a type depending on some polynomial over the
rationals.

An example of a type which depends on a matrix (namely the matrix
defining a quadratic form) is the one which is built by the domain
constructor $\tf{CliffordAlgebra}$ (see \cite[Sec.~9.9]{Jenk92}.

In group theory programs, very often a group is represented with
respect to its generators, cf.\ \cite{GAPx17}.  So the concept of
types depending on elements is a possibility to treat certain
structures which are treated as objects of a computation in a certain
context as {\em types} in another one (cf.\ Sec.~\ref{sgroupth}).

Some of the examples given above could be reformulated such that the
concept of types depending on elements is no longer necessary in order
to describe them.  So it might be sufficient to have only a type of
matrices of arbitrary dimension (over some ring) in the system and not
a type of $m \times n$-matrices.  Then matrix-multiplication or even
addition of two matrices would be partial functions only.  A treatment
of partial functions (cf.\ Sec.~\ref{secpartfunc}) would be sufficient
and the additional concept of types depending on elements could be
avoided.

However, for the case of $\ZZ_m$ it seems to be necessary to have for
any $m \in \NN$ also a type corresponding to $\ZZ_m$ in a system which
also allows the possibility to have computations on the integer $m$.

So the concept of types depending on elements is important for many
computer algebra applications.  Unfortunately, as we will show below
it is not possible to have type-safe compile-time type-checking.

\subsubsection{Undecidability of Type Checking}

\label{undetychtydeel}

{\bf Lemma 8.}
\label{lemundetychtydeel}
{\sl Let ${\cal R}$ be the class of unary recursive functions.
Then the following questions are undecidable:
\begin{enumerate}
\item For $f \in {\cal R}$, is  $f(x)=n$
for some fixed $n \in \NN$ and for all $x$?
\item  For $f \in {\cal R}$, is $f(x)$  a prime number for all $x$?
\item  For $f \in {\cal R}$, is $\gcd(f(x),n)=1$ for some fixed $n \in \NN$ 
and for all $x$?
\end{enumerate}}

\begin{proof}
All of the questions above are equal to determining the membership of
$f$ in certain classes of partial recursive functions, which are all
non-trivial.  So the lemma is proved by applying Rice's Theorem (see
e.\,g.\ \cite[p.~150]{Odif92}).  \qed
\end{proof}

Assume that the language is universal, i.\,e.\ every partial recursive
function can be computed in the language.  Assume that there is a type
corresponding to $\NN$ present in the language and that indeed every
unary recursive function can be represented in the system as one
having type $\NN \longrightarrow \NN$.  Moreover, assume that there is
a type corresponding to $\ZZ_m$ for any $m \in \NN$.

Let $n \in \NN$ and let $f: \NN \longrightarrow \NN$ be a unary
recursive function.  By Lemma 8 it cannot be
decided by a compiler, whether $\ZZ_{f(x)}$ and $\ZZ_n$ are equal.
Thus having $a \in \ZZ_{f(x)}$ and $b \in \ZZ_n$ and having a
polymorphic operation ${\tt op}$ with type
$$\forall t \, . \, t \times t \longrightarrow \tf{Boolean}$$
like the check for equality it cannot be decided at compile time
whether 
$${\tt op}(a,b)$$
is well typed.

Determining whether $\ZZ_{f(x)}$ is a field, i.\,e.\ whether $f(x)$ is
prime is also not possible at compile time.  So it cannot be decided
whether computations requiring that $\ZZ_{f(x)}$ is a field are legal.

Since it cannot be decided by the compiler whether $\gcd(f(x),n)=1$ it
is also impossible to decide whether the lifting connected with the
Chinese remainder theorem can be applied to an element of $\ZZ_{f(x)}$
and to one of $\ZZ_n$ giving one of $\ZZ_{f(x) \cdot n}$.

In the following we will show that it is necessary to allow such
run-time computations of elements a type depends on for many important
applications in computer algebra.

\subsubsection{Necessity of Run-Time Computations
of Elements Types Depend on}
 
Frequently, computations in $\ZZ_m$\footnote{Or in the ring of
  polynomials over $\ZZ_m$, etc.  In our framework these structures
  can be all expressed as types having $\ZZ_m$ substituted for a type
  variable.}  are done in the context of computer algebra because of
the following observation:

If one wants to have the solution for a problem over the integers,
then it is often possible to compute a $b \in \NN$ (a ``bound'') such
that for all $n \geq b$ the result of the computation in $\ZZ_n$ can
easily be extended to a solution for the problem over the
integers.\footnote{Many books on computer algebra can serve as
references, e.\,g.\ \cite{Buch82} --- especially \cite{Laue82} or
\cite{Kalt83a} --- or \cite{Dave88}, \cite{Lips81}, \cite{Gedd92}, and
also \cite{Knut71}.}

Very often, these computations are not done directly in $\ZZ_b$, but
in $\ZZ_{p_1}, \ldots, \ZZ_{p_h}$ for primes $p_1, \ldots, p_h$.  The
results are then ``lifted'' either to $\ZZ_{p_1 \cdots p_h}$ by an
application of the Chinese remainder theorem or to $\ZZ_{p^l}$ by a
Hensel lifting.  The choice of $p_1, \ldots, p_h$ resp.\ of $p$ and
$l$ are such that $p_1 \cdots p_h \geq b$ resp.\ $p^l \geq b$.

However, the class of algorithms which is used to compute the bounds
can be fairly complicated.  Technically speaking, if $f(x)$ and $g(x)$
are two functions that can be computed by the class of algorithms used
for the bound computations, then it is undecidable whether
$$f(x) \equiv g(x) \quad\quad\forall x.$$
Let us now assume that we could restrict the occurring types to the
ones corresponding to $\ZZ_{p_1 \cdots p_k}$, where $\{p_1, p_2, p_3,
\ldots \}$ is the set of prime numbers.  However, it is undecidable
whether $p_1 \cdots p_k = p_1 \cdots p_{k'}$, if $k$ and $k'$ are
minimal such that $p_1 \cdots p_k \geq f(x)$ and $p_1 \cdots p_{k'}
\geq g(x)$.  So a compiler cannot decide whether a statement involving
an element of $\ZZ_{p_1 \cdots p_k}$ and one of $\ZZ_{p_1 \cdots
p_{k'}}$ requiring both to have the same type\footnote{Simple
operations such as a test for equality or addition can serve as
examples.}  will lead to a typing error or not.

\subsubsection{Calculi Dealing with Types Depending on Elements}

The results of this section show that it is useful to distinguish
between domains and elements as parameters of domain constructors.

Having only type classes as additional typing construct a static
typechecking is possible in principle in the former case.  In the
latter case it becomes undecidable, where we have argued that this
undecidability results are relevant for many examples occurring in
practical computer algebra applications.

For a user interface it is usually sufficient to perform type
inference on expressions which do not allow recursion and which do not
form a Turing-complete language for computations on elements types
depend on.

So the problems which yield that the type inference problem and even
the type checking problem is undecidable in the case of a computer
algebra language do not apply to the case of a user interface of a
computer algebra system.

Since the type of an element another type depends on can nevertheless
be quite complicated (see the examples given above) it seems to be
useful to have some sophisticated techniques available also for this
case.

During the last years several general type theories having the concept
``types depending on elements'' have been developed.  Some are
Martin-L\"of's Type Theory \cite{Mart80}, and the {\em Calculus of
  Constructions} of Coquand and Huet \cite{Coqu86}, They have been
explored extensively, especially as ``logical frameworks''
\cite{Huet91}.  For this purpose several subcalculi and variations
such as LF \cite{Harp93}, or Elf \cite{Pfen89}, \cite{Pfen91},
\cite{Pfen92} have been defined.  Some extensions of unification
algorithms to these type theories have been given in \cite{Elli89},
\cite{Pfen91a}.  For the purpose of computer algebra probably
another variant of this theories will be more suited than the
existing.  Nevertheless, it seems to be very likely that some of the
obtained results are applicable to the type inference problem for a
user interface of a computer algebra system.

\chapter{Finite Fields in Axiom (Grabmeier/Scheerhorn)}

This was written by Johannes Grabmeier and Alfred Scheerhorn.
\href{http://axiom-developer.org/axiom-website/GroupTheoryII/Salomone.html}
{Matthew Salomone's video course}\cite{Salo16}
provides useful background material.

Finite fields play an important role in mathematics and in many
applications as coding theory or factorizing polynomials in computer
algebra systems. They are the finite sets which have a computational
structure as the classical fields of rational or complex numbers,
i.e. addition + and multiplication with inverses and the usual group
axioms as commutativity and associativity laws and their interaction
via the distributivity laws. For further details see any book on
algebra or our preferred reference for finite fields \cite{Lidl83}.

The finite fields are classified: For each prime power $q=p^n$ there
is up to isomorphism exactly one finite field of these size and there
are no more. So far this looks nice, easy and complete. However, there
are different constructions of a finite field of a given size $q$,
each having different advantages and disadvantages.  This paper deals
with such constructions and implementations in the computer algebra
system Axiom, various isomorphims and embeddings. We have three
different kinds of constructions, namely polynomial basis
representation, normal basis representation, and cyclic group
representation.

The various advantages and disadvantages which will be discussed along
with the special implementations of the representations in the
respective sections. All are strongly connected with the construction
of irreducible polynomials which have additional properties. The user
of Axiom may choose the representation which best meets the needs for
his applications. For each type we have provided automatic choices as
well as the liberty to use a favourite polynomial. In addition there
are implementations for mechanisms to convert the data from one
representation to the other.

The paper is organized as follows: For convenience of the readers we
first recall some basic facts from the theory of finite fields. Then
we introduce our category design of the finite field world in
Axiom. Next comes the description of all the functions which are valid
and useful for every finite field. We employ the abstract datatype
concepts of Axiom, which allows to implement such functions in the
default packages of these categories. This makes an implementation in
the different domains superfluous. Using a special kind of
representation the implementation of many functions can be improved in
order to have a more effcient computation. We describe these improved,
additional domain implementations in the sections according to the
special representations. Section \ref{section10} is devoted to the various
constructions of polynomials. In section \ref{section12} 
we finally give results of
time comparison of the various representations.

\section{Basic theory and notations}
\label{section2}

\href{http://axiom-developer.org/axiom-website/GroupTheoryII/Salomone.html}
{Salomone's lectures}
provide background material for this section.

We denote a finite field with $q=p^r$ elements, $p$ a prime and 
$r \in \mathbb{N}$, by $GF(q)$.  The {\sl prime field GF(p)} can be 
constructed as $\mathbb{Z}/p\mathbb{Z} = \{0,1,\ldots,p-1\}$.
The finite field $GF(q)$ is an algebraic extension of the field $GF(p)$ 
and isomorphic to the splitting field of $X^q-X$ over $GF(p)$. 
Let $\alpha,\beta \in GF(q)$ and $c \in GF(p)$, then we have 
$(\alpha+\beta)^p=\alpha^p+\beta^p$ and $c\alpha^p=(c\alpha)^p$.
Therefore powering with $p$ or powers of $p$ is a linear operation 
over $GF(p)$. Let $E=GF(q^n)$ be an extension of $F=GF(q)$ of degree
$n \in \mathbb{N}$. The automorphism group of $E$ over $F$ is cyclic of 
order $n$ and generated by
\[\sigma : \alpha \rightarrow \alpha^q\]
which is called a {\sl Frobenius automorphism}.

Let $f \in F[X]$ be a monic, irreducible polynomial of degree $n$. Then
\[E \simeq F[X]/(f)\]
where $(f)=f\cdot F[X]$ denotes the principal ideal generated by $f$, and
the isomorphism is given by 
$\alpha \mapsto (X {\rm\ mod\ }f)$, where $\alpha$
is a root of $f$ in $E$. $\alpha$ generates a {\sl polynomial basis}
$\{1,\alpha,\ldots,\alpha^{n-1}\}$ of $E$ over $F$. Every element 
$\beta \in E$ can be expressed uniquely in the form
\[\beta=\sum_{i=0}^{n-1}{b_i\alpha^i}\]
This kinds of representing elements of $E$ is called {\sl polynomial basis
representation}.

Let $\alpha \in E$, the monic, irreducible polynomial $f \in F[X]$ with
$f(\alpha)=0$ is called the {\sl minimal polynomial} of $\alpha$ over $F$.
All the roots of $f$ are given by the set of {\sl conjugates}
\[\{\alpha,\alpha^q,\ldots,\alpha^{q^n}\}\]
of $\alpha$ of $F$. Therefore
\[f=\prod_{i=0}^{n-1}{(X-\alpha^{q^i})}\]

The {\sl trace} $T_{E/F}(\alpha)$ and the {\sl norm} $N_{E/F}(\alpha)$ of 
$\alpha$ over $F$ are defined by the sum and the product of the conjugates
of $\alpha$, respectively,
\[T_{E/F}(\alpha)=\sum_{i=0}^{n-1}{\alpha^{q^i}},\quad
N_{E/F}(\alpha)=\prod_{i=0}^{n-1}{\alpha^{q^i}}=
\alpha^{\frac{q^n-1}{q-1}}\eqno{(1)}\]
These values can be read off from the minimal polynomial
\[f=\sum_{i=0}^n{f_iX^i} \in F[X]\] of $\alpha$ over $F$:
\[T_{E/F}(\alpha)=-f_{n-1},\quad N_{E/F}(\alpha)=(-1)^n f_0\]
The {\sl degree} of $\alpha$ over $F$ is the degree of the smallest
subfield of $E$ over $F$, which contains $\alpha$, i.e. the minimal
integer $d>0$ for which
\[\alpha^{q^d}=\alpha\]
If $\alpha$ has degree $d$ over $F$, the minimal polynomial 
$m_\alpha(X)$ of $\alpha$ then has degree $d$, too.

The multiplicative group $E^*$ of $E$ is cyclic of order $q^n-1$. A
generator of this group is called a {\sl primitive element} of $E$
and the minimal polynomial of such an element is called a
{\sl primitive polynomial}. Every nonzero element $\beta \in E$ can be
expressed as a power of $\alpha$:
\[\beta=\alpha^e\]
where $0\le e < q^n-1$ is uniquely determined. This kind of representing
the elements of $E$ is called {\sl cyclic group representation} of $E$.
The exponent $e$ is called the {\sl discrete logarithm} of $\beta$ to
base $\alpha$ denoted by $e=log_\alpha(\beta)$. Note, that exponentiaion
\[\mathbb{Z}\times E \rightarrow E:(e,\alpha)\mapsto \alpha^e\]
defined a $\mathbb{Z}$-module structure on the multiplicative group $E^*$.

Analogically one can define a module structure on the additive group
of $E$ in the following way.

Let $\circ : F[X] \times E \rightarrow E$ be defined by
\[\sum_i{a_iX^i} \circ \alpha := \sum_i{a_i\alpha^{q^i}}\]
Then we get for $\alpha \in E$, 
$g=\sum_i{g_iX^i}$, 
$f=\sum_j{f_jX^j} \in F[X]$,
\[g\circ(f\circ\alpha)=g\circ(\sum_j{f_j\alpha^{q^j}})=
\sum_i{g_i(\sum_j{f_j\alpha^{q^j}})^{q^i}}=
\sum_{i,j}{g_if_j\alpha^{q^{i+j}}}=
(g\cdot f)\circ\alpha\]
This proves that the operation $\circ$ defines an $F[X]$-module
structure on the additive group of $E$.

For $\alpha\in E$ the annihilator ideal 
$Ann_\alpha=\{f\in F[X] : f\circ\alpha=0\}$ of $\alpha$ is
generated by a single polynomial of $F[X]$, since $F[X]$ is a principle
ideal domain. We call the unique, monic generator of $Ann_\alpha$ the
{\sl linear associated order} of $\alpha$ over $F$, denoted by
${\rm Ord}_q(\alpha)$:
\[\{f\in F[X] : f\circ\alpha=0\}={\rm Ord}_q(\alpha)F[X]\]
Since $(X^n-1)\circ\alpha=\alpha^{q^n}-\alpha=0$ for all $\alpha\in E$,
${\rm Ord}_q(\alpha)$ divides $(X^n-1)$.

If ${\rm Ord}_q(\alpha)=(X^n-1)$ then there exists no polynomial of
degree less $n$ in F[X] annihilating $\alpha$, i.e. if $f\in F[X]$ is
of degree ${\rm deg}(f)<n$ and $f\circ\alpha=0$, then $f=0$. That is
\[\sum_{i=0}^{n-1}{f_i\alpha^{q^i}}=0~{\rm implicates}~f_i=0,
0 \le i < n\]
Therefore $\{\alpha,\alpha^q,\ldots,\alpha^{a^{n-1}}\}$ constitutes a
basis of $E$ over $F$, called a {\sl normal basis}.
Therefore, we call an element with linear associated order $(X^n-1)$
{\sl normal (over F)}. In this case every element $\beta\in E$ can be
expressed uniquely in the form
\[\beta=\sum_{i=0}^{n-1}{b_i\alpha^{q^i}}=b(X)\circ\alpha,\quad
b(X)=\sum_{i=0}^{n-1}{b_iX^i}\in F[X]\]
This kind of representing elements of $E$ is called
{\sl normal basis representation} of $E$. The polynomial $b(X)$ of
degree ${\rm deg}(b) < n$ is called the {\sl linear associated logarithm}
of $\beta$ to base $\alpha$ over $F$, denoted by 
$b(X)={\rm Log}_{q,\alpha}(\beta)$. It is uniquely determined modulo
${\rm Ord}_q(\alpha)$.

Again, with $\alpha$ all of its conjugates are normal over $F$, too.
Therefore we call an irreducible polynomial $f\in F[X]$ of degree $n$
normal over $F$, if the roots of $f$ in $GF(q^n)$ are linearly independent.

\section{Categories for finite field domains}
\label{section3}

Each domain in Axiom which represents a finite field is an object
in the category
\begin{verbatim}
   FFIELDC FiniteFieldCategory
\end{verbatim}

This category defines functions which are typical for finite sets as
{\tt order} or {\tt random} and those which are typical for finite
fields and do not depend on the ground field. An example is {\tt
primeFrobenius}, which implements the Frobenius automorphism with
respect to the prime field, or functions concerning the cyclic
multiplicative group structure as {\tt primitiveElement}, 
see section \ref{section4.3}

Functions which need to know the {\sl groundfield} $F$ are defined in
\begin{verbatim}
   FAXF(F)  FiniteAlgebraicExtensionField(F)
\end{verbatim}
These are functions considering the extension field as an algebra of
finite rank over $F$, 
see section \ref{section4.1}, and functions concerning the 
$F[X]$-module structure of the additive group of the extension field,
see section \ref{section4.2}

Every finite field can be considered as an extension field of each of
its subfields, in particular the prime field. In Axiom every finite
field constructor is implemented to belong to the category {\tt
FAXF(F)} for exactly one $F$. Constructors as {\tt FF} are considered
as extensions of its prime field, while others use the explicit given
ground field. Note that even a prime field is an extension of
itself. Mainly for technical reasons, but also for restrictions of the
present compiler, every finite field datatype in Axiom is an extension
of exactly {\sl one} subfield, usually called the {\sl ground
field}. Otherwise functions like {\tt extensionDegree} would depend on
the various ground fields and this would be not very convenient. For
possible enhancements in future releases, 
see section \ref{section11}. NOt that
this of course does not affect the ability to build arbitrary towers
of finite field extensions.

\section{General finite field functions}
\label{section4}

Let $E=GF(q^n)$ and $F=GF(q)$ be represented by {\tt E} of type 
{\tt FAXF(F)} and {\tt F} respectively. The characteristic of $E$ is 
given by {\tt characteristic()\$E}.

\subsection{$E$ as an algebra of rank $n$ over $F$}
\label{section4.1}

The degree $n$ of $E$ over $F$ is returned by {\tt extensionDegree()\$E}

The {\tt definingPolynomial()\$E} yields the polynomial $f\in F[X]$ by 
which the field extension $E$ over $F$ is defined. The element of {\tt E}
representing $(X {\rm\ mod\ } f(X))$, 
i.e. a root of the defining polynomial, is
returned by {\tt generator()\$E}.

A basis of $E$ over $F$ is yielded by calling {\tt basis()\$E}. In the
polynomial basis representation and cyclic group representation this is
the polynomial basis generated by {\tt generator()\$E}. In the normal basis
representation it is the normal basis generated by this element.

Let $\alpha=\sum_{i=0}^{n-1}{a_i\alpha_i}$, where $a_i$ are elements of
{\tt F} and $(\alpha_0,\ldots,\alpha_{n-1})=${\tt basis()\$E}.
The {\tt coordinates}$(\alpha)$ function computes the coordinate vector
$(a_0,\ldots,a_{n-1})$ of type {\tt Vector F} of $\alpha$ over $F$. 
The {\tt represents} function is the inverse of {\tt coordinates} and
yields $\alpha$, if applied to the vector $(a_0,\ldots,a_{n-1})$.

The implementation of these functions is straightforward in the polynomial
and normal basis representation. For the cyclic group representation see
section \ref{section7.3}

The functions {\tt degree}, {\tt trace}, and {\tt norm} applied to 
$\alpha \in E$ compute the degree, trace, and norm of $\alpha$ over $F$,
respectively. For their implementation see the sections according to the
special representations.

There are two ways of computing the minimal polynomial $m_\alpha(X)$ of
$\alpha\in E$ over $F$. The first method is to compute
\[m_\alpha(X)=\prod_{i=0}^{d-1}{(X-\alpha^{q^d})}\]
where $d$ denotes the degree of $\alpha$ over $F$. It unfortunately needs
$d(d-1)/2$ multiplications in $E$ and $(d-1)$ exponentiations by $q$.

The second method is to compute first a matrix $M\in F^{n\times(d+1)}$
whose $i$-th column is the coordinate vector of $\alpha^i$ w.r.t. an
arbitrary base of $E$ over $F$, for $0 \le i \le d$. Then there exists a
vector $b=(b_0,\ldots,b_d)^t$ in the nullspace of $M$, which can be 
computed. Hence, the polynomial
\[m_\alpha=X^d+\frac{1}{b^d}\sum_{i=0}^{d-1}{b_iX^i}\]
has the root $\alpha$ and is of correct degree, hence it is the minimal
polynomial. This method requires only about $(d-1)$ multiplications in $E$
and $O(d^2n)$ operations in $F$. Which approach is more time efficient
depends on the relation between the computation time of operations in $F$
and multiplications in $E$. Since multiplication in $E$ is cheap in a cyclic
group represention of $E$ 
(see section \ref{section7.1}) we use the first method there.
The second method was found to be more time efficient for the other 
representations.

Let $d$ be a divisor of $n$ and $L$ be a subfield of $E$ of degree $d$ over 
$F$. For the functions {\tt trace}, {\tt norm}, {\tt basis}, and 
{\tt minimalPolynomial} we offer a second version, which gets $d$ as an
additional parameter. {\tt basis}$(d)$ returns a basis of $L$ over $F$.
In the polynomial basis representation and cyclic group representation, this
is the polynomial basis generated by {\tt norm(primitiveElement()\$E,d)}.
In the normal basis representation it is the normal basis generated by
{\tt trace(generator()\$E,d)}. For $\alpha\in E$ {\tt trace}$(\alpha,d)$
and {\tt norm}$(\alpha,d)$ compute $T_{E/L}(\alpha)$ and $N_{E/L}(\alpha)$,
respectively, by 
\[T_{E/L}(\alpha)=\sum_{i=0}^{n/d-1}{\alpha^{q^{id}}}\quad{\rm and}\quad
N_{E/L}(\alpha)=\prod_{i=0}^{n/d-1}{\alpha^{q^{id}}}\]
Similarly we get the minimal polynomial of $\alpha$ over $L$ by
\[\prod_{i=0}^{m-1}{(X-\alpha^{q^{id}})}\]
where $m$ denotes the degree of $\alpha$ over $L$.

\subsection{The $F[X]$-module structure of $E$}
\label{section4.2}

In this section we discuss the three operations
{\tt linearAssociatedExp}, {\tt linearAssociatedLog}, and
{\tt linearAssociatedOrder}.

For $f=\sum_{i=0}^{n-1}{f_iX^i}\in F[X]$ and $\alpha\in E$ the function
{\tt linearAssociatedExp}$(\alpha,f)$ computes $f\circ\alpha$ in a 
straightforward way via
\[f\circ\alpha = \sum_{i=0}^{n-1}{f_i\alpha^{q^i}}\]

For $\alpha\in E$ we want to construct the monic polynomial 
{\tt Ord}$_q(\alpha)$ of least degree $d$ such that
\[{\rm Ord}_q(\alpha)\circ \alpha=0\]
This suggests to analyze the linear relations between the $q$-powers of
$\alpha$. Therefore we define $m\in F^{n\times n}$ to be the matrix whose
$i$-th column is the coordinate vector of $\alpha^{q^{i}}$ w.r.t. a basis
$B$ of $E$ over $F$, for $0 \le i \le n$. Its rank equals the degree $d$ of
{\tt Ord}$_q(\alpha)$. If $m$ has rank $n$ then $\alpha$ is normal in $E$
over $F$ and {\tt Ord}$_q(\alpha)=X^n-1$. Otherwise a vector
$b=(b_0,\ldots,b_{n-1})^t$ of the nullspace of $M$ is computed. It is easy
to see that we can choose $b_{d+1},b_{d+2},\ldots,b_{n-1}=0$ and finally
we get
\[{\rm Ord}_q(\alpha)=X^d+\frac{1}{b^d}\sum_{i=0}^{d-1}{b_iX^i}\]
which is computed by {\tt linearAssociatedOrder}$(\alpha)$ in Axiom.

For $\beta$ and $\alpha\in E$ {\tt linearAllocatedLog}$(\alpha,\beta)$
computed {\tt Log}$_{q,\alpha}(\beta)$, if it exists, i.e. the unique
polynomial $f=\sum_i{f_iX^i}\in F[X]$ of degree less then 
deg$({\rm Ord}_q(\alpha))$, for which $f\circ\alpha=\beta$.

As before we solve a system of linear equations
\[M\cdot(f_0,f_1,\ldots,f_{n-1})^t=b^t\]
where $M$ is as above and $b$ denotes the coordinate vector of $\beta$
w.r.t. the basis $B$. If there exists a solution satisfying
$f_d,f_{d+1},\ldots,f_{n-1}=0$ for the degree $d$ of ${\rm Ord}_q(\alpha)$,
then
\[{\rm Log}_{q,\alpha}(\beta)=\sum_{i=0}^{d-1}{f_iX^i}\]
otherwise {\tt linearAssociatedLog}$(\alpha,\beta)$ returned {\tt "failed"}.

The function {\tt linearAssociatedLog}$(\alpha)$ with only one argument
computed the linear associated logarithm of $\alpha$ to the base given by
{\tt normalElement()\$E}.

\subsection{The cyclic group $E^*$}
\label{section4.3}

The inheritance mechanisms of Axiom provide for each multiplicative
structure automatically an implementation of the exponentiation by the
defaulting repeating squaring algorithm. Finite Fields are an example
where it makes sense to overwrite this defaulting function. In all the
implementations we first reduce the exponent modulo $q^n-1$ and then do
repeated squaring or other algorithms.

To compute the multiplicative order $e:={\rm order}(\alpha)$ of
$\alpha\in E$ we proceed as follows: We start with $e:=q^n-1$. For every
prime $p$ of $(q^n-1)$ we divide $e$ by $p$: $e:=e/p$, as long as $e$
is divisible by $p$ and $\alpha^{e/p}=1$.

Note that this implementation requires factorizing the order $(q^n-1)$ of
the multiplicative group. Other functions as {\tt discreteLog} also need
these factors. Since factoring $(q^n-1)$ is time expensive, 
the factorization of $(q^n-1)$ is stored in a global variable
\begin{verbatim}
   facOfGroupSize: List Record(factor:Integer,exponent:Integer)
\end{verbatim}
in the respective domain. To make this information available to the category
for default implementations, we export the function
\begin{verbatim}
   factorsOfCyclicGroupSize()
\end{verbatim}
which returns the factorization. $(q^n-1)$ is factored only the first time
this function is called.

\subsection{Discrete logarithm}
\label{section4.4}

For the computation of discrete logarithms we implemented the
{\sl Silver-Pohlig-Hellman algorithm} combined with the 
{\sl baby steps-giant
steps} technique of Shank. A nice survey of discrete logarithm algorithms
included the algorithms we used is given by Odlyzko in \cite{Odly85}.

The Silver-Pohlig-Hellman algorithm breaks the problem of computing
discrete logarithms in a cyclic group of order $q^n-1$ down to computing
discrete logarithms in cyclic groups of order $p$, where $p$ ranges over
all prime factors of $q^n-1$. For a detailed description 
see \cite{Odly85} or \cite{Pohl78}.

To compute discrete logarithms in cyclic groups of prime order $p$ we
use Shank's algorithm implemented by
\begin{verbatim}
   shanksDiscLogAlgorithm:(M,M,NonNegativeInteger) ->
                           Union(NonNegativeInteger,"failed")
\end{verbatim}
in the package {\tt DiscreteLogarithmPackage(M)} and {\tt M} has to be
of type {\tt Join(Monoid,Finite)}, i.e. a finite multiplicative Monoid.
\begin{verbatim}
   shanksDiscLogAlgorithm(b,a,p)
\end{verbatim}
computes $e$ with $b^e=a$ assuming that $b$ and $a$ are elements of a
finite cyclic group of order $p$. If no such $e$ exists, it returns
{\tt "failed"}.

Here is a brief description of the algorithm: Let $\tilde{p}$ be an
integer close to $\sqrt{p}$. First we create a key-access table with
entries $(b^k,k)$. Then we look whether the table contains an entry
with key $a\cdot b^{-j\tilde{p}}$ and get $k$ with 
$a\cdot b^{-j\tilde{p}}=b^k$ for the smallest $j=0,1,\ldots,
\ceiling{p/\tilde{p}}-1$ or {\tt "failed"}. In the first case the result
is $e=k+j\tilde{p}$.

In the Silver-Pohlig-Hellman algorithm for a given base $\beta\in E$,
the first argument $b$ when calling {\tt shanksDiscLogAlgorithm(b,.,.)}
is for a fixed prime factor $p$ of $(q^n-1)$ always the same. To compute
logarithms to the base given by {\tt primitiveElement()\$E} efficiently,
the tables needed by Shanks algorithm are precomputed and stored in the
global variable
\begin{verbatim}
   discLogTable : Table(PI,Table(PI,NNI))
\end{verbatim}
in {\tt E}. It is initialized at the first call of the function
{\tt discreteLog}. Here {\tt PI} abbreviates {\tt PositiveInteger}
and {\tt NNI} abbreviates {\tt NonNegativeInteger}.

To implement the discrete logarithm function on category level in 
{\tt FiniteFieldCategory}, we have to make this data available to the
category. This is done by exporting the function
\begin{verbatim}
   tableForDiscreteLogarithm: Integer -> Table(PI,NNI)
\end{verbatim}
Called with a prime divisor $p$ of $q^n-1$ as argument, it returns a
table of size roughly $\tilde{p}\approx\sqrt{p}$, whose $k$-th entry for
$0 \le k < \tilde{p}$ is of the form
\begin{verbatim}
   [lookup(a**k),k) : Record(key:PI,entry:PI)
\end{verbatim}
where $a=\alpha^{(q^n-1)/p}$

We implemented two functions for discrete logarithms:
\begin{verbatim}
   discreteLog: E     -> NonNegativeInteger
   discreteLog: (E,E) -> Unioin(NonNegativeInteger,"failed")
\end{verbatim}
The first one computes discrete logarithms to the base
{\tt primitiveElement()\$E} using the precomputed tables. 
{\tt discreteLog(b,a)} computes ${\rm log}_b(a)$ if $a$ belongs
to the cyclic group generated by $b$ and fails otherwise. This function
does not use the table {\tt discLogTable}. No initialization of this
table is performed using the second function.

\subsection{Elements of maximal order}
\label{section4.5}

The functions
\begin{verbatim}
   primitiveElement()$E
   normalElement()$E
\end{verbatim}
yields a primitive element of $E$ and a generator of a normal basis of
$E$ over $F$, respectively. The first having maximal multiplicative order
$(q^n-1)$, the second maximal linear associated order $(X^n-1)$.

To compute elements of maximal order there exist algorithms which construct
elements of high order from elements of low order. For a unifying module
theoretic approach see L\"uneburg\cite{Lune87} chap.IV. For the construction of
primitive elements see \cite{Rybo89} where an algorithm is described which is 
originally given in \cite{Vars81}. 
Algorithms for finding generators of normal basis
are described in \cite{Gath90}, \cite{Lens91}, or \cite{Pinc89}.

Experiments have shown that in practice it is more efficient to simply run
through the field and test until an element of maximal order is found. 
However, not very many theoretical results are known on deterministic
search procedures. In some situations there are results depending on the
Extended Riemann Hypothesis, see \cite{Shou92}.

To aviod searching a second time for the same element, we store the results
after the first computation of such an element using the helper functions
\begin{verbatim}
   createPrimitiveElement()$E
   createNormalElement()$E
\end{verbatim}

The functions
\begin{verbatim}
  primitive?
  normal?
\end{verbatim}
check whether the multiplicative resp. linear associated order of an
element is maximal.

The primitivity of an $\alpha$ in $E$ is tested using the fact that
$\alpha$ is primitive in $E$ if and only if for all prime factors $p$
of $(q^n-1)$ holds:
\[\alpha^{(q^n-1)/p} \ne 1\]
see e.g. Theorem 2 of chap. I.X.1.3 in \cite{Lips81}.

For testing whether a given element $\alpha$ generates a normal basis
of $E$ over $F$ we use Theorem 2.39 of \cite{Lidl83}: $\alpha\in E$ generates a
normal basis of $E$ over $F$ if and only if $(X^n-1)$ and
$\sum_{i=0}^{n-1}{\alpha^{q^i}X^{n-1-i}}$ are relatively prime in $E[X]$

\subsection{Enumeration of elements of $E$}
\label{section4.6}

The number of elements of $E$ can be found out by calling {\tt size()\$E}.
The elements of {\tt E} are enumerated by 
\begin{verbatim}
   index: PositiveInteger -> E
\end{verbatim}
with inverse
\begin{verbatim}
   lookup: E -> PositiveInteger
\end{verbatim}

These functions implement a bijection between {\tt E} and the set of 
positive integers $\{1,2,\ldots,q^n\}$. They allow iterating over all
field elements. {\tt lookup} can be used to store field elements in a
variable of type {\tt PositiveInteger}. This is often less memory
expensive than storing the field element which may be represented in a
complicated way. For time efficiency reasons these functions have different
implementations according to the different representations. ALl of them have
in common that {\tt index}$(q^n)=0\$E$ and {\tt lookup}$(0\$E)=q^n$.

\subsection{Conversion between elements of the field and its groundfield}
\label{section4.7}

To check whether a given element $\alpha$, representation by {\tt a} in
the field {\tt E}, belongs to its groundfield $F$ use 
{\tt inGroundField?(a)}. If $\alpha$ belongs of $F$, datatype conversion
is provided by {\tt retract(a)}.

Embedding from {\tt F} to {\tt E} is done using {\tt coerce} abbreviated
by {\tt ::} in Axiom. If {\tt a} is the representation of $\alpha$ in 
{\tt F}, then {\tt (a::E)} is the element of {\tt E} representing $\alpha$.

All these functions depend on the representation used and are explained in
the sections according to the special representations.

\section{Prime field}
\label{section5}

Let $p$ be a prime number. Since $GF(p)\simeq\mathbb{Z}/p\mathbb{Z}$
in Axiom the internal representation of elements of the domains
\begin{verbatim}
   IPF(p)   InnerPrimeField(p)
   PF(p)    PrimeField(p)
\end{verbatim}
is {\tt IntegerMod(p)}, from which most functions are inherited. The
only difference between {\tt IPF} and {\tt PF} is, that in {\tt PF} it 
is checked whether the parameter $p$ is prime while this is not checked
in {\tt IPF}.

Many functions as {\tt trace} or {\tt inGroundField?} are trivial for
a prime field. For a human being there is no problem to consider a
prime field as an extension of degree 1 of itself. For recursions
depending e.g. on the extension degree and simply for completeness,
we have decided to make {\tt PrimeField} an {\tt FiniteAlgebraicExtensionField}
of itself. The trivial implementations include
\begin{verbatim}
   normalElement()    == 1
   inGroundField?(a)  == true
   generator()        == 1
\end{verbatim}

Since the value returned by {\tt generator()} should be a root of the
defining polynomial of the field extension, we had to code
{\tt definingPolynomial} to be $X-1$ and not e.g. $X$.

\subsection{Extension Constructors of Finite Fields}
\label{section5.1}

There are three choices to make when one wants to construct a
finite field as an extension of a ground field in Axiom.

The first choice is the type of representation. It can be remainder
classes of polynomials, 
see section \ref{section6}, exponents of a primitive element,
see section \ref{section7}, or a normal basis representation, 
see section \ref{section8}. The part of the 
abbreviation of the corresponding domain constructors are
{\tt FF}, {\tt FFCG}, and {\tt FFNB}, respectively.

Secondly, we have to decide which ground field we choose, either the
prime field or any other subfield. In the first case the first parameter
of all domain constructors is just a prime number and one has to use
the names above. In the other case the first parameter is the domain
constructed recursively to represent the chosen subfield.

The second parameter governs the extension. All constructions depend on an
irreducible polynomial, whose degree is the extension degree and has, if
necessary, additional properties. If one doesn't care about this polynomial,
one has to give the degree as the second parameter and the polynomial will
be chosen approximately by Axiom. In the case where we define the prime
field by supplying a prime number, this is the only choice. In the other
case one has to append the letter {\tt X} to the receive the domain 
constructors {\tt FFX}, {\tt FFCGX}, and {\tt FFNBX}, respectively. If
one wants to supply one's favorite polynomial as the second parameter we
have to substitute the letter {\tt X} by {\tt P}.

Here are a few datatype constructions for these nine possibilities:
{\tt FF(2,10)} implements an extension of the prime field {\tt PF 2}
of degree 10. Axiom chooses an irreducible polynomial of degree 10 for
this polynomial basis representation.

{\tt FFNBX(FFCGP(3,f),5)} implements an extension of the field with $3^n$
elements, represented as exponents of a primitive element, where $f$ is a
primitive polynomial of degree $n$. The extension of degree 5 is realized
by Axiom by choosing a normal polynomial of degree 5 with coefficients in
{\tt FFCGP(3,f)}.

As overloading of constructor names is not supported by the current
compiler, we had to create all these different names as explained above.
As soon as the new compiler will support this we may consider to unify
these domain domain names, see section \ref{section11}.

\section{Polynomial basis representation}
\label{section6}

Let $E=GF(q^n)$ be an extension of degree $n$ over $F=GF(q)$. Then
\[E\simeq F[X]/(f)\]
where $f\in F[X]$ is an arbitrary monic irreducible polynomial 
of degree $n$.
If $\alpha$ is a root of $f$ in $E$, then $\{1,\alpha,\ldots,\alpha^{n-1}\}$
constitutes a basis of $E$ over $F$ and we can write all elements
$\beta\in E$ uniquely in the form:
\[\beta=\sum_{i=0}^{n-1}{b_i\alpha^i},\quad b_i\in F\]

This kind of representation is used in the domains
\begin{verbatim}
   FFP   FiniteFieldExtensionByPolynomial
   FFX   FiniteFieldExtension
   IFF   InnerFiniteField
   FF    FiniteField
\end{verbatim}

The only difference between these domains are the different natures of
their parameterization, 
see section \ref{section5.1}. The datatype {\tt InnerFiniteField}
extends the prime field {\tt InnerPrimeField}, 
see section \ref{section5}. Let {\tt F}
be the domain representing $F$ and {\tt E:=FFP(F,f)} the extension of $F$
defined by $f$.

For the internal representation of elements of $E$ we use polynomials
modulo $f$. This structure is in Axiom implemented by the domain
\begin{verbatim}
   SAE(F,SUP(F),f)  SimpleAlgebraicExtension(F,SUP(F),f)
\end{verbatim}
Here {\tt SUP(F)} abbreviates {\tt SparseUnivariatePolynomial(F)} a
domain representing polynomials over $F$. Most arithmetic operations are
inherited from this domain. There are only a few functions which have a
special implementation.

The imbedding of {\tt F} in {\tt E} is obvious:
$\sum_{i=0}^{n-1}{a_i\alpha^i}$ is in $F$ if and only if
$a_1,a_2,\ldots,a_{n-1}=0$. Using this, the functions
\begin{verbatim}
   retract:       E -> F
   coerce:        F -> E
   inGroundfield? E -> Boolean
\end{verbatim}
are implemented in the obvious way.

To check whether $\beta$ is normal in $E$ over $F$ we proceed as follows.
Let $M \in F^{n\times n}$ be the matrix, whose $i$-th column is the 
coordinate vector of $\beta^{q^i}$ with respect to the polynomial basis
for $0 \le i < n$. The element $\beta$ is normal if and only if the rank
of $M$ equals $n$.

The coordinates of $\beta\alpha^i$ collected in a matrix similar as before
give the {\sl regular matrix representation} 
(see e.g. chap.7.3 in \cite{Jaco85}) of
$\beta$. The functions {\tt trace} and {\tt norm} compute the trace and
norm of $\beta$ over $F$ by computing the trace and determinant of this
matrix, respectively.

\section{Cyclic group representation}
\label{section7}

In this section we make use of the fact, that the multiplicative group of 
a finite Field $E$ with $q^n$ Elements is cyclic of order $q^n-1$. Therefore
it is isomorphic to $\mathbb{Z}/(q^n-1)\mathbb{Z}$. Once a primitive
element $\alpha$ of $E$ is fixed (i.e. a generator of the multiplicative
group), ever nonzero element $\beta$ of $E$ is uniquely determined by
its discrete logarithm $e$ to $\alpha$, i.e. the element 
$0 \le e \le q^n-1$ with $\alpha^e=\beta$.

In the three domains
\begin{verbatim}
   FFCGP   FiniteFieldCyclicGroupExtensionByPolynomial
   FFCGX   FiniteFieldCyclicGroupExtension
   FFCG    FiniteFieldCyclicGroup
\end{verbatim}
the nonzero field elements are represented by powers of a fixed primitive
element. Let {\tt F} be a finite field domain representing $F$, $E$ the
extension of $F$ defined by the monic, irreducible polynomial $f(x) \in F[X]$
with root $\alpha \in E$. Let $\alpha$ be primitive in $E$ and
{\tt E:=FFCGP(F,f)} be the domain representing $E$.

As the fixed primitive element used for the representation, we take the root
$\alpha$ of $f$. It is returned by calling {\tt generator()\$E}, which is 
equal to the function {\tt primitiveElement()\$E} in this representation.

The aim of a cyclic group representation of finite fields is to offer
a very fast field arithmetic. All operation concerning the multiplicative
structure of the field are quite easy to compute. To have a quick addition
one has to store a Zech (Jacobi) logarithm table in memory, see setion 7.2.
This table is of size about $q^n/2$. For efficiency reasons we also want
to use {\tt SmallInteger} and not {\tt Integer} as the internal 
representation of the field elements. Therefore we restricted ourself
to a field size of maximal $2^{20}$ elements:
\begin{verbatim}
   if sizeFF > 2**20 then
     error "field too large for cyclic group representation"
\end{verbatim}

A nonzero element $\beta\in E$ is represented by the unique
$n \in \{0,1,\ldots,q^n-2\}$ of type {\tt SmallInteger} with
$\alpha^n=\beta$ and $0\in E$ is represented by the {\tt SmallInteger} $-1$.

\subsection{Operations of multiplicative nature}
\label{section7.1}

The implementation of the operations concerning the multiplicative group
is very easy. Since $\alpha^n\cdot \alpha^m=\alpha^{n+m}$, multiplication
of nonzero elements becomes a {\tt SmallInteger} addition modulo $q^n-1$.
Similarly the exponentiation of field elements and the norm function are
done by a modular {\tt SmallInteger} multiplication. Inversion is nothing
more than changing the sign of the representing {\tt SmallInteger} module
$q^n-1$. Discrete logarithms to base $\alpha$ can be read off directly 
from the representation and for computing the discrete logarithm to a
arbitrary base one has to perform the extended euclidean algorithm in
$\mathbb{Z}$.

If we want to compute the discrete logarithms $d$ of $\gamma=\alpha^m$
to base $\beta=\alpha^b$ we have to solve:
\[(\alpha^b)^d=\alpha^m\]

This is solvable if and only if $m$ is divisible by 
\[g:={\rm gcd}(b,q^n-1)=rb+s(q^n-1)\]. In this case we set
\[d=\frac{rm}{g}{\rm mod}(q^n-1)\]

Computing the multiplicative order of elements is done by
\[{\rm ord}(\alpha^e)=\frac{{\rm ord}(\alpha)}{{\rm gcd}(e,{\rm ord}(\alpha))}
=\frac{q^n-1}{{\rm gcd}(e,q^n-1)}\]

Therefore an element $\beta$ is primitive in $E$ if and only if its
representation is relatively prime to $q^n-1$.

\subsection{Addition and Zech logarithm}
\label{section7.2}

Addition is performed via the Zech (or Jacobi) logarithm $Z(k)$ which is
defined by
\[\alpha^{Z(k)}=\alpha^k+1\]

In the domain {\tt E} the Zech logarithm array is stored in the global
variable
\begin{verbatim}
   zechlog : PrimitiveArray(SmallInteger)
\end{verbatim}
of the datatype of length $(q^n+1)/2$. Its $k$-th entry
corresponds to $Z(k)$. $Z(k)$ is undefined, if $\alpha^k+1=0$. 
This exception appears in characteristic 2 if $k=0$ and in odd
characteristic if $k=(q^n-1)/2$. To indicate this we define
{\tt zechlog.k=-1} in these cases. Now the sum of $\alpha^i$ and
$\alpha^j$ is computed in the following way:

Let $k$ be the smaller one of $\{(i-j){\rm\ mod\ }(q^n-1)$,
$(j-i){\rm\ mod\ }(q^n-1)\}$. Notice that $k\le(q^n-1)/2$. Then
\[\alpha^i+\alpha^j:=
\begin{cases}
0 & {\rm if}~Z(k)~{\rm undefined}\\
\alpha^{i+Z(k)} & {\rm if}~k=(j-i)~{\rm mod}~(q^n-1)\\
\alpha^{j+Z(k)} & {\rm if}~k=(j-i)~{\rm mod}~(q^n-1)
\end{cases}
\]

In addition to some {\tt SmallInteger} operatoins there is only
one access to the Jacobi logarithm array. Therefore addition is
very fast too.

Since $-1=\alpha^{(q^n-1)/2}$ in odd characteristic, $-\alpha^n$
can be computed by
\[-\alpha^n:=
\begin{cases}
\alpha^n & {\rm if\ char}(E)=2\\
\alpha^{n+(q^n-1)/2} & {\rm otherwise}
\end{cases}
\]

The Jacobi logarithm array is initialized at the first time when
it is needed. This procedure may last some time. The initialization
is done by calling the function {\tt createZechTable(f)} parameterized
with the defining polynomial of the field in the package
{\tt FiniteFieldFunctions(F)}. The user gets access to this table
by calling {\tt getZechTable()\$E}.

In K. Huber\cite{Hube90} explains how to reduce the size of the table needed
to compute all Jacobi logarithms. These observations are useful for
high extension degree, but the times for addition are increased,
therefore these ideas were not implemented.

To check if a given element belongs to $F$ is quite easy. It depends
on whether the representation $e$ of $\alpha^e$ is divisible by
$(q^n-1)/(q-1)$ or not. If $e=k(q^n-1)/(q-1)$ we have
\[(\alpha^e)^{q-1}=\alpha^{k(q^n-1)}=1\]
and therefore $\alpha^e$ belongs to $F$. The degree of $\alpha^e$ 
over $F$ is given by the minimal integer $d>0$ for which
$eq^d\equiv e{\rm\ mod\ }(q^n-1)$. The trace and the norm function
are computed directly using (1) on page 6.

\subsection{Time expensive operations}
\label{section7.3}

But there remain some operations, which are quite time expensive.
That are those operations, which change the representation of the
field elements:
\begin{verbatim}
   coerce:      F -> E
   retract:     E -> F
   represents:  Vector F -> E
   coordinates: E -> Vector F
\end{verbatim}

Let $\beta=N_{E/F}(\alpha)$ be the norm of $\alpha$ over $F$. 
Since $\beta$ is primitive in $F$, every nonzero element $\gamma$
of $F$ can be expressed by a power of $\beta$. We get
\[\gamma=\beta^e=\alpha^{e\frac{q^n-1}{q-1}}\]
for a suitable value $0 \le e \le q-1$. This $\beta$ is stored
in the global variable {\tt primEltGF} in {\tt E}. The function
{\tt retract} applied to $(\alpha^e)$ first checks if the element
$\alpha^e$ belongs to $F$. In this case $e$ is divisible by
$(q^n-1)/(q-1)$ and {\tt retract} can raise {\tt primEltGF} in 
{\tt F} to the power $e(q-1)/(q^n-1)$.

{\tt coerce} applied to $\gamma\in F$, $\gamma \ne 0$, computes
the discrete logarithm of $\gamma$ to the base {\tt primEltGF} in
{\tt F} and multiples this value by $(q^n-1)/(q-1)$ to get the
desired representation of $\gamma$ in {\tt E}.

{\tt coordinates} applied to $(\alpha^e)$ raises the residue 
class $(X{\rm\ mod\ }f)$ to the power $e$. This is performed in a
{\tt SimpleAlgebraicExtension} of the {\tt F} by {\tt f}. The
returned vector is the coordinate vector of $\alpha^e$ to the
polynomial basis generated by $\alpha$.

{\tt represents} considers the given vector as coordinate vector of
an element $\beta$ in\\
{\tt FiniteFieldExtensionByPolynomial(F,f)}
and computes its discrete logarithm to base $\alpha$ in that domain.
This logarithm is the representation of $\beta$ in {\tt E}.

\section{Normal basis representation}
\label{section8}

Let $E=GF(q^n)$ be an extension of degree $n$ over the finite
field $F=GF(q)$ and $f\in F[X]$ be the polynomial which defines
the extension. Assume further that the roots 
$\{\alpha,\alpha^q,\ldots,\alpha^{q^{n-1}}\}$ of $f$ in $E$ are
linearly independent over $F$. Then $\alpha$ is normal in $E$
over $F$ and every element of $E$ can be expressed in the form
\[\beta=\sum_{i=0}^{n-1}{b_i\alpha^{q^i}}\] 
with $b_i\in F$. This kind of representation is used in the domains
\begin{verbatim}
   FFNBP   FiniteFieldNormalBasisExtensionByPolynomial
   FFNBX   FiniteFieldNormalBasisExtension
   FFNB    FiniteFieldNormalBasis
\end{verbatim}

Let {\tt F} be a finite field domain representing $F$ and
{\tt E:=FFNBP(F,f)} the normal basis extension of $F$ by $f$.

We get the root $\alpha$ of $f$ in $E$ by calling {\tt generator()\$E}
which in this representation is equal to {\tt normalElement()\$E}.

The internal representation of the elements of {\tt E} is
{\tt Vector F}. The element $\beta=\sum_{i=0}^{n-1}{b_i\alpha^{q^i}}$
is represented by the coordinate vector
$(b_0,b_1,\ldots,b_{n-1})$ of $\beta$ w.r.t. the normal basis
generated by $\alpha$ and computed by {\tt coordinates}$(\beta)$.
The normal basis is returned by {\tt basis()\$E}. In the sequel we
identify coordinate vectors $(b_0,b_1,\ldots,b_{n-1})$
representing $\sum_{i=0}^{n-1}{b_i\alpha^{q^i}}$ with the
corresponding polynomial
$b=\sum_{i=0}^{n-1}{b_iX^i}\in F[X]/(X^n-1)$, since
\[b\circ\alpha=\sum_{i=0}^{n-1}{b_i\alpha^{q^i}}\]

The lengthy code for the arithmetic is shared by the three different
versions of normal basis representations and the package
{\tt FiniteFieldFunctions}. Hence, we decide to have a 
parameterized package
\begin{verbatim}
   INBFF   InnerNormalBasisFieldFunctions(F)
\end{verbatim}
where most of the arithmetic in {\tt E} is performed.

\subsection{Operations of additive nature}
\label{section8.1}

All field functions concerning the cyclic $F[X]$-module structure
of the additive group of $E$ are very easy to implement and to 
compute. Since
\[\beta^q=\sum_{i=0}^{n-1}{b_{(i-1){\rm\ mod\ }n}\alpha^{q^i}}\]
the Frobenius automorphism becomes a simple cyclic shift of the
coordinate vector. The linear associated logarithm of $\beta$ to
base $\alpha$ can be directly read off from the representation of
$\beta$
\[Log_\alpha(\beta)=\sum_{i=0}^{n-1}{b_iX^i=b}\]
To compute the linear associated logarithm $a$ of $\beta$ to
another logarithm base $\gamma=c\circ\alpha$, one has to perform
an extended euclidean algorithm in $F[X]$:
\[a\circ\gamma=\beta \Longleftrightarrow (ac)\circ\alpha=b\circ\alpha\]
This is solvable if and only if $b$ is divisible by 
$g:={\rm gcd}(c,X^n-1)=rc+s(x^n-1)$. In this case we get
\[a=\frac{rb}{g}{\rm\ mod\ }(X^n-1)\]

The operation $\circ$ becomes a modular polynomial multiplication
\[h\circ\beta=(hb)\circ\alpha\]
for $h\in F[X]$. The linear associated order of $\beta$ can be
computed using
\[{\rm Ord}_q(b\circ\alpha)=
\frac{{\rm Ord}_q(\alpha)}{{\rm gcd}(b,{\rm Ord}_q(\alpha))}=
\frac{(X^n-1)}{{\rm gcd}(b,X^n-1)}\]
Therefore $\beta$ is normal in $E$ over $F$ if and only if
gcd$(b,X^n-1)=1$, which is quite easy to check. The degree of
$\beta$ over $F$ is given by the minimal integer $d>0$ which satisfies
\[bX^d\equiv b{\rm\ mod\ }(X^n-1)\]

The embedding of {\tt F} into {\tt E} is determined by the trace of
$\alpha$: Let $a=T_{E/F}(\alpha)$, then 
$1-T_{E/F}(a^{-1}\alpha)=\sum_{i=0}^{n-1}{a^{-1}\alpha^{q^i}}$
and we get for $d\in F$
\[d=\sum_{i=0}^{n-1}{(a^{-1}d)\alpha^{q^i}}\]
which gives as representation of $d$ in {\tt E} the vector 
consisting of equal entries $a^{-1}d$. Since the value
$T_{E/F}(\alpha)$ is needed quite often, it is stored in the global
variable {\tt traceAlpha} in {\tt E}. The trace $T_{E/F}(\beta)$ of
$\beta=\sum_{i=0}^{n-1}{b_i\alpha^{q^i}}$ is simply computed by
\[T_{E/F}(\beta)=\sum_{i,j=1}^{n-1}{b_i\alpha^{q^{i+j}}}=
\sum_{j=0}^{n-1}{(\sum_{i=0}^{n-1}{b_i})\alpha^{q^i}}\]
Traces onto intermediate fields of $F \le E$ are computed in a
similar fashion.

\subsection{Multiplication and normal basis complexity}
\label{section8.2}

In the contrary to the {\sl additive} functions, the operations
concerning the multiplicative structure of the field are more
difficult to compute. Actually the multiplication of field elements
is somewhat complicated and hence slow.

To multiply field elements we use the representing matrix
$M_\alpha=(m_{i,j})\in F^{n\times n}$ of the left multiplication
by $\alpha$ w.r.t. the distinguished normal basis, which is called
{\sl multiplication matrix} in Geiselmann/Gollmann \cite{Geis89}:
\[\alpha\alpha^{q^i}=\sum_{j=0}^{n-1}{m_{i,j}\alpha^{q^j}}\]

Knowing this matrix the product of 
$\beta=\sum_{i=0}^{n-1}{b_i\alpha^{q^i}}$ and
$\gamma=\sum_{j=0}^{n-1}{c_j\alpha^{q^j}}$ is given by
\[\beta\gamma=\sum_{i,j=0}^{n-1}{b_ic_j(\alpha^{q^{i-j}+1})^{q^j}}=
\sum_{i,j,k=0}^{n-1}{b_ic_jm_{i-j,k-j}\alpha^{q^k}}\]
with indices module $n$. This shows immediately that multiplication
in this representation needs $O(n^3)$ $F$-operations.

Recently there has been much interest in so called {\sl low
complexity} and {\sl optimal} normal bases: By choosing the normal
element $\alpha\in E$ carefully one tries to minimize the number of
nonzero entries in $M_\alpha$, which is called the {\sl complexity}
of the normal basis. This obviously reduces the multiplication time. 
In the best case one can reduce the number of entries to $2n-1$.
For special pairs $(q,n)$ a direct construction of a normal base
of low complexity $O(kn)$, $k\ll n$, is possible (see Wassermann \cite{Wass89},
Beth/Geiselmann/Meyer\cite{Beth91}, Mullin/Onyszchuk/Vanstone\cite{Mull88}
or Ash/Blake/Vanstone\cite{Ashx89}). The problem of efficiently computing
the minimal normal basis complexity or even a generator of such
a base for given $(q,n)$ is unsolved.

The algorithm described by A. Wassermann in \cite{Wass89} is implemented
in the function
\begin{verbatim}
   createLowComplexityNormalBasis(n)
\end{verbatim}
in the package {\tt FiniteFieldFunctions(F)}. If for the given
$(q,n)$ a direct construction of a low complexity normal basis
is possible, the algorithm computes the multiplication matrix of
this base and the function returns this matrix in form of a
variable of type 
{\tt Vector List Record(value:F,index:SmallInteger)} (see below).
If such a construction is not possible for $(q,n)$ the function
\begin{verbatim}
   createNormalPoly(n)$FiniteFieldPolynomialPackage(F)
\end{verbatim}
is called to produce a normal polynomial of degree $n$. To have the
nice embedding $d\mapsto(d,d,\ldots,d)$ of {\tt F} in {\tt E},
the computed normal basis has in both cases the property that its
generator has trace 1 over $F$. The constructors {\tt FFNBX} and
{\tt FFNB} makes use of this function and use, if possible, 
automatically a low complexity normal basis.

If we would store the multiplication Matrix $M_\alpha$ in a variable
of type {\tt Matrix F}, everytime we multiply we would have to inspect
all $n^2$ entries of $M_\alpha$. In this case a low complexity basis
would hardly speed up the multiplication time.

This is why we store $M_\alpha$ in the domain $E$ in a global
variable of the form
\begin{verbatim}
   multTable : Vector List Record(value:F,index:SmallInteger)
\end{verbatim}

The entry $m_{i,j}$ of $M_\alpha$ corresponds to the element of
{\tt multTable.i} with {\tt index} $j-1$ and {\tt value} $m_{i,j}$.
Of course only the nonzero entries of $M_\alpha$ are stored in
{\tt multTable}. When multiplying now we are inspecting only the
nonzero entries of $M_\alpha$ and get time advantages using bases
of low complexity.

The first time when the multiplication matrix $M_\alpha$ is needed,
it is initialized by an automatic call of the function
{\tt createMultiplicationTable(f)} in the package 
{\tt FinitFieldFunctions(F)}.

The user has access to the multiplication matrix of the field by
\begin{verbatim}
   getMultiplicationTable: () ->
        Vector List Record(value:F,index:SmallInteger)
     ++ getMultiplicationTable() returns the multiplication
     ++ table for the normal basis of the field
   getMultiplicationMatrix: () -> Matrix F
     ++ getMultiplicationMatrix() returns the multiplication
     ++ table in the form of a matrix
\end{verbatim}

The complexity of the normal basis can be found out by calling
\begin{verbatim}
   sizeMultiplication:() -> NonNegativeInteger
\end{verbatim}

\subsection{Norm and multiplicative inverse}
\label{section8.3}

The functions {\tt norm} and {\tt inv} are the power functions with
exponents $(q^n-1)/(q-1)$ and $(q^n-2)$, respectively. We do not use
the default repeated squaring algorithm, as we can do better: The
algorithm due to Itoh and Tsujii\cite{Itoh88} uses a clever partitioning of
these special exponents and the following help function {\tt expPot}.
It computes
\[expPot(\beta,k,d)=\prod_{i=0}^{k-1}{\beta^{q^{id}}}\]
for $\beta\in E$ and integers $k,d>0$ is computed by the (slightly
simplified) algorithm
\begin{verbatim}
   expPot(beta,k,d) ==
     e:Integer:=0
     gamma:E:=1
     for i in 0..length(k) repeat
       if bit?(k,i) then gamma:=gamma * beta**(q**e); e:=e+d
       beta:=beta * beta**(q**d); d:=2*d
     return(gamma)
\end{verbatim}
where {\tt length(k)} denotes the number of bits of $k$, i.e.
$\ceiling{log_2(k+1)}$, and {\tt bit?(k,i)} tests whether the
$i$-th bit of $k$, binary represented, is set or not. The average
number of $E$-multiplications of this algorithm is about 
$3/2\ceiling{log_2(k)}$

Let $d$ be a divisor of $n$ and $K$ and extension of degree $d$
over $F$. With the above algorithm we can compute the norm of
$\beta\in E$ over $K$ by
\[N_{E/K}(\beta)={\rm expPot}(\beta,n/d,d)\]
using about $3/2\ceiling{log_2(n/d)}$ multiplications in $E$.

For computing the inverse $\beta^{-1}=\beta^{q^n-2}$ of $\beta\in E$
notice that
\[q^n-2=(q-2)(\frac{q^n-1}{q-1})+q(\frac{q^{n-1}-1}{q-1})\]
therefore
\[\beta^{-1}=N_{E/F}(\beta)^{-1}\cdot(\beta^{\frac{q^{m-1}-1}{q-1}})^q\]

Now we get $\beta^{-1}$ by computing first
\[\gamma={\rm expPot}(\beta,n-1,1)^q=(\beta^{\frac{q^{m-1}-1}{q-1}})^q\]
and then $\beta^{-1}=(\beta\gamma)^{-1}\cdot\gamma$. Notice that the
inversion of $\beta\gamma$ is performed in $F$. Altogether we used
$3/2\ceiling{log_2(n)}+2$ multiplications in $E$.

\subsection{Exponentiation}
\label{section8.4}

Next we show how we have implemented the function
\[(\beta,e)\rightarrow \beta^e\]

For a $1 \le k < n$ we can write
\[\beta^e=\prod_i{(\beta^{e_i})^{q^{ki}}}\]
if $e=\sum_i{e_iq^{ik}}$ is the $q^k$-adic expansion of $e$.

An obvious implementation of this formula first of all has to 
initialize the array $[\beta,\beta^2,\ldots,\beta^{q^k-1}]$.
This costs $(q-1)q^{k-1}-1$ (expensive) field multiplications.
Taking $q^{ki}$-powers is cheap while multiplying the results
together costs another $\ceiling{(log_q(e)/k)}-1$ field
multiplications, altogether this algorithm requires
\[M(q,k,e):=(q-1)q^{k-1}+\ceiling{\frac{log_q(e)}{k}}-2\]
multiplications.

Depending on $k$ there is a tradeoff between slow multiplication
and fast powering. Therefore we adaptively choose a good $k$
depending on $e$ and $q$ to minimize the number of multiplications.

The computation of such 
$k\sim log_q log_q(e)-log_q log_q log_q(e)$ is performed using
exclusively {\tt SmallInteger} arithmetic to minimize the decision
time. It is supported by the two global variables
\begin{verbatim}
   logq:List SmallInteger
   expTable:List List SmallInteger
\end{verbatim}
which contain some precomputed auxiliary values.

Then the actual number of multiplications is compared with the
number $3/2\ceiling{log_2(e)}$ of multiplications needed by the
standard repeated squaring algorithm and the better method is
chosen.

The ideas of this divide and conquer algorithm are due to Stinson,
see \cite{Stin90}, for the case $q=2$.

\section{Homomorphisms between finite fields}
\label{section9}

Changing an object from one finite field to another can be necessary
in three cases. The first case is that we have two different defining
polynomials for the same field in the same type of representations. In
order to consider one object in the other data type we have to
implement a field isomorphism. A generalization thereof is when one
field is isomorphic to a subfield of another field in the same type of
representation. The most complicated case is when in addition to the
last situation we also change the representation.

These data type conversions - called {\sl coercions} in Axiom, and
hence are under the control of the interpreter - are realized by the
package 
\begin{verbatim}
   FFHOM FiniteFieldHomomorphisms 
\end{verbatim}

It is parameterized by three parameters: a source field $K_1$
represented by {\tt K1}, a target field $K_2$ represented by {\tt K2}
and a common groundfield $F$ of $K_1$ and $K_2$, represented by {\tt
F}. Note, that due to the symmetry of the provided functions, the
order of the parameters can either be $(K1,F,K2)$ or $(K2,F,K1)$. The
order comes from arranging the situation in a lattice.

However, this package cannot be used for the general situation as this
settings suggests. We had to restrict ourselves to the case where both
{\tt K1} and {\tt K2} are realized as simple extensions of {\tt F},
i.e. of type 
\begin{verbatim} 
   FiniteAlgebraicExtensionField(F)
\end{verbatim}

To implement the general case also, it would be necessary to have a
function 
\begin{verbatim} 
   groundfield: () -> FiniteFieldCategory
\end{verbatim}

Its result could be used for package calling functions of that
subfield. Using such a function it would be possible to build a
recursive coercion algorithm between different towers of finite field
extensions. As the old compiler does not support functions whose
values are domains, this idea will be possible when the new compiler
will be available.

The source field and the destination field may appear in arbitrary
order in the parametrization of {\tt FFHOM}, since {\tt FFHOM}
supports coercions in both directions: 
\begin{verbatim} 
   coerce: K1 -> K2 
   coerce: K2 -> K1 
\end{verbatim}

Restricted to $K_1 \cap K_2$ these two mappings are inverses of each
other, i.e. for $\alpha \in K_1 \cap K_2$ and $a$ being a
representation of $\alpha$ in {\tt K1} or in {\tt K2} holds:
\begin{verbatim} 
   coerce(coerce(a)$FFHOM(K1,F,K2))$FFHOM(K1,F,K2)=a
\end{verbatim}

To be independent of the ordering of the arguments we have ordered the
fields inside the package by comparing the defining polynomials of the
fields lexicographically using the local function {\tt compare}.

To explain the details of the implementation let $\beta\in K_1$ and
{\tt b} be the representation of $\beta$ in {\tt K1}. We have to
distinguish between some cases:

First check whether $\beta$ is in $F$. In this case 
\begin{verbatim}
   retract(b)@F$K1::K2 
\end{verbatim} 
is used.

The next case is that {\tt K1} and {\tt K2} are constructed using the
same defining polynomial $f$. If furthermore {\tt K1} and {\tt K2} are
represented in the same say, the elements of {\tt K1} and {\tt K2} are
represented completely identical. Therefore the coercion may be
performed by 
\begin{verbatim} 
   b pretend K2 
\end{verbatim}

Now assume that one of {\tt K1} and {\tt K2} is represented using
cyclic groups and the other one represented by a polynomial basis.

If {\tt K1} is cyclically represented, we coerce by 
\begin{verbatim}
   represents(coordinates(b)$K1)$K2 
\end{verbatim} 
and vice versa, if {\tt K2} is cyclically represented.

All remaining cases are treated in the same way which we explain
now. Denote by {\tt degree1} and {\tt degree2} the extension degrees
of $K_1$ and $K_2$ over $F$, respectively. The first time a coercion
from {\tt K1} into {\tt K2}, or vice versa, is called, two conversion
matrices stored in global variables 
\begin{verbatim} 
   conMat1to2:Matrix F 
     -- conversion Matrix for the conversion direction K1 -> K2
   conMat2to1:Matrix F 
     -- conversion Matrix for the conversion direction K2 -> K1 
\end{verbatim} 
in the package are initialized. Once these
matrices are initialized, the coercion is performed by
\begin{verbatim} 
   represents(conMat1to2 * coordinates(b)$K1)$K2
\end{verbatim}

Notice, that we do not have to care about the coercion between cyclic
representation and polynomial representation as above, since this step
is implicitly performed by the function calls to {\tt represents} and
{\tt coordinates} (see section \ref{section7.3}).

The rest of this section describes the initialization of the conversion
matrices.

\subsection{Basis change between normal and polynomial basis
representation}
\label{section9.1}

We first consider the case of equal defining polynomials. We can
assume without loss of
generality that $E=K_1=K_2$ and the root $\alpha\in E$ of this
polynomial both generates a polynomial and a normal basis. To
convert $\beta=\sum_{i=0}^{n-1}{b_i\alpha^i}$ into
$\beta=\sum_{j=0}^{n-1}{c_j\alpha^{q^j}}$ we have to set up the
basis change matrix $M=(m_{i,j})$, defined by
\[\alpha^{q^j}=\sum_{i=0}^{n-1}{m_{i,j}\alpha^i,\quad 0\le j<n}\]
Then we have
\[\beta=\sum_{j=0}^{n-1}{c_j\alpha^{q^j}}=
\sum_{i,j=0}^{n-1}{c_jm_{i,j}\alpha^i}\]
consequently
\[b_i=\sum_{j=0}^{n-1}{c_jm_{i,j}}\]

Therefore $M$ can be used to change between polynomial representation
and normal representation by
\[M(c_0,c_1,\ldots,c_{n-1})^t=(b_0,b_1,\ldots,b_{n-1})^t\]
and
\[M^{-1}(b_0,b_1,\ldots,b_{n-1})^t=(c_0,c_1,\ldots,c_{n-1})^t\]
where the $t$ denotes the transposition of the vector. The matrix
$M$ is computed efficiently by using the function
{\tt reducedQPowers} in the package\\
{\tt FiniteFieldPolynomialPackage(F)}.

\subsection{Conversion between different extensions}
\label{section9.2}

Now suppose that {\tt K1} and {\tt K2} are built using different
defining polynomials {\tt defpol1} and {\tt defpol2}, respectively.
As noticed earlier we have to order these polynomials to get the
same homomorphisms when calling {\tt FFHOM} with parameters
{\tt K1} and {\tt K2} swapped.

Without loss of generality 
let {\tt defpol1} be the lexicographical smaller polynomial
of {\tt defpol1} and {\tt defpol2}. In the other case '1' and '2' are
swapped in what follows. Let $n_1$ and $n_2$ be the extension 
degrees of $K_1$ and $K_2$ over $F$, respectively. Note that
$n_2 \ge n_1$.

We first compute a root 
$\alpha_1=\sum_{i=0}^{n_2-1}{a_i\alpha_2^i}$ of {\tt defpol1} in a
polynomial basis representation {\tt FFP(F,defpol2)} of $K_2$ using
\begin{verbatim}
   rootOfIrreduciblePoly(defpol1)$FFPOLY2(FFP(F,defpol2),F)
\end{verbatim}

By powering this root we compute the Matrix 
$R=(r_{i,j}) \in F^{n_2\times n_1}$ defined by
\[\alpha_1^j=\sum_{i=0}^{n_2-1}{r_{i,j}\alpha_2^i}\quad 0\le j<n_1\]

If $\beta=\sum_{j=0}^{n_1-1}{b_j\alpha_1^j\in K_1}$ we get
\[\beta=\sum_{j=0}^{n_1-1}\sum_{i=0}^{n_2-1}{b_jr_{i,j}\alpha_2^i}=
\sum_{i=0}^{n_2-1}{c_i\alpha_2^i}\]
where $c_i=\sum_{j=0}^{n_1-1}{b_jr_{i,j}}$. In the situation where
both representations are polynomial it is enough to use $R$ for
converting elements from $K_1$ into $K_2$ by
\[(c_0,c_1,\ldots,c_{n_2-1})^t=R(b_0,b_1,\ldots,b_{n_1-1})^t\]

To construct a concrete matrix representation $S$ of a left inverse
linear map of the $F$-Monomorphism represented by $R$ we proceed as
follows. We construct a square matrix $P$ by taking the first $n_1$
linearly independent rows of $R$, invert $P$ and put zero columns
at the proper places of $P$ to get $S\in F^{n_1\times n_2}$.

If one or both representations are normal, we use the basis change
matrices from section \ref{section9.1} 
to reduce this case to the polynomial case.
More precisely, if $K_2$ is normal basis represented we construct a
basis change matrix $M_2\in F^{n_2\times n_2}$ and compute
\[R:=M_2^{-1}R,\quad S:=SM_2\]

If $K_1$ is normal basis represented we construct a basis change
matrix $M_1\in F^{n_1\times n_1}$ and compute
\[R:=RM_1,\quad S:=M_1^{-1}S\].

After this computation $R$ and $S$ are the desired conversion matrices
\begin{verbatim}
   conMat1to2:=R
   conMat2to1:=S
\end{verbatim}

Now we can coerce from {\tt K1} into {\tt K2} by
\begin{verbatim}
   represents(conMat1to2 * coordinates(b)$K1)$K2
\end{verbatim}
and in the other direction by
\begin{verbatim}
   represents(conMat2to1 * coordinates(b)$K2)$K1
\end{verbatim}

\section{Polynomials over finite fields}
\label{section10}

Let $F=GF(q)$ and $E=GF(q^n)$ be represented by the domain
{\tt F} and {\tt E}, respectively. There are two packages with
functions concerning polynomials over finite fields.
\begin{verbatim}
   FFPOLY(F)      FiniteFieldPolynomialPackage(F)
   FFPOLY2(E,F)   FiniteFieldPolynomialPackage2(E,F)
\end{verbatim}

\subsection{Root finding}
\label{section10.1}

Although Axiom has a good factorizer for polynomials over fields,
which can be used to find roots in extensions, we have implemented
a function
\begin{verbatim}
   rootOfIrreduciblePoly(f)
\end{verbatim}
in the package {\tt FFPOLY2(E,F)}. This function computes only one
root in $E$ of a monic, irreducible polynomial $f\in F[X]$. This is
useful as in section \ref{section9} 
we have described how the knowledge of one
root is enough to find homomorphisms between finite fields by the
package {\tt FFHOM}. Furthermore, this implementation is in general
faster than the whole factorization. It uses Berlekamp's algorithm
as described in chap.3.4 of \cite{Lidl83}.

\subsection{Polynomials with certain properties}
\label{section10.2}

In {\tt FFPOLY(F)} there are functions concerning the properties
{\sl irreducibility}, {\sl primitivity}, {\sl normality} of monic
polynomials. Furthermore, Lenstra and Schoof proved in \cite{Lens87} the
existence of polynomials of arbitrary degree, which are both
primitive and normal for arbitrary finite of finite fields $F$.

Hence, we have four kinds of properties. We have implemented 
functions to check, whether a given polynomial is of a certain kind,
create a polynomial of a given degree of a certain kind, search the
next polynomial of a certain kind w.r.t. an ordering that will be
described below, and compute the number of polynomials of a given
degree of a certain kind.

The functions for normal polynomials are for example
\begin{verbatim}
   normal?(f)
   createNormalPoly(n)
   nextNormalPoly(f)
   numberOfNormalPoly(n)
\end{verbatim}

Unfortunately, there is no formula known to compute the number of
primitive and normal polynomials of a given degree. Up to this
exception similar operations are available for all four kinds of
properties. To get these functions one has to substitute {\tt normal}
by {\tt irreducible}, {\tt primitive}, or {\tt normalPrimitive}
(or equivently {\tt primitiveNormal}), respectively.

\subsection{Testing whether a polynomial is of a given kind}
\label{section10.3}

The function {\tt irreducible?} is in the package
{\tt DistinctDegreeFactorize}, where it had been implemented for
polynomial factorization purposes. It uses the fact that a
polynomial $f\in F[X]$ of degree $n$ is irreducible, if and only 
if $f$ is relatively prime to $X^{q^i}-X$ for all 
$i=1,2,\ldots,\floor{n/2}$. This and two other methods for
testing irreducibility are described in a nice way in A.K. Lenstra's
paper\cite{Lens82}.

To check whether a polynomial $f\in F[X]$ of degree $n$ is primitive
we use the algorithm described on page 87 in \cite{Lidl83} together with
theorem 3.16. It is the same algorithm as used for testing elements
of finite field domains on primitivity:

$f$ is primitive if and only if $X^{q^n-1}{\rm\ mod\ }f=1$ and if
for all prime factors $p$ of $q^n-1$ holds:
\[X^{(q^n-1)/p}{\rm\ mod\ }f \ne 1\]

To check whether a polynomial $f\in F[X]$ of degree $n\ge 1$ is
normal, we first check its irreducibility and compute then the
residues
\[X^{q^i}{\rm\ mod\ f},\quad {\rm for\ }0\le i < n\]
using the function {\tt reducedQPowers(f)}. Now $f$ is normal if
and only if this $n$ residues are linearly independent over $F$.

To check whether a polynomial is primitive and normal we have to
combine both tests.

\subsection{Searching the next polynomial of a given kind}
\label{section10.4}

In this section we describe various total orderings on the set of
monic polynomials $f=X^n+\sum_{i=0}^{n-1}{f_iX^i}\in F[X]$ of degree
$n$ with constant nonzero term $f_0$. These orderings are chosen
carefully to reflect special requirements on embedding and
constructing properties and sparsity for the various kinds.

Let $\#f$ be the number of nonzero coefficients of $f$ and 
terms$(f)$ denote the vector $(i:f_i\ne 0)$ of exponents appearing
in $f$ in ascending order. The core of all our orderings is the
following total order. If $g=X^n+\sum_{i=0}^{n-1}{g_iX^i}$ is
another element in $F[X]$, then $f<g$, if $f$ is more sparse than
$g$, i.e. $\#f < \#g$. This is very useful, as dealing with sparse
polynomials in polynomially represented finite field arithmetic 
is less time consuming, see section \ref{section12}. 
The ordering then is refined
by the lexicographical ordering of the exponent vectors terms$(f)$
and terms$(g)$, and, if they are equal, too, by the lexicographical
ordering of the coefficient vectors, induced by the comparison of
elements in $F$ via {\tt lookup}, i.e. for elements {\tt fi},
{\tt gi} of {\tt F}, {\tt fi $<$ gi} if and only if
{\tt lookup(fi) $<$ lookup(gi)}.

For the irreducibility we directly use this ordering. 
{\tt nextIrreduciblePoly(f)} returns the next irreducible polynomial
of degree $n$ which is greater than $f$.

The ordering for {\tt nextPrimitivePoly(f)} first compares the
constant terms. If $f_0=g_0$, then we use the ordering defined
above. This is for efficiency reasons. Changing $f_0$ requires the
computation of the next (w.r.t {\tt lookup}) primitive element of
$F$. This procedure takes more time than changing other coefficients
and is therefore done last.

Similarly, for normal polynomials we first pick the trace coefficients
$f_{n-1}$ and $g_{n-1}$ and compare these, if they are equal we
compare as described above. So we have tied together all those
normal polynomials, which define the same embedding of the ground
field.

For {\tt nextPrimitiveNormalPoly(f)} we order first according to
the constant terms, then according to the trace terms and finally
as above.

All of the functions yield {\tt "failed"} when called with a 
polynomial of degree $n$, which is greater than the greatest 
polynomial with the required property.

\subsection{Creating polynomials}
\label{section10.5}

To create an irreducible polynomial Axiom proceeds as follows.
Depending on the desired degree $n$, a start polynomial $f$ is
initialized with one value of 
$\{X^n,X^n+1,X^n+X\}$. Then {\tt nextIrreduciblePoly(f)} is called
to produce an irreducible polynomial.

If $f(X)=X^n+\sum_{i=0}^{n-1}{f_iX^i}\in F[X]$ is primitive and
$\alpha$ is a root of $f$ in $E$ then $N_{E/F}(\alpha)=(-1)^nf_0$
is primitive in $F$. Since the norm function is surjective, there
exist primitive polynomials $f$ for all $(-1)^nf_0$ which are
primitive in $F$. Therefore, to get a primitive polynomial $f_0$
is set to $(-1)^n$ times {\tt primitiveElement()\$F}. Then the
polynomials with $(f_1,\ldots,f_{n-1})$ ranging over $\{0,1\}^{n-1}$
are checked on primitivity in increasing order. The used ordering
of $\{0,1\}^{n-1}$ is ordering of the vectors by (hamming) weight,
where the vectors of equal weight are ordered lexicographically.
If no primitive polynomial is found by this procedure
{\tt nextPrimitivePoly}$(X^n+f_0)$ is called to produce one.

If $f(X)=X^n+\sum_{i=0}^{n-1}{f_iX^i}\in F[X]$ is normal over $F$
and $\alpha$ is a root of $f$ in $E$ then 
$T_{E/F}(\alpha)=(-f_{n-1})\ne 0$. Since the trace function is
surjective, there exist normal polynomials $f$ for all choices of
$0 \ne f_{n-1}\in F$. We set $f_{n-1}=-1$, thus $T_{E/F}(\alpha)=1$
and we get a nice embedding of $F$ into $E$ 
(see section \ref{section8.1}). A
normal polynomial is generated by calling 
{\tt nextNormalPoly}($X^n-X^{n-1})$.

Normal polynomials which are primitive, too, are constructed
analogically. We set $f_{n-1}=-1$ and $f_0=(-1)^nc$, where
$c=${\tt primitiveElement()\$F}. Then
{\tt nextNormalPrimitivePoly}$(X^n-X^{n-1}+f_0)$ is called.

\subsection{Number of polynomials of a given kind and degree}
\label{section10.6}

Formulae for the number of polynomials of the above kinds can be
found in many textbooks about finite fields, e.g. \cite{Lidl83}

The number of monic irreducible polynomials in $F[X]$ of degree $n$
is given by 
\[\frac{1}{n}\sum_{d|n}{\mu(n/d)q^d}\]
where $\mu$ denotes the number-theoretical M\"obius function on
$(\mathbb{N},|)$, see Theorem 3.25 in \cite{Lidl83}.

The number of primitive polynomials in $F[X]$ of degree $n$ is 1 if
$q=2$ and $n=1$. Otherwise it is given by
\[\frac{1}{n}\varphi(q^n-1)\]
where $\varphi$ denotes the Euler $\varphi$-function, 
see Theorem 3.5 in \cite{Lidl83}.

The number of normal polynomials in $F[X]$ of degree $n$ is given by
\[\frac{1}{n}q^n\prod_{n_i}{(1-q^{-n_i})}\]
where the product ranges over the degrees $n_i$ of the distinct
monic irreducible polynomials appearing in the canonical 
factorization of $(X^n-1)$ in $F[X]$, see Theorem 3.73 in \cite{Lidl83}.

\subsection{Some other functions concerning polynomials}
\label{section10.7}

There are two functions for generating random polynomials.

{\tt random(n)} yields a monic random polynomial of degree $n$.
For the random selection of the coefficients the function
{\tt random()\$F} from {\tt F} is used.

The function {\tt random(n,m)} yields a random polynomial $f$ of
degree $n \le deg(f) \le m$. After determining a random degree $d$
by using the function {\tt random()\$Integer} the polynomial is
computed by calling {\tt random(d)}.

The least affine multiple of a polynomial $f$ is defined to be
the polynomial $g\in F[X]$ of least degree of the form
\[g=\sum_i{g_iX^{q^i}}+\tilde{g},\quad g_i,\tilde{g}\in F_q\]
which is divisble by $f$ -- remember that $q$ is the order of $F!$.
The function call\\
{\tt leastAffineMultiple(f)} computes the least
affine multiple of $f$ using the algorithm describe on page 112
of \cite{Lidl83}.

Let $f\in F[X]$ be monic irreducible of degree $n$ and $\alpha$ 
be a root of $f$ in $E$. The matrix $Q\in F^{n\times n}$ defined by
\[\alpha^{q^j}=\sum_{i=0}^{n-1}{q_{i,j}\alpha^i},\quad
0 \le j < n\]
plays an important role in many computations concerning finite
fields (see e.g. section \ref{section9.1}). 
Therefore we implemented the function
{\tt reducedQPowers(f)}, which yields the array
\[[X{\rm\ mod\ }f,X^q{\rm\ mod\ }f,\ldots,X^{q^{n-1}}{\rm\ mod\ }f]\]
of $n$ polynomials. This array is computed efficiently using the
fact that powering by $q$ is a $F$-linear operation.

\section{Future directions}

As already mentioned in the text the new compiler will open much
more flexibility with datatypes. This could be used for an 
implementation which realizes each finite field datatype as an
extension of each of its subfields.

The use of domain-valued functions as e.g. {\tt groundField} would
allow more general extension mechanisms which in particular could
be used for recursive {\tt coerce}-functions between different
towers of extensions and representations.

Next we discuss the problem of better embedding between different
extensions for special representations. In the case of cyclic 
group representation J.H. Conway has suggested to use
{\sl norm-compatible} defining polynomials, which were called
{\sl Conway}-polynomials (satisfying an additional condition)
by R.A. Parker (see \cite{Nick88}). 
These polynomials are primitive polynomials,
whose roots are related to each other by the norm functions. Such
classes of polynomials are difficult to compute. The pay-off of this
computations are easy embeddings by integer multiplications.

The second author has developed a theory of {\sl trace-compatible}
defining polynomials for the embedding of normal basis representation
of the extensions. The resulting embedding are polynomials
multiplications in $F[X]$. He has implemementations of both these
methods in Axiom and furthermore he has used these for an
implementation of different representations of the algebraic
closure of a finite field. Contrary to all the other datatypes,
which became part of the Axiom system, these domain constructors
are right now not generally available. We hope that finally there
will be a way of distribution of this code.

A detailed description of both Conway's and the 
trace-compatibility method as well as the further implementations
can be found in the {\sl Doktorarbeit} \cite{Sche93} of the second author,
see also \cite{Lidl83}.

\section{Comparison of computation times between different
representations}
\label{section12}

To demonstrate the effects of different representations on time
efficiency, we added tables with computation times for
representative functions of some finite fields. The tables show
the computation times of these functions in milliseconds. The
times were yield in an Axiom session on an IBM RISC System/6000
model 550 workstation using the Axiom system command
{\tt )set message time on}. Each command was run 50 to 100 times
for random values and the measured time was divided by the
number of runs.

\subsection{The extension fields $GF(5^4)$ over $GF(5)$ and
$GF(2^{10})$ over $GF(2)$}
\label{section12.1}

We first consider two small fields:

\begin{center}
Table 1: Computation times for $GF(2^{10})$ and $GF(5^4)$
\end{center}
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
 & \multicolumn{3}{|c|}{$GF(2^{10})$}
 & \multicolumn{3}{|c|}{$GF(5^4)$}\\
\hline
operation      & poly & normal & cyclic & poly & normal & cyclic\\
\hline
addition       &    1 &      1 &      1 &    1 &      1 &      1\\
multiplication &    4 &     18 &      1 &    3 &      4 &      1\\
inversion      &   13 &    105 &      1 &   10 &     16 &      1\\
primitive?     &   71 &    158 &      1 &   28 &     41 &      1\\
normal?        &   88 &      4 &     15 &   25 &      4 &      5\\
minimalPolynomial & 130 &  290 &     28 &   33 &     44 &     10\\
degree         &   92 &      3 &      1 &   17 &      2 &      1\\
discreteLog    &  150 &    440 &      1 &  110 &    200 &      1\\
norm           &   10 &    106 &      1 &    6 &     17 &      1\\
associateLog   &   67 &      1 &    120 &   25 &      1 &     50\\
associateOrder &  150 &      5 &    310 &   49 &      4 &    100\\
associateEsp   &   22 &      4 &      2 &   29 &      4 &     15\\
trace          &   16 &      5 &      3 &    5 &      3 &      3\\
exponentiation &   37 &     70 &      1 &   20 &     21 &      1\\
\hline
\end{tabular}

$GF(2^{10})$ was built up as an extension of $GF(2)$ with the
polynomial $X^{10}+X^9+x^4+X+1$ which is primitive and normal
over $GF(2)$ at the same time.

An extension $GF(5^4)$ of $GF(5)$ via the primitive and normal
polynomial $X^4+4X^3+X+2$ over $GF(5)$.

Table 1 shows the computation times for the three different
representations of both fields.

\subsection{Different extensions of $GF(5^{21})$ over $GF(5)$}

For fields with many elements we omit the cyclic group representation.
In extensions of high degree the computation time depends on whether
one takes a 'good' or 'bad' polynomial for the representation. 'Good'
in the normal basis representation means a low complexity for the
according normal basis while 'good' in the polynomial basis 
representation means a small number of nonzero coefficients of the
polynomial.

\begin{center}
Table 2: Computation times for $GF(5^{21})$
\end{center}
\begin{tabular}{|l|r|r|r|r|}
\hline
 & \multicolumn{2}{|c|}{'good'}
 & \multicolumn{2}{|c|}{'bad'}\\
\hline
operation      & polynomial & normal & polynomial & normal\\
\hline
addition       &          1 &      1 &          1 &      1\\
multiplication &         35 &     86 &         34 &    188\\
inversion      &         95 &    710 &         95 &   3040\\
trace over GF(5) &      206 &     14 &        208 &     17\\
norm over GF(5) &        78 &    606 &         78 &   2660\\
minimalPolynomial &    1370 &   2590 &       1360 &   9000\\
associateOrder &       2780 &     35 &       2700 &     34\\
associateLog   &        710 &      1 &       1500 &      1\\
associateEsp   &       1140 &     28 &       1690 &     20\\
\hline
\multicolumn{5}{|l|}{exponentiation with exponents in range}\\
\hline
$<$100         &        210 &    350 &        250 &   1570\\
$<$1000        &        420 &    510 &        420 &   2200\\
$<$10000       &        590 &    600 &        600 &   2700\\
$\sim 5^{21}$   &       2400 &   1600 &       2500 &   7400\\
\hline
\end{tabular}

We examine the field extension $GF(5^{21})$ over $GF(5)$. In
the normal basis representation we chose the 'good' polynomial
\[X^{21}+X^{20}+X^{18}+X^{17}+3X^{16}+4X^{15}+2X^{11}+2X^{10}+
3X^8+3X^7+4X^6+2X^5+X+1\eqno{(2)}\]
which yields a normal basis of low complexity of 61 and the 'bad'
polynomial 
\[X^{21}+4X^{20}+1\]
which yields a normal basis complexity of 323.

For the polynomial basis representation we chose as the 'bad'
polynomial (2) and the 'good' polynomial $X^{21}+4X+1$. The
table2 shows the results.

Although the multiplication time is much higher in the normal
basis representation, the adaptive exponentiation algorithm
yields for high exponents lower exponentiation times.

\section{Dependencies between the constructors}
\label{section13}

The picture on the next page visualises the dependencies between
the different constructors of the finite field world of Axiom.

Here is the list of used abbreviations:

{\bf Categories}
\begin{verbatim}
   CHARNZ  CharacteristicNonZero
   FPC     FieldOfPrimeCharacteristic
   XF      ExtensionField
   FFC     FiniteFieldCategory
   FAXF    FiniteAlgebraicExtensionField
\end{verbatim}

{\bf Domains}
\begin{verbatim}
   SAE     SimpleAlgebraicExtension
   IPF     InnerPrimeField
   PF      PrimeField
   FFP     FiniteFieldExtensionByPolynomial
   FFCGP   FiniteFieldCyclicGroupExtensionByPolynomial
   FFNBP   FiniteFieldNormalBasisExtensionByPolynomial 
   FFX     FiniteFieldExtension
   FFCGX   FiniteFieldCyclicGroupExtension
   FFNBC   FiniteFieldNormalBasisExtension
   IFF     InnerFiniteField
   FF      FiniteField
   FFCG    FiniteFieldCyclicGroup
   FFNB    FiniteFieldNormalBasis
\end{verbatim}

{\bf Packages}
\begin{verbatim}
   DLP     DiscreteLogarithmPackage
   FFF     FiniteFieldFunctions
   INBFF   InnerNormalBasisFieldFunctions
   FFPOLY  FiniteFieldPolynomialPackage
   FFPOLY2 FiniteFieldPolynomialPackage2
   FFHOM   FiniteFieldHomomorphisms
\end{verbatim}

\chapter[Real Quantifier Elimination]
{Real Quantifier Elimination Tutorial by Hoon Hong}

This material is quoted from Hoon Hong's presentation at
Kyushu University in Japan in 2005. The state of the art
has moved a bit but this is an excellent presentation up
to that date.\cite{Hong05}

\section{Overview}

{\bf What is the real quantifier elimination problem?} As a simple example,
consider the following ``toy'' problem: Find a condition on $b$ and $c$ such
that $x^2 + bx + c > 0$ for all $x$. Recalling middle school math, we know
that an answer is $b^2-4c < 0$. A bit more formally, one can write the
above process as

\begin{tabular}{ll}
In: & $(\forall x)\quad x^2 +bx +c > 0$\\
&\\
Out: & $b^2 - 4c < 0$
\end{tabular}

Note that the input formula and the output formula are equivalent but
the output formula does not have the universal quantifier $(\forall x)$. 
We have just carried out the ``real quantifier elimination''. Generally, it
is a problem:

\begin{tabular}{ll}
In: & a formula (made of integral polynomials, $=$, $>$, $\land$, $\lor$,
$\lnot$, $\forall$, $\exists$),\\
& the so-called ``a well-formed formula in the first order theory\\
& of real closed field''.\\
&\\
Out: & an equivalent formula {\sl without} quantifiers.
\end{tabular}

Note that the decision problem (proving) is a special case where the
input formula does not have free variables (since it is trivial to
decide a formula without variables). The research goal, then, is to
devise efficient algorithms/software that carry out the real
quantifier elimination.

{\bf Why work on this problem?} There are two main sources of motivation for
tackling the quantifier elimination problem: the original motivation
from the foundational questions of mathematics, and the more recent
motivation from applications in various areas.

The original motivation was based on the observation that the
``existence'' of a quantifier elimination procedure often implies
various other important properties (such as completeness) about the
theory under investigation and other theories that can be reduced to it.

On the other hand, the more recent motivation is based on the
observation that real quantifier eliminating provides a simple but
expressive abstraction for various important problems arising from
constructive mathematics, and in particular various non-trivial
problems in science and engineering, such as geometric modeling,
geometric theorem proving and discovery, termination proof of term
rewrite systems, stability analysis of control systems or numerical
schemes for partial differential equations, approximation theory, 
optimization, constraint logic programming, fracture mechanics, robot
motion planning, etc. Thus, progress in real quantifier elimination
can have significant impact on algorithmic (computer-assisted)
mathematics, science and engineering.

{\bf What has been done so far?} 
Alfred Tarski\cite{Tars48}, around 1930, gave the first
algorithm for the problem.  This was an epoch making event in the
foundational studies in mathematics and logic because it meant
that not only the elementary real algebra but also various
mathematical theories built on real numbers are also decidable, for
instance, the elementary algebra of complex numbers, that of
quarternions, that of n-dimensional vectors, and elementary
geometry (Euclidean, non-Euclidean, projective).

Since then, various improved, new, and specialized algorithms have
been devised with better computing times for large inputs
(asymptotically) or for small inputs or for special inputs.  Now
computer implementations exist and are applied to various problems
from mathematics, sicence and engineering.

{\bf References} There are several collections dedicated to quantifier
elimination (as of 2005):
\begin{itemize}[noitemsep]
\item {\sl Algorithms in Real Algebraic Geometry.}\cite{Arno88a}
\item {\sl Computational Quantifier Elimination} Hoon Hong, Oxford
University Press (1993)
\item {\sl Quantifier Elimination and Cylindrical Algebraic Decomposition}
\cite{Cavi98}
\item {\sl Quantifier Elimination and Applications} Journal of Symbolic
Computation, Hoon Hong and Richard Liska, Academic Press (1996)
\end{itemize}

There are also a few books that give and exposition on the subject
\begin{itemize}[noitemsep]
\item {\sl Algorithmic Algebra}\cite{Mish93}
\item {\sl Goemetrie algebrique reelle} Jacek, Bochnak, Michel Coste,
Marie-Francoius Roy (1986)
\end{itemize}

\section{General Methods}

We begin by describing several general methods that work for arbitrary
formulas. We will start with the historically first method (by
Tarski\cite{Tars48}), then proceed to the next important method of cylindrical
algebraic decomposition (by Collins\cite{Coll75}), then conclude with recent
methods with better asymptotic computing bounds.

\subsection{The First Method}

Alfred Tarski, the Polish-born U.S. mathematician, logician and
philosopher, discovered the first quantifier elimination algorithm
about 1930, and submitted a monograph for publication in 1939 which
was scheduled to appear in 1939 under the title {\sl The completeness of
elementary algebra and geometry} in the collection {\sl Autualit\'es
scientifiques et industrielles}. However due to the World War, the
publication did not materialize.  In 1948, the monograph was rewritten
by his friend J. C. C. McKinsey and was published under the title {\sl A
Decision Method for Elementary Algebra and Geometry}. In 1967,
Tarksi's original monograph was also published by the Institute Blaise
Pascal in Paris.

The method of Tarski is not efficient. The worst case asymptotic
computing time is {\sl non-elementary} in that it cannot be bounded by an
finite tower of exponential functions in the number of variables. But
it contains several important ideas/techniques that had significant
influence on the later methods.

\subsubsection{Method}

Tarskis method carries out a series of reduction steps until it comes
down to some simple cases which are tackled using 
Sturms theorem\cite{Stur1829} and its generalization.

\vskip 0.5cm
\noindent
{\bf Reduction 1}: First, the general quantifier elimination problem can be
easily reduced to that for the formulas with {\sl one existential}
quantifier. This is because we can rewrite universal quantifiers in
term of existential quantifiers 
($\forall x \phi$ is equivalent to $\lnot\exists x \lnot \phi$) and then
repeatedly eliminate the innermost existential quantifier one at a time.

\vskip 0.5cm
\noindent
{\bf Reduction 2}: Now, we need to eliminate the quantifier from a
formula of the form $\exists x \phi$. By pushing the negations inward
onto the relational operators, rewriting $\ge$ in terms of disjunction
of $=$ and $>$, putting the resulting formula into disjunctive normal
form, and by using that the fact that disjunction commutes with an
existential quantifier, we can reduce the problem to the following
three types:

\begin{tabular}{ll}
 & \\
(1) & $\exists x [P_1 = 0 \land \cdots \land P_r=0]$\\
(2) & $\exists x [P_1 = 0 \land \cdots \land P_r=0 \land Q_1 > 0 \land
\cdots \land Q_s]$\\
(3) & $\exists x [Q_1 > 0 \land \cdots \land Q_x > 0]$\\
 & \\
\end{tabular}

\vskip 0.5cm
\noindent
{\bf Reduction 3}: Using the fact that
$P_1 = 0 \land \cdots \land P_r = 0$ is equivalent to
$P_1^2 + \ldots + P_r^2$, we can reduce the above (1) and (2) to

\begin{tabular}{ll}
 & \\
($1^\prime$) & $\exists x [P = 0]$\\
($2^\prime$) & $\exists x [P=0 \land Q_1 > 0 \land \cdots \land Q_s]$\\
 & \\
\end{tabular}

\vskip 0.5cm
\noindent
{\bf Reduction 4}: Next we will reduce the case (3) to the case
($2^\prime$). Suppose that all the polynomials $Q_i$'s are positive
on some value $p$ of $x$. Then there are only three possibilities:
\begin{itemize}
\item All the polynomials continued to be positive for $x > p$.
\item All the polynomials continued to be positive for $x < p$
\item There are two values $p_1$ and $p_2$, $p_1 < p < p_2$ such that
one of the polynomials vanish on $p_1$ and one (possibly the same)
of the polynomials vanish on $p_2$. More succinctly put, the product of
all the polynomials vanishes on $p_1$ and $p_2$. 
By Rolle's theorem\cite{Roll1691}, then,
there must exist in ($p_1,p_2$) a real root of the derivative of the
product of the polynomials.
\end{itemize}

Let lc denote leading coefficient. We can write the above case
analysis formally:
\begin{itemize}
\item lc$(Q_1) > 0 \land \cdots \land $ lc$(Q_s) > 0$
\item lc$(Q_1)(-1)^{{\rm deg}(Q_1)} > 0 \land \cdots \land $
lc$(Q_s)(-1)^{{\rm deg}(Q_s)} > 0$
\item $\exists x [(Q_1\cdots Q_s)^\prime = 0 \land Q_1 > 0\cdots Q_s > 0]$
\end{itemize}

Note that the first two are already quantifier-free and the last one
belongs to the case $(2^\prime)$. Thus, the case (3) has been reduced
to the case ($2^\prime$).

Actually the reasoning above is not entirely correct. It can happen
that a (formal) leading coefficient might depend on free variables,
and for some values of the free variables, the leading coefficient
might vanish, making it no longer the leading coefficient. In order to
correct it, we will have do some more easy but tedious case analysis
depending on the vanishing of the coefficients.

\vskip 0.5cm
\noindent
{\bf Reduction 5}: Let $\exists_k x \phi$ means that there exactly $k$
distinct real values of $x$ that satisfy $\phi$. Using this notation,
we can reduce the case ($1^\prime$) and ($2^\prime$) to the following:

\begin{tabular}{ll}
&\\
($1^{\prime\prime}$) & $\exists_0 x [P = 0]$\\
&\\
($2^{\prime\prime}$) & $\exists_0 x [P=0 \land Q_1 > 0 \land \cdots
\land Q_s]$\\
&\\
\end{tabular}

This is because $\exists x \phi$ is equivalent to
$\lnot \exists_0 x \phi$. The reason for such rewriting is to prepare
for a certain induction done in the next reduction.

\vskip 0.5cm
\noindent
{\bf Reduction 6}: We will reduce the case ($2^{\prime\prime}$) to the
following:

\begin{tabular}{ll}
&\\
($2^{\prime\prime\prime}$) & $\exists_k x [P=0 \land Q > 0]$\\
&\\
\end{tabular}

\noindent
We will do this by repeatedly decreasing the number of inequalities one
at a time. For this, we only need to observe the following. Let
\[\begin{array}{rcl}
n_{++} & = &\#\{x|P = 0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
&&\hskip 2.5cm \land~ Q_{s-1} > 0 \land Q_s > 0\}\\
&&\\
n_{+-} & = &\#\{x|P = 0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
&&\hskip 2.5cm \land~ Q_{s-1} > 0 \land Q_s < 0\}\\
&&\\
n_{-+} & = &\#\{x|P = 0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
&&\hskip 2.5cm \land~ Q_{s-1} < 0 \land Q_s > 0\}\\
&&\\
r_1 & = & \#\{x|P=0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
&&\hskip 2.5cm \land~ Q_{s-1}Q_s^2 > 0\}\\
&&\\
r_2 & = & \#\{x|P=0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
&&\hskip 2.5cm \land~ Q_{s-1}^2Q_s > 0\}\\
&&\\
r_3 & = & \#\{x|P=0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
&&\hskip 2.5cm \land~ Q_{s-1}Q_s > 0\}\\
\end{array}\]
Then we have the following
\[\begin{array}{rcl}
r_1 & = & n_{++} + n_{+-}\\
r_2 & = & n_{++} + n_{-+}\\
r_3 & = & n_{+-} + n_{-+}
\end{array}\]
By solving for $n_{++}$, we immediately obtain
\[n_{++} = \frac{r_1+r_2-r_3}{2}\]
From this observation, we see that the formula
\[\exists_k x [P=0 \land Q_1 > 0 \land\cdots\land Q_s]\]
is equivalent to the formula with one less inequalities
\[\begin{array}{rl}
\displaystyle\bigvee_{k=\frac{r_1+r_2-r_3}{2}} &
\left\{\begin{array}{c}
\exists_{r_1} x [P=0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
\land~Q_{s-1}^2 Q_s > 0]~ \land\\
\exists_{r_2} x [P=0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
\land~Q_{s-1} Q_s^2 > 0]~ \land\\
\exists_{r_3} x [P=0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
\land~Q_{s-1} Q_s > 0]
\end{array}\right\}
\end{array}\]
By inductively applying the same trick on the above three subformulas
we can arrive at the case ($2^{\prime\prime\prime}$).

\vskip 0.5cm
\noindent

{\bf Summary of all reductions:} Through all the above reduction
steps, the general quantifier elimination problem is reduced to the
following two special cases:

\begin{tabular}{cl}
($1^{\prime\prime\prime}$) & $\exists_k x [P = 0]$\\
($2^{\prime\prime\prime}$) & $\exists_k x [P = 0 \land Q > 0]$
\end{tabular}

\noindent
The case ($1^{\prime\prime\prime}$) is a bit more general than the case
($1^{\prime\prime}$) that we actually need. But we will keep it that way
since it is symmetric to the case ($2^{\prime\prime\prime}$). From now on,
we will tackle these two special cases.

\vskip 0.5cm
\noindent
{\bf Case ($1^{\prime\prime\prime}$)}
This case can be readily handled by 
{\sl Sturm's theorem}\cite{Stur1829}. Let $P$ be a
univariate polynomial. Let
$P_1=P, P_2=P^\prime, P_3,\ldots,P_n$ be a sequence where $P_{k+1}$ is
the negative of the remainder obtained by dividing $P_{k-1}$ by $P_k$,
and $P_n$ is the last non-zero polynomial in the sequence. Let $\alpha$
be the number of changes of sign in the sequence for $x \rightarrow -\infty$
and let $\beta$ be that for $x \rightarrow \infty$. Then Sturm proved that
the number of distinct real roots of $P$ is exactly $\alpha - \beta$.

When there are no free variables in the formula
($1^{\prime\prime\prime}$), we only need
to apply the Sturms method to see if the number of distinct real
roots is indeed $k$. When there are free variables, we need to
parametrize the Sturms method. We will have to do some easy but
tedius case analysis depending the signs of the leading coefficients.

\vskip 0.5cm
\noindent
{\bf Case ($2^{\prime\prime\prime}$)}
This case can be handled again readily by Sylvester's generalization of
Sturm's theorem. Let $P$ and $Q$ be two univariate polynomials and let
$P_1=P, P_2=P^\prime Q, P_3,\ldots,P_n$ be a sequence obtained the same
way as in Sturm's method. Note that the only difference is that
$P^\prime Q$ is used in place of $P^\prime$. Then Sylvester proved that
\[\alpha - \beta = \#\{x|P = 0 \land Q > 0\}-\#\{x|P=0 \land Q < 0\}\]
Let $N(P,Q)$ denote this number. Further let
\[\begin{array}{rcl}
n_+ & = & \#\{x|P = 0 \land Q > 0\}\\
n_- & = & \#\{x|P = 0 \land Q < 0\}\\
n_= & = & \#\{x|P = 0 \land Q = 0\}\\
\end{array}\]
Then we have the following
\[\begin{array}{rcl}
N(P,1) & = & n_+ + n_- + n_=\\
N(P,Q) & = & n_+ - n_-\\
N(P^2+Q^2,1) & = & n_=
\end{array}\]
By solving for $n_+$, we immediately obtain
\[n_+ = \frac{N(P,1)+N(P,Q)-N(P^2+Q^2,1)}{2}\]
Again, when there are no free variables in the formula
($2^{\prime\prime\prime}$), we only need to apply Sylvester's method
to see if $n_+$ is indeed $k$. When there are free variables, we need
to parameterize the method. This concludes the description of Tarski's
method.

\subsection{Cylindrical Algebraic Decomposition Method}

Collins\cite{Coll75}, a former Ph.D. student of Rosser (the logician known
for Church-Rosser property of Lambda calculus) introduced a new method
which has a much better bound than Tarski's (doubly exponential in the
number of variables rather than non-elementary, thus the infinite
tower of exponents is reduced to two floors). 
Specifically, the maximum computing time of the method is
dominated by $(2n)^{2^{2r+8}}m^{2^{r+6}}d^3a$,
where $r$ is the number of variables in $\phi$ , $m$ is the
number of polynomials occurring in $\phi$, $n$ is the maximum degree of any
such polynomial in any variable, $d$ is the maximum length of any
integer coefficient of any such polynomial, and $a$ is the number of
occurrences of atomic formulas.

Since the introduction, this method has gone through various improvments by
Arnon\cite{Arno81}, 
Collins\cite{Coll91}\cite{Coll98},
Hong\cite{Hong90}\cite{Hong98}\cite{Hong90a}, 
McCallum\cite{Mcca93}\cite{Mcca84}
and also now has been implemented twice completely
Arnon\cite{Arno81},Hong\cite{Hong90a}

Hong's implementation
is currently used in several engineering/sicientific applications.
In this section, we describe the method and various improvements.

\subsubsection{Method}

We first begin by explaining the name of the method:
{\sl Cylindrical Algebraic Decomposition}

{\bf Definition 1}
\begin{itemize}[noitemsep]
\item A {\sl decomposition} of $U \subseteq \mathbb{R}^r$ is a finite
collection of disjoint connected subsets of $U$ whose union is $U$.
Each subset is called a {\sl cell}.
\item A cell $c$ is called {\sl algebraic} if it can be described by a
quantifier-free formula. The formula is called a {\sl defining formula}
of the cell, denoted by $d_c$.
\item A decomposition is called {\sl algebraic} if every cell is algebraic
\item a {\sl stack} over $U \subseteq \mathbb{R}^r$ is a decomposition of
$U\times\mathbb{R}$ such that the projection of each cell on $\mathbb{R}^r$
is exactly $U$.
\item A decomposition of $D$ of $\mathbb{R}^r$ is called {\sl cylindrical}
if $r=1$, or $r > 0$ and $D$ can be partitioned into stacks over cells of
a cylindrical decomposition of $D^\prime$ of $\mathbb{R}^{r-1}$. It is easy
to see that $D^\prime$ is unique and we call it the {\sl induced}
decomposition of $D$.
\item Let $A$ be a finite subset of $\mathbb{Z}[x_1,\ldots,x_r]$. 
A decomposition of $\mathbb{R}^\prime$ is 
called {\sl A-sign invariant} if each polynomial
in $A$ has a constant sign throughout each cell. \quad$\blacksquare$
\end{itemize}

Let $F\equiv (Q_{f+1}x_{f+1})\cdots(Q_rx_r)F(x_1,\ldots,x_r)$.
Let $A$ be the set of polynomials occurring in $F$.
Let $D_r$ be a $A$-sign invariant algebraic decomposition of $\mathbb{R}^r$.
Let $D_k$ be the induced cylindrical algebraic decomposition of
$D_{k+1}$ for $k=1,\ldots,r-1$. We can immediately observe the following:

{\bf Proposition 1}
\begin{itemize}[noitemsep]
\item The truth of $(Q_{f+1}x_{f+1})\cdots(Q_rx_r)F(x_1,\ldots,x_r)$
is constant throughout each cell $c$ of $D_k$. Let $v_c$ denote this
truth value.
\item Let $c$ be a cell of $D_r$, and let $s_c$ be a point in $c$.
Then $v_c=F(s_c)$. We will call $s_c$ a {\sl sample} point of $c$.
\item Let $c$ be a cell of $D_k$, $f\le k < r$, and let $c_1,\ldots,c_n$
be the cells in the stack over $c$. Then we have
\[v_c =
\left\{
\begin{array}{rl}
\lor_{i=1}^{n}v_{c_i} & {\rm\ if\ }Q_{k+1}=\exists\\
\land_{i=1}^{n}v_{c_i} & {\rm\ if\ }Q_{k+1}=\forall\\
\end{array}
\right.
\]
\item $F \Longleftrightarrow \bigvee_{c\in D_f, v_c={\rm true}}d_c$
\quad$\blacksquare$
\end{itemize}

Now assuming we have an algorithm (CAD) that constructs a $A$-sign
invariant cylindrical algebraic decomposition, we can immediately
devise a quantifier elimination algorithm.

\vskip 0.5cm
\noindent
{\bf Algorithm 1}
\vskip -0.2cm
\noindent
\hrulefill
\begin{center}
$F^\prime \longleftarrow QE(F)$
\end{center}
\noindent
\begin{tabular}{ll}
{\sl Input}: & $F$ is a formula\\
{\sl Output}: & $F^\prime$ is a quantifier-free formula equivalent to $F$
\end{tabular}

\begin{itemize}[noitemsep]
\item[(1)] $D_r \longleftarrow CAD(A)$
\item[(2)] For every cell $c$ in $D_r$, $v_c \longleftarrow F(s_c)$
\item[(3)] For $k=r-1,\ldots,f$ and for every cell $c$ in $D_k$
\begin{itemize}[noitemsep]
\item[(i)] Let $c_1,\ldots,c_n$ be the cells in the stack over $c$
\item[(ii)] $v_c \longleftarrow
\left\{
\begin{array}{rl}
\lor_{i=1}^n v_{c_i} & {\rm\ if\ }Q_{k+1} = \exists\\
\land_{i=1}^n v_{c_i} & {\rm\ if\ }Q_{k+1} = \forall\\
\end{array}
\right.$
\end{itemize}
\item[(4)] $F^\prime \longleftarrow \bigvee_{c\in D_f,v_c={\rm true}}d_c$
\quad$\blacksquare$
\end{itemize}
\vskip -0.5cm
\noindent
\hrulefill

Now it remains to describe how to construct an $A$-sign invariant
cylindrical algebraic decomposition. This will be done inductively
on the number of variables $r$.

\vskip 0.5cm
\noindent
{\bf CAD for $r=1$}.
Let $\alpha_1 < \cdots < \alpha_\ell$ be the real roots of the polynomials
in $A$. Then the $2\ell + 1$ cells
\[(-\infty,\alpha_1),[\alpha_1,\alpha_1],(\alpha_1,\alpha_2),\ldots,
(\alpha_{\ell-1},\alpha_{\ell}),[\alpha_{\ell},\alpha_{\ell}],
(\alpha_{\ell},\infty)\]
form a $A$-sign invariant cylindrical decomposition $D$ of $\mathbb{R}^1$.
Now for each $c$ in $D$, we need to construct a sample point $s_c$ and
a defining formula $d_c$. A sample point $s_c$ can be chosen to be any
point of $c$.

In order to construct a defining formula $d_c$, let $\sigma_1,\ldots,\sigma_n$
be the signs of the polynomials in $A$ on $c$. Let $\rho_i$ be
$>$, $=$, $<$ respectively if $\sigma_i$ is $+$, $0$, $-$. Then 
the formula
\[F_c\equiv A_1\rho_10 \land\cdots\land A_n\rho_n0\]
captures all points in $c$. If we are lucky, it will capture no other
points, and $F_c$ can be used as a defining formula $d_c$. But it can
happen that another cell shares the same signs, and $F_c$ captures that
cell also.

A general way to overcome this difficulty is to ``separate'' the cells
using derivatives.\footnote{Actually, for the $r=1$ case, one can
separate the cells simply by using the sample points of the sectors.
But this method is not generalizable to $r>1$.}
Specifically, let $A^\prime$ be the set of the
derivatives of the polynomials in $A$. We inductively build a $A^\prime$-sign
invariant cylindrical algebraic decomposition $D^\prime$ of $\mathbb{R}^1$.
Then for each cell $c$ of $D$, we can easily determine, by comparing the 
sample points of $D^\prime$ and $D$, 
the conjective cells $c_\mu^\prime,\ldots,c_\nu^\prime$ of $D^\prime$
whose union contains $c$ but is disjoint from any other cell of $D$ that
has the same signs as $c$. Then, we can set
\[d_c\equiv F_c \land \bigvee_{i=\mu}^\nu d_{c_i^\prime}\]

\vskip 0.5cm
\noindent
{\bf CAD for $r>1$}.
In order to do induction on the number of variables, it will be nice to
have a finite subset $P$ of 
$\mathbb{Z}[x_1,\ldots,x^{r1}]$ such that we can
``easily build'' an $A$-sign invariant cylindrical algebraic decomposition
$D$ of $\mathbb{R}^r$ from a $P$-sign invariant cylindrical algebraic 
decomposition $E$ of $\mathbb{R}^{r-1}$.

What do we mean by ``easily build''? Let $c$ be a cell of $E$. We want to
build an $A$-sign invariant stack over $c$. This becomes easy if
the zeros of the polynomials in $A$ naturally ``delineate'' the cylinder
over $c$, that is, the zeros of $A$ within the cylinder consist of
disjoint graphs of continous functions from $c$ to $\mathbb{R}$, 
say, $f_1 < \ldots < f_\ell$. If so, the following forms an
$A$-sign invariant stack over $c$:
\[\begin{array}{l}
\{x|x^\prime \in c,x_r < f_1(x^\prime)\},\\
\{x|x^\prime \in c,x_r = f_1(x^\prime)\},\\
\{x|x^\prime \in c,f_1(x^\prime) < x_r < f_2(x^\prime)\},\\
\{x|x^\prime \in c,x_r = f_2(x^\prime)\},\\
\{x|x^\prime \in c,f_2(x^\prime) < x_r < f_3(x^\prime)\},\\
\vdots\\
\{x|x^\prime \in c,x_r=f_\ell(x^\prime)\},\\
\{x|x^\prime \in c,f_\ell(x^\prime) < x_r\}\\
\end{array}\]
where $x$ stands for $(x_1,\ldots,x_r)$ and $x^\prime$ stands for
$(x_1,\ldots,x_{r-1})$. Let's call them $c_1,\ldots,c_{2\ell+1}$.
We call the odd-indexed cells {\sl sectors} and the even-indexed cells
{\sl sections}. Now we need to compute sample points and defining
formulas for these cells. Let $s=(s_1,\ldots,s_{r-1})$ be the sample
point of $c$. Let $A^*$ be the univariate polynomials in $x_r$ obtained
by evaluating the polynomials in $A$ on $s$. Then, we compute an $A^*$-sign
invariant cylindrical algebraic decomposition $D^*$ of $\mathbb{R}^1$ (as
shown before). Let $c_1^*,\ldots,c_{2\ell+1}^*$ be the cells of $D^*$.
Let $s_1^*,\ldots,s_{2\ell+1}^*$ be their sample points. Then, we can
set the following as the sample points of the cells in the stack over
$c$: $(s_1,\ldots,s_{r-1},s_i^*)$ where $1 \le i \le 2\ell+1$.

Let $d_1^*,\ldots,d_{2\ell+1}^*$ be the defining formulas of the cells of
$D^*$. We can obtain a defining formula for the cell $c_i$ by going
through the formula $d_i^*$ and replacing every instance of the
\[\frac{d^kA_\mu^*}{dx_r^k}{\rm\ by\ }
\frac{\partial^kA_\mu^*}{\partial x_r^k}\]

A caution. In order for the above argument to hold, the zeros of all
orders of the partial derivative of $A$ in $x_r$ (not just merely $A$)
must also delineate the cylinder over $c$. This is needed to ensure that
the relative positions of the zeros of the derivatives $A^*$ do not
change as $s$ ranges over $c$. If not, the defining formula for the cell
$c_i^*$ might not provide a structure for the defining formula for the
cell $c_i$.

Finally, it only remains to decide what to put into the set $P$ to
ensure the delineability of the zeros of $A$ and its derivatives.
Intuitively, we need to put sufficiently many polynomials so that their
zeros contain the projection of the ``critical'' points of the zeros of
$A$ and its derivatives. By critical points, we mean ``crossing'',
``vertical tangent'', ``vertical asymptotes'', and ``isolated points''.
Collins proved that the following set $P$ is sufficient.

\vskip 0.5cm
\noindent
{\bf Theorem 1 (Collin's projection)}
\[\begin{array}{rcl}
B &=& \{{\rm red}^k(a) ~|~ a\in A, {\rm deg}({\rm red}^k(a)) \ge 1\}\\
L &=& \{{\rm lc}(b) ~|~ b \in B\}\\
R &=& \{{\rm psc}_k(b_1,b_2)~|~
b_1,b_2\in B, 0\le k < {\rm min}({\rm deg}(b_1),{\rm deg}(b_2))\}\\
D &=& \{{\rm psc}_k(b,b^\prime) ~|~ b \in B, 0 \le k < {\rm deg}(b^\prime)\}\\
C &=& \{{\rm der}^k(b) ~|~ b \in B, 0 \le k < {\rm deg}(b)\}\\
D^\prime &=& \{{\rm psc}_k(c,c^\prime) ~|~ c \in C, 
0 \le k < {\rm deg}(c^\prime)\}\\
P &=& L \cup R \cup D \cup D^\prime
\end{array}\]
where
\begin{itemize}[noitemsep]
\item 'lc' stands for the leading coefficient
\item 'red' for polynomial reductum, that is, red($a$) is the polynomial
obtained from $a$ by removing the leading term.
\item 'der' for derivative
\item 'psc' for principal subresultant coefficient. Let
$a=a_mx^m+\ldots+a_0$ and $b=b_nx^n+\ldots+b_0$. Then
psc$_k(a,b)$ is the determinant of the square (Sylvester) matrix
\[\left[
\begin{array}{ccccccc}
a_m & a_{m-1} & \cdots  &  a_0   &         &        &\\
    &   a_m   & a_{m-1} & \cdots &  a_0    &        &\\
    &         & \cdots  & \cdots & \cdots  & \cdots &\\
    &         &         &  a_m   & a_{m-1} & \cdots & \cdots \\
b_n & b_{n-1} & \cdots  &  b_0   &         &        &\\
    &  b_n    & b_{n-1} & \cdots &  b_0    &        &\\
    &         & \cdots  & \cdots & \cdots  & \cdots &\\
    &         &         &  b_n   & b_{n-1} & \cdots & \cdots
\end{array}
\right]\]
in which there are $n-k$ rows of $a$ coefficients, $m-k$ rows of $b$
coefficients, and all elements not shown are zero.\quad$\blacksquare$
\end{itemize}
Roughly put, $L$ is there for vertical asymptotes, $R$ for crossing,
$D$ for vertical tangents or isolated points, and $D^\prime$ for
vertical tangents/isolated points of derivatives.

Let {\sl PROJ} denote the map that takes $A$ and produces $P$. Summarizing,
we have the following algorithm:

\vskip 0.5cm
\noindent
{\bf Algorithm 2}
\vskip -0.2cm
\noindent
\hrulefill

\begin{center}
$D \longleftarrow CAD(A)$
\end{center}

\noindent
\begin{tabular}{ll}
{\sl Input}: & $A$ is a finite set of polynomials in $r$ variables\\
{\sl Output}: & $D$ is an $A$-sign invariant cylindrical algebraic
decomposition
\end{tabular}

\begin{itemize}[noitemsep]
\item[(1)] if $r=1$ then build a $A$-cylindrical algebraic decomposition
$D$ of $\mathbb{R}^1$ as described above and return
\item[(2)] $P \longleftarrow PROJ(A)$
\item[(3)] $E \longleftarrow CAD(P)$
\item[(4)] For each cell $c$ of $E$, build an $A$-sign invariant stack $S_c$
over $c$ as described above
\item[(5)] $D \longleftarrow \cup_{c\in E}S_c$\quad$\blacksquare$
\end{itemize}
\vskip -0.5cm
\noindent
\hrulefill

\subsubsection{Improvements}

\noindent

Collins method allowed various improvements: 
clustering\cite{Arno81}\cite{Mcca02},
smaller projections \cite{Mcca84} \cite{Hong90} \cite{Brow01a},
efficient order of projections\cite{Dolz04},
partial cylindrical algebraic decomposition\cite{Hong90a}\cite{Coll91},
solution formula construction\cite{Hong98}\cite{Brow01a},
strict inequalities\cite{Mcca93},
equational constraints\cite{Coll98},
use of interval methods\cite{Coll02},
and so on. We will briefly discuss a few of the above improvements.

\vskip 0.5cm
\noindent
{\bf Clustering} Arnon\cite{Arno81}  made an observation that a 
stack can be constructed
over a union of cells, provided that the union (cluster) is connected
and the projection polynomials are sign-invariant on it. Thus, before
lifting we first combine cells into clusters and choose only one
sample point per each cluster and lift it. This usually significantly
reduces the number of stack constructions which must be performed.

This idea requires a method for computing a cluster, which boils down
to adjacency computation (finding out which cells are topologically
adjacent). In 
\cite{Arno84}\cite{Arno88b}\cite{Mcca02}
methods for 2 and 3 and more variables were developed.

\vskip 0.5cm
\noindent
{\bf Smaller Projection} In \cite{Mcca84}\cite{Mcca88} McCallum proved
that, if the polynomials in $A$ are ``well'' ordered, the sets
$R$, $D$, $C$, $D^\prime$ can be made smaller.

\[\begin{array}{rcl}
R &=& \{{\rm psc}_0(a_1,a_2)~|~a_1,a_2 \in A\}\\
D &=& \{{\rm psc}_0(a,a^\prime)~|~a\in A\}\\
C &=& \{{\rm der}^k(a)~|~ a\in A, 0\le k < {\rm deg}(a)\}\\
D^\prime &=& \{{\rm psc}_0(c,c^\prime)~|~c\in C\}
\end{array}\]

Note that these sets are much smaller than Collins original ones,
because it does not involve reductums nor higher order principal
subresultant coefficients.

Hong\cite{Hong90} proved that the set R in the original projection
could be restricted to (without any side conditions)
\[R = \{{\rm psc}_k(a,b)~|~a\in A, b\in B, 
0 \le k < {\rm min}({\rm deg}(a),{\rm deg}(b))\}\]

The resulting projection set is smaller than the original one and
larger than McCallums. But it can be useful since it, unlike
McCallums, does not impose any condition on the polynomials and
further McCallums projection requires that the clusters must be order
invariant. These can result in possibly smaller clusters.  
Brown\cite{Brow01a} made a further improvement.

\vskip 0.5cm
\noindent

{\bf Partial CADs} Hong\cite{Hong90a} (see also \cite{Coll91}) showed
that we can {\sl very often} complete quantifier elimination by a
partially built cylindrical algebraic decomposition, if we utilize,
during cylindrical algebraic decomposition, more information contained
in the input formula such as quantifiers, the boolean connectives,
the absence of some variables from some polynomials occurring in the
input formula, etc. These improvements did not change the asymptotic
worst case bound, but gave significant speedups in most problems
known in the literature, that many problems that would require at
least several months could now be solved within a few seconds.

As an example, one can utilize the quantifier information as follows:
Let us consider a sentence in two variables 
$(\exists x)(\exists y)F(x,y)$. 
The original method computes a certain decomposition $D_1$ of
$\mathbb{R}$ and then lifts this to a decomposition $D_2$ of
$\mathbb{R}^2$ by constructing a stack of cells in the cylinder over
each cell of $D_1$. Then the quantifier elimination proceeds by
determining the set of all cells of $D_1$ in which $(\exists y)F(x,y)$
is true.  Finally, it computes the truth value of 
$(\exists x)(\exists y)F(x,y)$ by checking whether the set is empty. In
contrast, one may construct only one stack at a time, aborting the CAD
construction as soon as a cell of $D_1$ is found which satisfies
$(\exists y)F(x,y)$,
if any such cell exists. The method illustrated above for two
variables extends in an obvious way to more variables, with even
greater effectiveness because the CAD construction can be partial in
each dimension.

\vskip 0.5cm
\noindent
{\bf Simple Solution Formula Construction}

Hong\cite{Hong90a}\cite{Hong98} 
devised a new method for constructing solution
formulas which is more efficient and produces much simpler
formulas. The original algorithm obtains a solution formula by forming
a disjunction of defining formulas of solution cells. This method
works for any input formula, but produces very large formulas and
often increases greatly the amount of computation required because of
the augmented projection (projection involving derivatives).

The method of Hong does not use augmented projection, but instead
tries to construct solution formulas using only projection
polynomials. It can fail to produce a solution formula, but the
experiments with many QE problems from diverse application areas
suggest that it will rarely fail. The method also uses a logic
minimization algorithm to simplify solution formulas. It carries out
simplification based not only on the logical connectives but also on
the relational operators. This is done with three-valued logic. It
further reduces the size of the inputs to the multiple-valued logic
minimization algorithm by taking advantage of the structure of the
input formula. As the result, the method produces simpler formulas (a
few lines instead of several pages) faster (a few seconds instead of
hours).

Brown\cite{Brow99} provided a method that generates the augmented
projection polynomials on demand, thus making the method complete
(never fails, unlike Hongs).

\vskip 0.5cm
\noindent
{\bf Strict Equalities}

McCallum\cite{Mcca93} observed that the stack construction phase can be
significantly improved if the input formula is an existentially
quantified sentence of a system of strict inequalities. This kind of
sentence arises naturally from geometric modeling, robot simulation,
non-linear optimization, etc.

The key observation is that the solution set of the quantifier-free
matrix (conjunction/disjunction of strict inequalities) is either
empty or must contain a full-dimension open set. Thus, during the
stack construction, one only needs build stacks over sectors (since
any cells belonging to stacks over sections cannot not be of
full-dimension). If one of the constructed full-dimensional cells
satisfies the matrix, then the sentence is true.  If not, one can stop
and report that the sentence is false.

This gives a huge savings not only because the number of the stack
constructions is reduced, but also the full-dimensional cells are much
cheaper to construct since it does not involve algebraic number
computation.

\vskip 0.5cm
\noindent
{\bf Equational Constraints}

McCallum\cite{Mcca93} observed that the projection phase and the stack
construction phase can be significantly improved when the input
formula has ``equational constraints''. A {\sl equational constraint} of a
prenex formula is a polynomial equation which is implied by its
quantifier-free matrix. For example, if the quantifier-free matrix is
of the form: $A=0 \land F$, then $A = 0$ is an equational constraint.

The key observation is that the matrix is false if $A \ne 0$  regardless
of the truth of $F$. Thus, we only need to ensure that the other
polynomials (occurring in $F$) are sign-invariant on the sections of
$A$. For a simple presentation, let us assume that the inputs
polynomials are well-oriented (and thus McCallums projection theorem
can be applied). Then, we only need to put the following into the
projection set:
\begin{itemize}[noitemsep]
\item The discriminant and (enough of) the coefficients of $A$
\item The resultant of $A$ and each polynomial occuring in $F$
\end{itemize}

Sometimes we can propagate the property of ``equational constraint''
down to lower dimensional space so that we can apply the smaller
projection operation again. For instance, let $A_1$ and $A_2$ both be
equational constraint polynomials. Let $R$ be their resultant. It is
well-known that 
$A_1=0 \land A_2 = 0 \Longrightarrow R=0$ Thus, $R=0$ is also an
equational constraint, we can use it to reduce the second
projection. This idea obviously generalizes, with greater effect, when
there are more than two equational constraints.

When there are equational constrains obtained by resultant
computations (as shown above), then they can be used for pruning stack
constructions. Whenever a stack has already been constructed in which
there are sections of an equational constraint polynomial, all other
cells in the stack can be marked false and it is unnecessary to
construct stacks over them. When there are many equational constraints
in the input formulas, the accumulated reduction on the number of
stack constructions can be drastic.

\subsection{Quantifier-Block Elimination Methods}

During last decade, there was an intensive research on improving
asymptotic worst case computing bounds 
\cite{Beno86},
\cite{Fitc87},
\cite{Grig88}, 
\cite{Grig88a},
\cite{Cann88},
\cite{Cann93},
\cite{Hein89},
\cite{Rene92},
\cite{Weis98}.
The key idea is that a consecutive block of same quantifiers can
be eliminated by a single projection in an exponential time (in the
number of variables). Thus, the total complexity is doubly exponential
in the number of quantifier alternations, not on the number of
variables. If all quantifiers are the same (1 quantifier-block), then
the total complexity is singly exponential in the number of variables.

Let $n$ be the number of variables in the input sentence, $m$ the number
of polynomials, $d$ the degree, $L$ the bit length bound for the
coefficients, and $n_i$ is the number of quantifiers in the $i$-th
quantifier block. The  best asymptotic bound so far (obtained by
Renegar) is is $L(log~L)(log~log~L)(md)^{\prod_{k=0}^\omega O(n_k)}$.

\subsubsection{Method}

There are several methods with the similar complexity bounds.
\cite{Grig88a}, 
\cite{Cann88},
\cite{Cann93},
\cite{Hein89},
\cite{Rene92}.
Though different in details, they are very similar in the
overall strategy. Thus in this tutorial, we will be satisfied with
sketching the overall strategy/ideas.

\vskip 0.5cm
\noindent
{\bf Overall Ideas}
\begin{enumerate}
\item First note that the arbitrary quantifier elimination problem can
be trivially reduced to the {\sl existential} quantifier elimination problem
(all quantifiers are existential), because if we have an algorithm for
the existential quantifier elimination, we can repeatedly apply it to
eliminate each block of quantifiers one at a time (for a universal
block after double negation).

One important exception. Renegar's method does not apply the
existential quantifier elimination repeatedly. Instead, it follows the
idea of Collins' Cylindrical Algebraic Decomposition: namely the
repeated projection. But its projection removes a block of variables
at once, while Collins' projection removes one variable one at time.

\item The existential quantifier elimination can be straightforwardly
(though very tedious) reduced to the {\sl existential} decision problem (no
free variables), because we can ``unravel'' a decision algorithm to turn
it into a parametric one. More specifically, a decision algorithm can
be viewed as a tree where each internal node corresponds computation
(such addition/multiplication) or comparison (for branching) and where
each leaf is associated with true or false. Given a sentence, we
follow only one path until we reach a leaf. If it is a true node, then
the sentence is true, else false. But when the input is not a sentence
and contains free variables, we do not know which path to follow at a
branching node. Thus, we follow all the paths while accumulating the
branch conditions (on the free variables) associated with each
path. Then, the disjunction of all the accumulated conditions
associated with true leaves will be the quantifier-free formula that
we desire.

Again, Renegar does not carry this unraveling at each quantifier
block. But he does this only once for the ``last'' free variable block.

\item The existential decision problem can be reduced to the {\sl smooth}
existential decision problem where the solution set of the
quantifier-free matrix is closed and smooth. This can be done by some
simple geometric tricks and (careful) infinitesimal smoothing.

\item Next the smooth existential decision problem is reduced to the
{\sl equationally constrained} existential decision problem:
\[\exists(x_1,\ldots,x_n)P_1(x_1,\ldots,x_n) \land\cdots
P_n(x_1,\ldots,x_n) \land F\]
where $F$ is a quantifier-free formula and the system
$P_1 = P_2 = \cdots = P_n = 0$
has only finitely many (complex) solutions. This can be done by
applying the Lagrange multiplier method since a smooth set has a
``optimal/critical'' point for ``suitably'' chosen objective functions
(Morse function). The suitable choice can be done either by (another)
infinitesimal perturbation or some searching (in a finite but very big set).

Note that this idea of using ``optimal/critical'' points is not new, but
was already used by Seidenberg\cite{Seid54} for improving Tarski's method. He
used the ``distance'' from the origin as the objective function. When
this turns out to be not suitable, then he tried linear transformation
of coordinates.

Weispfenning's method is an exception here in that it does not require
that the system $P_1,\ldots,P_n$ has a finitely many complex
solution. It checks if the system indeed has the property, if so
continue, continue, if not, instead of perturbing the system, it view
some of the variables as parameters and checks again whether the
system has finitely many complex solutions (parametrically). This can
be done by using the method of comprehensive Gr\"obner basis method
\cite{Weis92}. Thus in the worst case, the complexity of this method becomes
doubly exponential in the number of variables. But some preliminary
experiments show that this might be a very promising method for small
inputs.

\item The equationally constrained existential decision problem can be
solved in various ways. Usually, the system of equations are ``solved''
by using the $u$-resultants \cite{Cann87}
or the Hermite's quadratic form \cite{Pede93}
The sign computation of the polynomials occurring $F$ on the roots of
the system of equations can be obtained by the method of BKR \cite{Beno86}.

\end{enumerate}
\vskip 0.5cm
\noindent
{\bf Grigorev's Algorithm}:
Now for those who want to get a glance at the details of some
algorithms, we give a detailed description of the algorithm of
Grigor'ev and Vorobjov \cite{Grig88} and Renegar's \cite{Rene92}, but without
explanation. First we give Grigorev's algorithm for existential
decision problem. In the case of a positive answer, the algorithm also
constructs a representative set for the family of components of
connectivity of the set of all real solutions of the system, which is
$\mathcal{T}$ 
in Step (8) of the algorithm described below. We begin by defining
several notations which will be used in the algorithm description:
\begin{itemize}
\item Let $K$ be an arbitrary ordered field. The $\tilde{K}$ denotes
the algebraic closure $K$, and $\tilde{K}$ the real algebraic closure
of $K$.
\item Let $\epsilon_1$ be a positive infinitesimal over $\tilde{\mathbb{Q}}$.
Then $F_1$ denotes the real algebraic closure of 
$\tilde{\mathbb{Q}}(\epsilon_1)$.
\item Let $\epsilon$ be a positive infinitesimal over $F_1$.
Then $F$ denotes the real algebraic closure of $F_1(\epsilon)$.
\item Let $f_1,\ldots,f_m$ be elements of $K[x_1,\ldots,x_n]$.
Then $\{f_1=0,\ldots,f_m=0\}$ denotes the set of all solutions
of the system $f_1=\cdots =f_m = 0$ over the algebraic closure of $K$.
\end{itemize}

\vskip 0.5cm
\noindent
\hrulefill
\begin{center}
$t \longleftarrow {\bf Decide}(P)$
\end{center}

\noindent
\begin{tabular}{ll}
{\sl Input}: & $P$ is a quantifier-free formula of the following kind:\\
\end{tabular}
\[f_1 > 0 \land\cdots\land f_k>0 \land f_{k+1}\ge 0 \land\cdots\land
f_m \ge 0\]
\hskip 1.5cm where $f_i \in \mathbb{Z}[x_1,\ldots,x_n]$.

\noindent
\begin{tabular}{ll}
{\sl Output}: & $t$ is the truth of the sentence
$(\exists x_1 \in \mathbb{R})\cdots(\exists x_n \in \mathbb{R})
P(x_1,\ldots,x_n)$
\end{tabular}

\begin{itemize}[noitemsep]
\item[(1)] Let $f_{m+1} = x_0f_1\cdots f_{k-1}$\\
Let $g_1=(f_1+\epsilon_1)\cdots(f_{m+1}+\epsilon_1)-\epsilon_1^{m+1}
\in \mathbb{Z}[\epsilon_1][x_0,\ldots,x_n]$
\item[(2)] Let $\tilde{L}$ be a bit length bound for the coefficients of
$f_i$, and $\tilde{d}$ a degree bound for $f_i$ for every
$1 \le i \le m+1$.\\
Let $R=3^{(\tilde{L}+log(m+1))p(\tilde{d}^{n+1})}$,
where $p \in \mathbb{Z}[x]$ is a certain polynomial defined in Page 62 of
\cite{Grig88}.\\
Let $g=g_1^2+(x_0^2+\cdots+x_{n+1}^2-(R+1))^2\in 
\mathbb{Z}[\epsilon_1][x_0,\ldots,x_{n+1}]$
\item[(3)] Let $N = (8md)^{n+2}$\\
Let $\Gamma = \{1,\ldots,N\}^{n+1}$\\
Let $\mathcal{I}^\prime = \{\}$
\item[(4)] For each $\gamma = (\gamma_1,\ldots,\gamma_{n+1})\in \Gamma$ do
\begin{itemize}[noitemsep]
\item[(4.1)] Let $\tilde{V}^{(\epsilon)}\subset\tilde{F}^{n+2}$ be the
variety of the system
\[\begin{array}{rcl}
g-\epsilon&=&\displaystyle\left(\frac{\partial g}{\partial x_1}\right)^2
-\frac{\gamma_1}{N(n+2)}\Delta = \cdots\\
&&\\
&=&\displaystyle\left(\frac{\partial g}{\partial x_{n+1}}\right)^2
-\frac{\gamma_{n+1}}{N(n+2)}\Delta = 0
\end{array}\]
where $\Delta=\sum_{j=0}^{n+1}\left(\frac{\partial g}{\partial x_j}\right)^2$\\
Let $\tilde{V}^{(\epsilon)}=\bigcup_j \tilde{V}_j^{(\epsilon)}$,
where each $\tilde{V}_j^{(\epsilon)}$ is a component irreducible over the
field $\mathbb{Q}(\epsilon_1,\epsilon)$.
\item[(4.2)] For each null dimensional $\tilde{V}_j^{(\epsilon)}$ do
\begin{itemize}[noitemsep]
\item[(4.2.1)] Let $\mathcal{R}\subset\tilde{F}_1^n$ be a set containing the
standard part (relative to $\epsilon$) of every point (for which the standard
part is definable) from the component $\tilde{V}_j^{(\epsilon)}$.
\item[(4.2.2)] Let $\mathcal{R}_1^\prime = \mathcal{R}_1 \cap
\{g=0\}\cap F_1^{n+2}$
\item[(4.2.3)] Set $\mathcal{I}^\prime to 
~\mathcal{I}^\prime\cup\mathcal{R}_1^\prime$
\end{itemize}
\end{itemize}
\item[(5)] Let $\mathcal{I}=\pi(\mathcal{I}^\prime) \subset F_1^{n+1}$
where $\pi(x_0,\ldots,x_{n+1})=(x_0,\ldots,x_n)$.
\item[(6)] Let $\mathcal{R}\subset\tilde{\mathbb{Q}}^{n+1}$ be a set
containing the standard part (relative to $\epsilon_1$) of every point
(for which the standard part is definable) from the set $\mathcal{I}$.
\item[(7)] Let $\mathcal{T}^\prime=\mathcal{R}\cap\{f_1 \ge 0,
\ldots,f_{m+1} \ge 0\}\cap\tilde{\mathbb{Q}}^{n+1}$
\item[(8)] Let $\mathcal{T}=\pi_1(\mathcal{T}^\prime)$
where $\pi_1(x_0,\ldots,x_n)=(x_1,\ldots,x_n)$
\item[(9)] Finally let $t$ be true if $\mathcal{T}$ is non-empty,
false otherwise.\quad$\blacksquare$
\end{itemize}
\vskip -0.5cm
\noindent
\hrulefill

\vskip 0.5cm
In \cite{Grig88} it is shown that the worst case time complexity of 
this algorithm is dominated by $L(md)^{n^2}$.

In \cite{Grig88a} Grigor'ev, by generalizing this algorithm, gave a
decision algorithm for the first order theory of real closed fields.

\vskip 0.5cm
\noindent
{\bf Renegar's Method}. Now we give an overview of Renegar's\cite{Rene92}
algorithm.

\vskip 0.5cm
\noindent
\hrulefill
\begin{center}
$t \longleftarrow {\bf MainAlgorithm}(P)$
\end{center}

\noindent
\begin{tabular}{ll}
{\sl Input}: & $P$ is a quantifier-free formula with variables
$x_1,\ldots,x_n$.\\
{\sl Output}: & $t$ is the truth of the sentence
$(\exists x_1 \in \mathbb{R})\cdots(\exists x_n \in \mathbb{R})
P(x_1,\ldots,x_n)$.
\end{tabular}

\begin{itemize}[noitemsep]
\item[(1)] Let $g=\{g_1,\ldots,g_m\}$,$g_i\in\mathbb{Z}[x_1,\ldots,x_n]$
be the polynomials occurring in the formula $P$.\\
Let $h=\{h_1,\ldots,h_{6m+2}\}$ where
\[\begin{array}{lclcr}
h_i        &=& g_i       && i=1,\ldots,m\\
h_{m+i}    &=& x_0g_i-1  && i=1,\ldots,m\\
h_{2m+i}   &=& x_0g_i+1  && i=1,\ldots,m\\
h_{3m+1}   &=& x_0-1     &&\\
h_{3m+1+i} &=& -g_i      && i=1,\ldots,m\\
h_{4m+1+i} &=& -x_0g_i+1 && i=1,\ldots,m\\
h_{5m+1+i} &=& -x_0g_i-1 && i=1,\ldots,m\\
h_{6m+2}   &=& -x_0+1    &&\\
\end{array}\]
\item[(2)] Let $d$ be the maximum of the degrees of the polynomials $g_i$\\
Let $d^\prime$ be the least even integer which is greater than or equal to
$d+1$.\\
Let $\tilde{h}=\{\tilde{h}_1,\ldots,\tilde{h}_{6m+2}\}$ where
\[\tilde{h}_i(\delta;x_0,\ldots,x_n)=(1-\delta)h_i+
\delta(1+\sum_{j=0}^n i^jx_j^{d^\prime})\]
\item[(3)] For each $A\subseteq\{1,\ldots,6m+2\}$ such that
$|A|\le n+1$ do:
\begin{itemize}[noitemsep]
\item[(3.1)] Let $\tilde{h}(x_0,\ldots,x_n)=
\sum_{j=0}^n(6m+3)^jx_j^{d^\prime}$\\
Let $M_A$ be the matrix with the last row $\nabla_{x_0,\ldots,x_n}\hat{h}$
and with earlier rows $\nabla_{x_0,\ldots,x_n}\overline{h}, ~i\in A$,
ordered by increasing indices $i$.
\item[(3.2)] Let $h_A(\delta;x_0,\ldots,x_n)=
{\rm det}(M_AM_A^T)+\sum_{i\in A}\overline{h}_i^2$\\
Let $d_A$ be the degree of $h_A$ with respect to $x_0,\ldots,x_n$
\item[(3.3)] Let $\tilde{h}_A(\epsilon,\delta;x_0,\ldots,x_n)=
(1-\epsilon)h_A-\epsilon\sum_{j=0}^nx_j^{d_A}$\\
Let $\tilde{h}_A^{(i)}=\frac{\partial\tilde{h}_A}{\partial x_i}$ for each $i$
\item[(3.4)] Let $R_A(\epsilon,\delta;u_0,\ldots,u_{n+1})$ be the
$u$-resultant of the polynomials $\tilde{h}_A^{(0)},\ldots,\tilde{h}_A^{(n)}$
(Use the subalgorithm shown below)
\item[(3.5)] Let $R_A$ be expanded as
$\sum_{i,j,k}\epsilon^i\delta^ju_0^kR_A^{<i,j,k>}$
\end{itemize}
\item[(4)] Let $\mathcal{R}$ be the set of all $\mathcal{R}_A^{<i,j,k>}$
computed in Step (3)
\item[(5)] Let $\mathcal{B}_{n+1,D}=
\{(i^{n-1},i^{n-2},\ldots,1,0)|0 \le i \le nD^2\}$\\
For $R\in\mathcal{R}$, let $D_R$ be the degree of $R$\\
Let $\mathcal{Q}=\{\frac{d^j}{dt^j}\nabla_{u_1,\ldots,u_{n+1}}
R(\beta+te_{n+1})|R\in\mathcal{R},\beta\in\mathcal{B}_{n+1,D_R},
j\in[0,D_R]\}$
\item[(6)] Let $G=\{G_1,\ldots,G_m\}$,~$G_i\in\mathbb{Z}[x_1,\ldots,x_{n+1}]$
be such that each $G_i$ is the degree $d$-homogenization of $g_i$, i.e.
the monomials of $G_i$ are obtained from those of $g_i$ by multiplying by
the appropriate powers of $x_{n+1}$ so as to become of degree $d$\\
Let $\mathcal{F}^+=\{(G_1(q),\ldots,G_m(q),q_{n+1}|q\in\mathcal{Q}\}$\\
Let $\mathcal{F}^-=\{(G_1(-q),\ldots,G_m(-q),-q_{n+1}|q\in\mathcal{Q}\}$\\
Let $\mathcal{F}=\mathcal{F}^+\cup\mathcal{F}^-$
\item[(7)] For each $f\in\mathcal{F}$ let $\overline{S}_f$ be the set of
all consistent sign vectors for the polynomials in $f$. (Use the algorithm
of Ben-Or, Kozen, and Reif\cite{Beno86})
\item[(8)] Let $\overline{S}=\bigcup_{f\in\mathcal{F}}\overline{S}_f$\\
Let $S=\{(\sigma_1,\ldots,\sigma_m)|(\sigma_1,\ldots,\sigma_m,1)\in
\overline{S}\}$\\
Finally let $t=\bigvee_{\sigma\in S}P_\sigma$, where the formula
$P_\sigma$ is obtained from the formula $P$ by replacing $g_i$ with
$\sigma_i$
$\blacksquare$
\end{itemize}

\vskip 0.5cm
\noindent
\begin{tabular}{ll}
{\sl Input}: & $f_0,\ldots,f_n\in W[x_0,\ldots,x_n]$, where $W$
is a commutatuve ring with unity.\\
{\sl Output}: & $R\in W[u_0,\ldots,u_{n+1}]$ is the $u$-resultant of
$f_0,\ldots,f_n$.\\
& (See Renegar\cite{Rene92,Rene92a,Rene92b} for the definition)
\end{tabular}

\begin{itemize}[noitemsep]
\item[(1)] Let $\tilde{d}$ be the maximum of the degrees of $f_i$\\
Let $\mathbb{B}=\{x_0^{d_0}\cdots x_{n+1}^{d_{n+1}}|
d_0+\cdots+d_{n+1}=\hat{d}\}$ where $\hat{d}=(n+1)(\tilde{d}-1)+1$\\
Let $\mathbb{H}$ be the vector space generated by the basis $\mathbb{B}$
over the ring $W$
\item[(2)] For each $x_0^{d_0}\cdots x_{n+1}^{d_{n+1}} \in \mathbb{B}$,
let $i$ denote the least index $i\le n$ and $\tilde{d}\le d_i$, if such
an $i$ exists, otherwise let $i=n+1$\\
Let $F_i\in W[x_0,\ldots,x_{n+1}]$ be the degreee $\tilde{d}$-homogenization
of $f_i$ for each $i$\\
Let $\mathcal{T}:\mathbb{H}\longrightarrow\mathbb{H}$ be the linear
transformation defined by the following mapping on the basis $\mathbb{B}$
\[\mathcal{T}(x_0^{d_0}\cdots x_{n+1}^{d_{n+1}})=
\left\{
\begin{array}{ll}
x_0^{d_0}\cdots x_i^{d_i-\tilde{d}}\cdots x_{n+1}^{d_{n+1}}F_i &
{\rm if\ }i\le n\\
x_0^{d_0}\cdots x_n^{d_n}x_{n+1}^{d_{n+1}-1}u\cdot x &
{\rm if\ }i = n+1
\end{array}
\right.\]
where $u=(u_0,\ldots,u_{n+1})$ and $x=(x_0,\ldots,x_{n+1})$\\
Let $M$ be the matrix representing $\mathcal{T}$ with respect to the
basis $\mathbb{B}$
\item[(3)] Finally set $R=D!{\rm det}(M)$ where $m$ is a $D\times D$
matrix.$\blacksquare$
\end{itemize}

In \cite{Rene92} it is shown that the worst case time complexity of this
algorithm is dominated by $L(log~L)(log~log~L)(md)^{O(n)}$,
which is the best compared to the theoretical complexities of all other
algorithms proposed in the literature so far.

Renegar\cite{Rene92}, by generalizing the above algorithm, gave a
quantifier elimination algorithm for the first order theory of the reals.

\section{Special Methods}

During last several years, there have been efforts for developing
methods that work on a restricted class of inputs. Such research is
motivated by the observation that one can identify interesting and
useful sub-class of problems and that one might be able to develop
more efficient (in practice) methods for them.

\subsection{Low Degrees}

Weispfenning\cite{Weis88}\cite{Weis94}\cite{Weis97},
Loos\cite{Loos93}, Anai\cite{Anai00} and others devised methods for
formulas where the bound variables occur in low degrees (1,2, and 3).

\subsubsection{Restricted Problem Class}

A formula with one quantifier is called $degree~n$ if the bound variable
occur in degree at most $n$. We would like to find methods for formulas
with small degrees.

For formulas with many quantifiers, we can repeatedly apply such a
method to eliminate all the quantifiers one at a time, starting from
the innermost ones, provided that the result of each quantifier
elimination stays degree at most $n$ (in the remaining bound variables).

\subsubsection{Method}

In order to get the main intuition, let us consider the case when
there are no free variables. The initial idea is similar to that of
the Cylindrical Algebraic Decomposition. 

Let $\alpha_1 < \cdots < \alpha_\ell$ be the
real roots of the polynomials occurring in the quantifier-free matrix
$F$. Then the sign of each polynomial (and thus the truth of the
sentence) is constant throughout each cell:
\[(-\infty,\alpha_1),[\alpha_1,\alpha_1],(\alpha_1,\alpha_2),\ldots,
(\alpha_{\ell-1}),[\alpha_\ell,\alpha_\ell],(\alpha_\ell,\infty)\]
Thus, we only need to ``sample'' one point from each cell. For instance,
we could choose
\[S=\{\alpha_i|1\le i\le \ell\}\cup\{\alpha_\ell+1,\alpha_1-1\}\cup
\{\frac{1}{2}(\alpha_i+\alpha_{i+1})|1\le i\le \ell\}\]
Then we have
\[\exists x F(x) \Longleftrightarrow \bigvee_{t\in S}F(t)\]
\[\forall x F(x) \Longleftrightarrow \bigwedge_{t\in S}F(t)\]
Thus, the finite set $S$ contains sufficiently many samples from an
infinite set $\mathbb{R}$ for the purpose of deciding the sentences.
Let us call such a set the {\sl sample set}.

This method is generalizable in a obvious way to the case with free
variables, but not {\sl efficiently}, because when there are free
variables, the order of the roots depend on the values of the free
variables and we will have anticipate all the ``potential'' orders,
blowing up the size of the output formula.

A better approach (taken by Weispfenning) is to utilize the symbolic
devices such as infinities and (positive) infinitesimals: $\infty$ and
$\epsilon$. Using these, one can see immediately the following set also 
forms a sample set (though a rigorous proof will require either the 
tedious $\epsilon - \delta$ reasoning or the non-standard analysis):
\[S=\{\alpha_i~|~1\le i \le \ell\}\cup\{-\infty\}\cup
\{\alpha_i+\epsilon~|~1\le i\le \ell\}\]

Note that the resulting formula is quantifier-free but it instead
contains other new symbols such as $\infty$, and the ``roots''
$\alpha_i$. Now, we
need to eliminate them, thus the quantifier elimination problem is
reduced to infinity elimination, infinitesimal elimination, and root
elimination. Clearly it suffices to show how to eliminate those
symbols from each atomic formula.

\vskip 0.5cm
\noindent
{\bf Infinity Elimination}: Let $f_n(x)=a_nx^n+\cdots+a_0$ be a polynomial
in an atomic formula. Then from the intended meaning of $-\infty$, the
correctness of the following rewrite rules is immediate:
\[\begin{array}{rcl}
f_n(-\infty)=0 & \rightarrow & a_n=0 \land \cdots \land a_0=0,\\
f_n(-\infty)<0 & \rightarrow & 
(-1)^na_n < 0 \lor [a_n=0 \land f_{n-1}(-\infty)<0]
\end{array}\]
For the case of $<$, we only need to apply the rule repeatedly until $n$
becomes 1. Atomic formulas with other relational operators can be
rewritten to formulas involving only $=$ and $<$ to which the above rules
can be applied.

\vskip 0.5cm
\noindent
{\bf Infinitesimal Elimination}:
Let $f(x)=a_nx^n+\cdots+a_0$ be a polynomial in an atomic formula.
From the intended meaning of $\epsilon$, we have the followings (assuming
that $f$ is a non-zero polynomial):
\begin{itemize}
\item $f(\alpha + \epsilon) \ne 0$
\item The sign of $f(\alpha + \epsilon)$ is the same as that of the
highest non-vanishing derivative of $f$ an $\alpha$.
\end{itemize}
From these observations, we immediately obtain the following rewrite rules:
\[\begin{array}{rcl}
f(\alpha+\epsilon)=0 & \rightarrow & a_n=0 \land\cdots\land a_0=0\\
f(\alpha+\epsilon)<0 & \rightarrow & f(\alpha)<0 \lor
[f(\alpha)=0 \land f^\prime(\alpha+\epsilon)<0]
\end{array}\]
For the case of $<$, we only need to apply the rule repeatedly until the
degree of $f$ becomes 1.

\vskip 0.5cm
\noindent
{\bf Linear Root Elimination}: Let $f$ be a polynomial in $x$. Let $\alpha$
be a real root of a linear polynomial $a_1x+a_0=0$, thus $\alpha=-a_0/a_1$.
We would like to eliminate the root (or root term) from the atomic formula
$f(-a_0/a_1)~\rho~0$ where $\rho$ is a relational operator. Essentially we
would like to eliminate the division symbol: $/$. One could immediately
think of the following rewrite rule:
\[\begin{array}{rcl}
f(b/a)=0 & \rightarrow & a^nf(b/a)=0\\
f(b/a)<0 & \rightarrow & a^{n+\delta}f(b/a)<0
\end{array}\]
where $n$ is the formal degree of $f$, $\delta$ is its parity, 
and $a^kf(b/a)$
stands for the polynomial obtained by canceling out the denominators
of $f(b/a)$ by multiplying with $a^k$. But this is not correct. The
denominator $a$ might vanish for some values of the free variables, thus
making the root undefined. But it also means that it is not a root for
the specific values of the free variables. Thus, we could simply
ignore it. But it can happen that all roots are such. In that case, we
should not ignore all of them, since we need to have at least one
sample point. One solution is to view such a case (vanishing
denominator) as always supplying the sample point 0.  The following
rewrite rules implement these ideas.\footnote{At the time of writing
this tutorial, I notice that it is not actually necessary to duplicate
0 so many times. We need only one 0 to ensure that there is at least one
sample point. Further $a\ne 0$ can be ``factored out''. So we can go back
to the ``incorrect'' rewrite rules:
\[\begin{array}{rcl}
f(b/a)=0 & \rightarrow & a^nf(b/a)=0\\
f(b/a)<0 & \rightarrow & a^{n+\delta}f(b/a)<0
\end{array}\]
provided that we use
\[\exists x F \Longleftrightarrow F(0) \lor \bigvee_{b/a\in S}
a\ne 0 \land F(b/a)\]}
\[\begin{array}{rcl}
f(b/a)=0 & \rightarrow & [a=0 \land f(0)=0]\lor
[a\ne 0 \land b^nf(b/a)=0]\\
f(b/a)<0 & \rightarrow & [a=0 \land f(0)<0]\lor
[a\ne 0 \land b^{n+\delta}f(b/a)<0]
\end{array}\]

\vskip 0.5cm
\noindent
{\bf Quadratic Root Elimination}: Let $f$ be a polynomial in $x$.
Let $\alpha$ be a real root of a quadratic polynomial
$a_2x^2 + a_1x + a_0=0$, thus $\alpha$ is one of
$\frac{-a_1\pm \sqrt{a_1^2-a_2a_0}}{2a_2}$. We would like to eliminate the
root from the atomic formula
$f(\frac{-a_1\pm \sqrt{a_1^2-a_2a_0}}{2a_2})~\rho~0$ where $\rho$ is a
relational operator. Essentially we would like to eliminate the symbols:
$\sqrt{\phantom{x}}$ and $/$. The following observation helps.
\[\displaystyle
\frac{a+b\sqrt{c}}{d}+\frac{a^\prime+b^\prime\sqrt{c^\prime}}{d^\prime} =
\frac{(ad^\prime+a^\prime d)+(bd^\prime+b^\prime d)\sqrt{c}}{dd^\prime}\]
\[\displaystyle
\frac{a+b\sqrt{c}}{d}\times\frac{a^\prime+b^\prime\sqrt{c^\prime}}{d^\prime} =
\frac{(aa^\prime+bb^\prime c)+(ab^\prime+a^\prime b)\sqrt{c}}{dd^\prime}\]

By applying these equality repeatedly,
$f(\frac{a+b\sqrt{c}}{d})$ results in the same form:
\[\frac{a^*+b^*\sqrt{c^*}}{d^*}\]
Note that $d^*=d^k$ where $k$ is the
formal degree of $f$. Let $\delta$ be the parity of $k$. Then we can rewrite:
\[\begin{array}{rcl}
\displaystyle
f(\frac{a+b\sqrt{c}}{d})=0 &\rightarrow& a^{*^2}-b^{*^2}c=0 \land a^*b^*\le 0\\
&&\\
\displaystyle
f(\frac{a+b\sqrt{c}}{d})<0 & \rightarrow &
[[a^*d^\delta\le 0 \land a^{*^2}-b^{*^2}c\ge 0]\lor b^*d^\delta\le 0]\land\\
&&\\
\displaystyle
&&[a^*d^\delta \le 0 \lor a^{*^2}-b^{*^2}c\le 0]
\end{array}\]

Again we need to take care of the ``bad'' situations such as $d=0$ or
$c < 0$. This can be done by doing some case analysis, and this will
increase the size of the formula a bit.

\vskip 0.5cm
\noindent
{\bf Improvements} In \cite{Loos93}, 
several optimizations are described, such as
reducing the size of the sample set by utilizing the relational operators,
the boolean structure, the quantification structure, etc.

\subsection{Constrained by Quadratic Equation}

Hong\cite{Hong93}\cite{Hong93a}
devised an algorithm that eliminates a quantifier
from a formula which is {\sl constrained by a quadratic equation}. The
output formulas are made of resultants and their variants called {\sl slope}
resultants.  The slope resultants can be, like the resultants,
expressed as determinants of certain matrices.

Weispfenning\cite{Weis94} gave a method (as described in the previous
section) which handles this case (and more). The method there does not
require extended resultant calculus. The outputs are also very similar
except that the method of Weispfenning systematically introduces some
extraneous factors in the output polynomials.  In fact, the initial
motivation for developing the variant resultant calculus was to avoid
the introduction of extraneous factors. Further the variant resultant
calculus has some nice properties and might provide some new way to
higher degrees.

\subsubsection{Restricted Problem Class}

A formula is said to be {\sl constrained by a quadratic equation} if it is
of the following form:
\[(\exists x \in \mathbb{R})[a_2x^2+a_1x+a_0=0 \land F]\]
where
\begin{itemize}
\item $F$ is a quantifier free formula in $x_1,\ldots,x_r,x$
\item $a_2,a_1,a_0$ are polynomials over $x_1,\ldots,x_r$ such that
$a_2,a_1$ and $a_0$ do not have a common real zero in $\mathbb{R}^r$.
\end{itemize}

\subsubsection{Mathematical Tool: Slope Resultants}

Let us first define a variant of resultant, which we call {\sl slope}
resultant. Later we will use this while developing a quantifier
elimination algorithm for the problem stated above.

Let $I$ be an integral domain, and let $\tilde{I}$ be the unique (up to
isomorphism) algebraic closure of the quotient field of $I$.

\vskip 0.5cm
\noindent
{\bf Defintion 2 (Slope)} {\sl Let P be a univariate polynomial over I.
The $k$-th slope of $P$, written as $P^{<k>}$, for $k\ge 0$,
is the $(k+1)$-variate polynomial over $I$ defined recursively by}
\[\begin{array}{rcl}
P^{<0>}(x_1) &=& P(x_1)\\
P^{<k>}(x_1,\ldots,x_{k+1}) &=& \displaystyle
\frac{P^{<k-1>}(x_1,\ldots,x_k)-P^{<k-1>}(x_2,\ldots,x_{k+1})}
{x_1-x_{k+1}}
\end{array}\]
{\sl for\ }$k\ge 1$.$\blacksquare$

Though the definition of slope involves rational functions, the
divisions are always exact, and thus a slope is a polynomial. We keep
the rational function formulation in this definition because it is
more natural and intuitive.

\vskip 0.5cm
\noindent
{\bf Definition 3 (Slope Resultant)} {\sl Let A and B be univariate
polynomials over I.\\
Let $A=a_m\prod_{i=1}^m(x-\alpha_i)$ where
$\alpha_i\in\tilde{I}$. Then the k-th slope resultant of A and B,
written as ${\rm sres}_k(A,B)$, is defined by}
\[{\rm sres}_k(A,B)=a_m^{n-k}
\sum_{1\le i_1 <\cdots < i_{k+1} \le m} B^{<k>}
(\alpha_{i_1},\ldots\alpha_{i_{k+1}})\quad\blacksquare\]

Intuitively this is the average of the $k$-th ``derivative'' of $B$
at the roots of $A$, up to a constant factor.

The slope resultant can be computed in various different ways:
recurrence formula, generating functions, and determinants of certain
matrix \cite{Hong98a}\cite{Hong93a}. 
Here we only give the determinant method.

\vskip 0.5cm
\noindent
{\bf Theorem 2 (Slope Resultant as Determinant)}
{\sl Let $A=\sum_{i=0}^m a_ix^i$ and $B=\sum_{i=0}^n b_ix^i$. Then we have}
\[{\rm sres}_k(A,B)={\rm det}(M)\]
{\sl where $M$ is the $n+1-k$ by $n+1-k$ matrix defined by}
\[M=\left[
\begin{array}{cccccc}
a_m & a_{m-1} & \cdots & \cdots  & \cdots  &\\
    & \cdots  & \cdots & \cdots  & \cdots  & \cdots\\
    &         &  a_m   & a_{m-1} & a_{m-2} & c_3^{k+1}\cdot a_{m-3}\\
    &         &        &  a_m    & a_{m-1} & c_2^{k+1}\cdot a_{m-2}\\
    &         &        &         &  a_m    & c_1^{k+1}\cdot a_{m-1}\\
b_n & b_{n-1} & \cdots & \cdots  & b_{k+1} & c_m^{k+1}\cdot b_k\\
\end{array}
\right]\]
{\sl where $c_i^k={m \choose k}-{{m-i} \choose k}$. Let
$n^\prime = n + 1 - k$. Precisely, the matrix is defined by}
\[M_{i,j}=
\left\{\begin{array}{lr}
a_{m-(j-i)} & {\rm if}~i < n^\prime ~{\rm and}~ j < n^\prime\\
&\\
b_{n-j+1}   & {\rm if}~i = n^\prime ~{\rm and}~ j < n^\prime\\
&\\
c_{j-i}^{k+1}a_{m-(j-i)}  & {\rm if}~i < n^\prime ~{\rm and}~ j = n^\prime\\
&\\
c_m^{k+1}b_k   & {\rm if}~i = n^\prime ~{\rm and}~ j = n^\prime\\
\end{array}\right.\]
{\sl where $a_\mu=0$ if $\mu > m$ or $\mu < 0$ and $b_\nu=0$ if
$\nu > n$ or $\nu < 0$}\quad$\blacksquare$

\subsubsection{Method}

Now we are ready to give a quantifier elimination algorithm that
utilizes the slope resultants. Without losing generality, let us
assume that the quantifier free forula $F$ involves only the two
relation operators: $=$ and $>$. We can decompose the input formula $F^*$
into two sub-problems in an obvious way.
\[F^* \Longleftrightarrow [a_2=0 \land F_1^*] \lor F_2^*\]
where
\[\begin{array}{rcl}
F_1^* & \equiv & a_1\ne 0 \land (\exists x)[a_1x+a_0=0 \land F]\\
F_2^* & \equiv & a_2\ne 0 \land (\exists x)[a_2x^2+a_1x+a_0=0 \land F]
\end{array}\]

Thus the problem is reduced into two quantifier elimination problems:
one for $F_1^*$ and the other for $F_2^*$ .  One way for solving these
problems might be to put the ``parametric'' roots (expressions) into $F$
and eliminate them by rewriting. (as we have seen in the previous
section on Linear Quantifier Elimination). But the following two
theorems show us that we can bypass these two steps by utlizing the
resultants and the slope resultants.

\vskip 0.5cm
\noindent
{\bf Theorem 3 (Linear Case)}
{\sl Let $F^\prime$ be the quantifier free formula in the variables
$x_1,\ldots,x_r$ defined recursively by}
\[F^\prime \equiv
\left\{
\begin{array}{ll}
R=0 & {\rm if\ }F\equiv B=0\\
R>0 & {\rm if\ }F\equiv B > 0, {\rm deg}_x(B){\rm\ is\ even}\\
a_1R>0 & {\rm if\ }F\equiv B>0,{\rm deg}_x(B){\rm\ is\ odd}\\
F_1^\prime \lor F_2^\prime & {\rm if\ }F\equiv F_1 \lor F_2\\
F_1^\prime \land F_2^\prime & {\rm if\ }F\equiv F_1 \land F_2\\
\lnot F_1^\prime & {\rm if\ }F\equiv \lnot F_1
\end{array}\right.\]
{\sl where $R={\rm res}(a_1x+a_0,B)$. Then we have}
\[F_1^* \Longleftrightarrow a_1\ne 0 \land F^\prime\quad\blacksquare\]

The following theorem shows a way to eliminate the existential
quantifier from the formula $F_2^*$.

\vskip 0.5cm
\noindent
{\bf Theorem 4 (Quadratic Case)}
{\sl Let $F^{(1)}$ be the quantifier free formula in the variables
$x_1,\ldots,x_r$ defined recursively by}
\[F^{(1)}\equiv
\left\{\begin{array}{ll}
R=0 \land TS \le 0 & {\rm if\ }F\equiv B=0\\
R>0 \land T > 0 ~\lor &\\
\quad R<0 \land S > 0 ~\lor &\\
\quad T > 0 \land S > 0 & {\rm if\ }F \equiv B > 0,\\
&\quad {\rm deg}_x(B){\rm\ is\ even}\\
a_2 R>0 \land a_2 T > 0 ~\lor&\\
\quad a_2R < 0 \land a_2S > 0 ~\lor&\\
\quad a_2T > 0 \land a_2S > 0 & {\rm if\ }F \equiv B > 0\\
& \quad {\rm deg}_x(B){\rm\ is\ odd}\\
F_1^{(1)} \lor F_2^{(1)} & {\rm if\ }F \equiv F_1 \lor F_2\\
F_1^{(1)} \land F_2^{(1)}  & {\rm if\ }F \equiv F_1 \land F_2\\
\lnot F_1^{(1)} & {\rm if\ }F \equiv \lnot F_1
\end{array}\right.\]
{\sl where}
\[\begin{array}{rcl}
R & = & {\rm res}(a_2x^2 + a_1x + a_0,B)\\
T & = & {\rm sres}_0(a_2x^2 + a_1x + a_0,B)\\
S & = & {\rm sres}_1(a_2x^2 + a_1x + a_0,B)\\
\end{array}\]

{\sl Let $F^{(2)}$ be the quantifier formula in the variables
$x_1,\ldots x_r$ defined in the same way as $F^{(1)}$, except that
$S$, $F_1^{(1)}$, and $F_2^{(1)}$ are replaced by
$-S$, $F_1^{(2)}$, and $F_2^{(2)}$. Then we have}
\[F_2^* \Longleftrightarrow a_2\ne 0 \land a_1^2-4a_2a_0 \ge 0 \land
[F^{(1)} \lor F^{(2)}]\quad\blacksquare\]

We summarize the above results in the following algorithm.

\vskip 0.5cm
\noindent
{\bf Algorithm 3 (Quantifier Elimination)}
\vskip 0.2cm
\noindent
\begin{tabular}{ll}
{\sl Input\ \ }: & A formula $F^*$ of the form
\end{tabular}
\[(\exists x \in \mathbb{R})[a_2x^2 +a_1x +a_0 = 0 \land F]\]
\begin{tabular}{ll}
&where $F$ is a quantifier free formula in $x_1,\ldots,x_r,x$ and\\
&$a_2,a_1,a_0$ are polynomials over $x_1,\ldots,x_r$ such that\\
&$a_2,a_1$ and $a_0$ do not have a common real zero in $\mathbb{R}^r$\\
{\sl Output}: & A quantifier-free formulat $\tilde{F}$ equivalent to $F$
\end{tabular}

\begin{itemize}[noitemsep]
\item[(1)] Apply on $F$ the recursive algorithm in Theorem 3, obtaining
a quantifier free formula $F^\prime$.
\item[(2)] Apply on $F$ the recursive algorithm in Theorem 4, obtaining
two quantifier free formulas $F^{(1)}$ and $F^{(2)}$
\item[(3)] Obtain $\tilde{F}$ by putting together $F^\prime$, $F^{(1)}$,
and $F^{(2)}$ as follows:
\[\begin{array}{rcl}
\tilde{F} & \equiv & a_2=0 \land a_1 \ne 0 \land F^\prime \lor\\
&&a_2 \ne 0 \land a_1^2-4a_2a_0 \ge 0 \land [F^{(1)} \lor F^{(2)}]
\quad\blacksquare
\end{array}\]
\end{itemize}

\vskip 0.5cm
\noindent
{\bf Performance}
If we allow the determinant symbol in the output, the computing time
of the algorithm is {\sl linear} in the length of the input. If not, the
computing time is dominated by
$N(n^{2r+1}\ell+n^{2r}\ell^2)$ where $N$ is the
number of polynomials in the input formula, $r$ is the number of
variables, $n$ is the maximum of the degrees for every variable, and $\ell$
is the maximum of the integer coefficient bit lengths. Experiments
with implementation suggest that the algorithm is sufficiently
efficient to be useful in practice.

\subsection{Single Atomic Formula}

Gonzales-Vega\cite{Gonz98} gave algorithms for formulas with single
atomic formula. The algorithms first compute a finite number of
polynomials and select among all the possible sign conditions over
these polynomials those making the considered formula true. The main
mathematical tool used is the Sturm-Habicht sequence\cite{Gonz89}
which is essentially a careful adaptation of Sturm's sequence to the
subresultant chain.

\subsubsection{Restricted Problem Class}

Let $P_n(\underbar{a},x)$ denote the polynomial
$x^n+a_{n-1}x^{n-1}+\ldots+a_1x+a_0$. Then we are interested in the
formulas of the following kind:
\[(Qx)P_n(\underbar{a},x)~\rho~0\]
where $Q$ is either $\forall$ or $\exists$, and $\rho$ is a relational
operator. One sees that the problem can be trivially reduced to the
following two types:
\[\begin{array}{rcl}
{\bf Type1}: & \exists x & P_n(\underbar{a},x) = 0\\
{\bf Type2}: & \exists x & P_n(\underbar{a},x) < 0\\
\end{array}\]
for even $n$.

\subsubsection{Mathematical Tool: Sturm-Habicht}

Let $\mathbb{D}$ be an integral domain

\vskip 0.2cm
\noindent
{\bf Definition 4}.
{\sl Let $P=\sum_{k=0}^p a_kx^k$ and $Q=\sum_{k=0}^q b_kx^k$ be polynomials
in $\mathbb{D}[x]$ with {\rm deg}$(P)\le p$ and 
{\rm deg}$(Q)\le q$. The $i$-th formal
principal subresultant coefficient.} {\bf sres}$_i(P,p,Q,q)$, {\sl is
the determinant of the following matrix:}

\[\begin{array}{cl}

\overbrace{
\begin{array}{ccccc}
& \quad\quad & \quad\quad & \quad\quad & \quad\quad \\
\end{array}}^{p+q-2i} &\\

\left(\begin{array}{ccccc}
a_p & a_{p-1} & \cdots &        &\\
    & \ddots  &        & \ddots  &\\
    &         &   a_p  & a_{p-1} & \cdots\\
b_1 & b_{q-1} & \cdots &         &\\
    & \ddots  &        & \cdots  &\\
    &         &   b_q  & b_{q-1} & \cdots\\
\end{array}\right) 
&
\begin{array}{l}
\left.
\begin{array}{l}
\\
\\
\\
\end{array}\right\} 
q-i \\
\left.
\begin{array}{l}
\\
\\
\\
\end{array}\right\} 
p-i
\end{array}
\end{array}\]
\noindent\quad$\blacksquare$

This is essentially the same as the usual definition of the principal
subresultant coefficients (used in the projection of the Cylindrical
Algebraic Decomposition method), except that it allows {\sl formal} leading
coefficients, that is, deg$(P)$ and deg$(Q)$ do not have to be exactly $p$
and $q$.

\vskip 0.5cm
\noindent
{\bf Definition 5} {\sl Let $P,Q$ be polynomials in $\mathbb{D}[x]$.
Let $v={\rm deg}(P)+{\rm deg}(Q)-1$. The principal $i$-th Sturm-Habicht
coefficient,} {\bf stha}$_i(P,Q)$, {\sl is defined by}
\[{\rm {\bf stha}}_i(P,Q)=(-1)^{(v-i)(v-i+1)/2}
{\rm {\bf sres}}_i(P,v+1,P^\prime Q,v)\quad\blacksquare\]

The {\bf stha} has a nice {\sl specialization property}. The $\phi$
stand for the specialization of $a_i$ and $b_i$ to some real numbers
such that the degrees stay the same. Then we obviously have the following:
\[\phi({\rm {\bf stha}}_i(P,Q))={\rm {\bf stha}}_i(\phi(P),\phi(Q))\]
that is, the {\bf stha} commutes with specialization.

\vskip 0.2cm
\noindent
{\bf Definition 6} {\sl Let $a_0,a_1,\ldots,a_n$ be a list of non zero
elements in $\mathbb{R}$. We define:
\begin{itemize}[noitemsep]
\item {\bf P}($a_0,a_1,\ldots,a_n$) as the number of sign permanence in
the list $a_0,a_1,\ldots,a_n$
\item {\bf V}($a_0,a_1,\ldots,a_n$) as the number of sign variations in
the list $a_0,a_1,\ldots,a_n$\quad$\blacksquare$
\end{itemize}}

\vskip 0.2cm
\noindent
{\bf Definition 7} {\sl Let $a_0,a_1,\ldots,a_n$ be elements in $\mathbb{R}$
with $a_0\ne 0$. Suppose that it is made of $A_1,Z_1,A_2,Z_2,\ldots,A_t,Z_t$
where each $A_i$ is a sequence of non-zeros and $Z_i$ is a sequence of zeros.
Let $k_i$ be the length of $Z_i$. Let $h_i$ and $t_i$ be respectively, the
sign of the head and the tail element of $A_i$. Then we define}
\[{\rm {\bf C}}(a_0,a_1,\ldots,a_n)=\sum_{i=1}^t{\rm {\bf P}}(A_i)-
\sum_{i=1}^t{\rm {\bf V}}(A_i)+\sum_{i=1}^{t-1}\epsilon_i\]
{\sl where}
\[\epsilon_i=\left\{
\begin{array}{ll}
0 & {\rm if\ }k_i{\rm\ is\ odd}\\
(-1)^{\frac{k_i}{2}}t_ih_{i+1} & {\rm if\ }k_i{\rm\ is\ even}\quad\blacksquare
\end{array}\right.\]

\vskip 0.2cm
\noindent
{\bf Definition 8} {\sl Let $P$ and $Q$ be polynomials in $\mathbb{R}[x]$.
Then let}
\[c_\epsilon(P;Q)={\rm card}(\{\alpha \in \mathbb{R}|P(\alpha)=0,\quad
{\rm sign}(Q(\alpha))=\epsilon\})\]
{\sl where $\epsilon$ is a sign} ($+$,$-$,0).\quad$\blacksquare$

\vskip 0.2cm
\noindent
{\bf Theorem 5} {\sl Let $P$ and $Q$ be polynomials in $\mathbb{R}[x]$ with
{\rm deg}$(P)=p$. Then we have}
\[{\rm {\bf C}}({\rm {\bf stha}}_p(P,Q),\ldots,{\rm {\bf stha}}_0(P,Q))=
c_+(P;Q)-c_-(P;Q)\quad\blacksquare\]

\subsubsection{Method}

Let us tackle now the two types of quantifier elimination problems
mentioned above. From now, let
\[{\rm {\bf C}}(P,Q)={\rm {\bf C}}({\rm {\bf stha}}_p(P,Q),\ldots,
{\rm {\bf stha}}_0(P,Q))\]

\noindent
{\bf Type 1}
From the good specialization property of {\bf stha} and Theorem 5, 
we immediately obtain
\[\exists x P_n(\underbar{a},x)=0 \Longleftrightarrow
{\rm {\bf C}}(P_n,1)>0\]
Now we only need to ``expand'' the right hand side by checking all the 
$3^{n+1}$ possible sign conditions over the polynomials
{\bf stha}$_i(P_n,1)$ and keeping those making $C > 0$.

\vskip 0.5cm
\noindent
{\bf Type 2}
Let $I_n^i$ denote the formula stating that the polynomial $P_n(a,x)$
has a real root with multiplicity $i$.  Obviously we have
\[\exists x P_n(\underbar{a},x)<0 \Longleftrightarrow
\bigvee_{i=1,{\rm odd}}^{n-1} I_n^i(\underbar{a})\]
So we only need to find a quantifier-free formula for $I_n^i$. From
the definition of multiplicity, we immediately have
\[\begin{array}{rcl}
I_n^i(\underbar{a}) & \Longleftrightarrow &
\displaystyle
(\exists x)[P_n^{(0)}(\underbar{a},x)=
\ldots =P_n^{(i-1)}(\underbar{a},x)=0\\
&&\land~ P_n^{(i)}(\underbar{a},x)\ne 0]\\
 & \Longleftrightarrow &
\displaystyle
(\exists x)[\sum_{k=0}^{i-1}(P_n^{(k)}(\underbar{a},x))^2
 =0 \land P_n^{(i)}(\underbar{a},x)\ne 0]\\
 & \Longleftrightarrow &
\displaystyle
(\exists x)[\sum_{k=0}^{i-1}(P_n^{(k)}(\underbar{a},x))^2
 =0 \land (P_n^{(i)}(\underbar{a},x))^2 > 0]
\end{array}\]
Since square of real number is non-negative, 
we immediately see from Theorem 5 that
\[I_n^i(\underbar{a}) \Longleftrightarrow
\displaystyle
{\rm {\bf C}}(\sum_{k=0}^{i-1}(P_n^{(k)}(\underbar{a},x))^2,
(P_n^{(i)})^2)>0\]
Thus, finally we have
\[\exists x P_n(\underbar{a},x)<0 \Longleftrightarrow
\displaystyle
\bigvee_{i=1,{\rm odd}}^{n-1}
\displaystyle
{\rm {\bf C}}(\sum_{k=0}^{i-1}(P_n^{(k)}(\underbar{a},x))^2,
(P_n^{(i)})^2)>0\]
Again we will have to expand the right hand side by checking all the
possible sign conditions over the {\bf stha} polynomials and keeping
those making ${\bf C}>0$.

\section{Approximate Methods}

By now, it is well understood both theoretically and experimentally
that the full exact quantifier elimination is intrinsically difficult,
posing a formidable barrier to efforts for devising general but
efficient methods. One response to this is specialization, as we have
surveyed in the previous chapter. In this chapter, we see another
response: {\sl approximation}.

In many applications (in particular in science and engineering),
an approximate answer is acceptable. In fact, sometimes, it is even
meaningless to think of exact answers because the input itself often
contain non-exactness.

This is a good news for algorithm designers, since experience shows
that the enormous complexity of quantifier elimination mainly stems
from the insistence on exactness. If we are willing to compromise
this, then we might be able to devise methods that can handle large
problems arising from real-life applications.

This does not mean that approximation will give a better asymptotic
worst case bound. It might be the case that the approximated methods
have the same bound, or even worse. But there is evidence that the
approximated methods are much more efficient for a large class of
moderate size inputs.

The word ``approximate'' carries different meanings in different
contexts. In this chapter, we survey two different interpretations and
methods for them: {\sl generic} and {\sl volume-approximate}.

\subsection{Generic Quantifier Elimination}

One often observes that exact algorithms spend most of their computing
time in analyzing and taking care of non-generic cases. But the
end-users of the methods are often interested only in generic
(regular) cases. In such situations, the exact methods wasted
computing resource to produce unnecessary information. It will be
better to have a method that works only on generic cases.

This observation lead Hong to interpret approximatedness as
being {\sl correct except for some non- generic cases}. The non-generic
cases usually form a measure-zero set, and thus generically correct
answers are different from the exact answers only by measure zero
sets. Thus, it can be also viewed as being {\sl correct up to measure zero}.

\subsubsection{Problem}

We begin by giving a precise statement of the generic quantifier
elimination problem. 
Let $F(x_1,\ldots,x_f)$ be a formula. Let $S(F)$
stand for the solution set of $F$, that is, the set\\
$\{p\in\mathbb{R}^f~|~F(p){\rm\ is\ true}\}$.
We will define two notions: {\sl generic quantifiers} and
{\sl generic equivalence}.

\vskip 0.5cm
\noindent
{\bf Definition 9 (Generic Quantifiers)}
{\sl Let $F$ be a formula with one free variable $x$.}
\begin{itemize}[noitemsep]
\item {\sl The {\rm generic universal quantifier}, $\dot{\forall}$,
is defined by: $\dot{\forall} x F(x)$ is true iff $S(\lnot F)$ is of
measure zero}
\item {\sl The {\rm generic existential quantifier}, $\dot{\exists}$,
is defined by: $\dot{\exists} x F(x)$ is true iff $S(F)$ is of
measure zero}\quad$\blacksquare$
\end{itemize}
Intuitively, the generic universal quantifier can be read as ``for
almost all'', and the generic existential quantifier can be be read as
``for sufficiently many''. As expected, the following properties hold
for the generic quantifiers.

\vskip 0.5cm
\noindent
{\bf Proposition 2 (Commutativity)}
\begin{itemize}[noitemsep]
\item $\dot{\forall}x\dot{\forall}y F(x,y) \Longleftrightarrow
\dot{\forall} y\dot{\forall} x F(x,y)$
\item $\dot{\exists}x\dot{\exists}y F(x,y) \Longleftrightarrow
\dot{\exists} y\dot{\exists} x F(x,y)\quad\blacksquare$
\end{itemize}

\vskip 0.5cm
\noindent
{\bf Proposition 3 (Negation)}
\begin{itemize}[noitemsep]
\item $\lnot\dot{\forall} x F(x) \Longleftrightarrow
\dot{\exists} x \lnot F(x)$
\item $\lnot\dot{\exists} x F(x) \Longleftrightarrow
\dot{\forall} x \lnot F(x)\quad\blacksquare$
\end{itemize}
The ``strength'' of the four quantifiers (two exact and two generic)
form the following spectrum.

\vskip 0.5cm
\noindent
{\bf Proposition 4 (Spectrum of quantification strength)}
\[\forall x F(x) \Longrightarrow
\dot{\forall} x F(x) \Longrightarrow
\dot{\exists} x F(x) \Longrightarrow
\exists x F(x)\quad\blacksquare\]


\vskip 0.5cm
\noindent
{\bf Definition 10 (Generic equivalence)}
{\sl Two formulas $F$ and $G$ with free variables
$x_1,\ldots,x_f$ are generically equivalent iff the sentence}
\[\dot{\forall}x_1,\ldots\dot{\forall}x_f
[~F(x_1,\ldots,x_f) \Longleftrightarrow G(x_1,\ldots,x_f)~]\]
{\sl is true}\quad$\blacksquare$

\vskip 0.2cm
\noindent
Intuitively, two formulas are generically equivalent if their solution
sets are ``almost the same''. Finally we are ready to state the generic
quantifier elimination problem.

\vskip 0.5cm
\noindent
{\bf Problem}: Device an algorithm with the specification:
\begin{itemize}[noitemsep]
\item[{\sl In:}] A formula $F(x_1,\ldots,x_f)$ involving only generic
quantifiers
\item[{\sl Out:}] A formula $G(x_1,\ldots,x_f)$ such that it involves
no (generic) quantifiers and $F$ and $G$ are generically equaivalent.
\quad$\blacksquare$
\end{itemize}

\subsubsection{Method}

A natural way is to take an existing exact algorithm and turn it into
a generic one. We will do so with the method of (partial) cylindrical
algebraic decomposition \cite{Coll75}\cite{Hong90a}\cite{Coll91}.

\vskip 0.5cm
\noindent
{\bf Idea 1: No algebraic numbers}.
Since the generic quantifier elimination ignores measure zero sets, we
do not need to construct stacks over even-indexed cells (because they
are of measure zero). This results in a significant reduction of
computing time since we can completely eliminate the heavy real
algebraic number computations (which are needed for working with even
indexed cells).

\vskip 0.5cm
\noindent
{\bf Idea 2: Smaller Projection}.
Further, the projection polynomials are non-zero throughout odd
indexed cells. Thus we can reduce the size of projection by removing
the polynomials that are put in there to ensure delineability over
even indexed cells. Precisely, we have the following:

\vskip 0.5cm
\noindent
{\bf Definition 11 (Generic Projection)}
{\sl Let $A$ be a finite subset of $\mathbb{Z}[x_1,\ldots,x_r]$.
The generic projection of $A$, written as {\rm GProj}$(A)$, is defined as:}
\[\begin{array}{rcl}
{\rm GProj}(A) & = & L \cup R \cup D\\
L & = & \{lc(a)~|~a\in A\}\\
R & = & \{{\rm psc}_0(a_1,a_2)~|~a_1,a_2\in A\}\\
D & = & \{{\rm psc}_0(a,a^\prime)~|~a\in A\}\quad\blacksquare
\end{array}\]

\vskip 0.5cm
\noindent
{\bf Theorem 6 (Generic Projectiong gives Delineability)}
{\sl Let $A$ be a finite subset of $\mathbb{Z}[x_1,\ldots,x_r]$, $r\ge 2$.
Let $c$ be a non-empty connected subset of $\mathbb{R}^{r-1}$ such that
for all $J \in {\rm GProj}(A)$ and for all $p \in c$ we have
$J(p)\ne 0$. Then the zero of $A$ delineates the cylinder over $c$.}
\quad$\blacksquare$

\vskip 0.5cm
\noindent
{\bf Idea 3: No square-free factorization needed}.
Stack construction consists of the following steps: 
\begin{itemize}[noitemsep]
\item[(1)] Evaluate the
projection polynomials on a sample sample, obtaining a set of
univariate polynomials. 
\item[(2)] Factorize the univariate polynomials,
obtaining square-free and relatively prime factors. 
\item[(3)] Isolate the
real roots of the univariate polynomials. 
\end{itemize}
The step (2) is often most
time-consuming. But in generic quantifier elimination, we can drop
this step since:
\begin{itemize}[noitemsep]
\item During the projection phase, the projection polynomials are
already made to be square-free and relatively prime. (If not, their
projection polynomials will identically vanish and provide no information).
\item Since the leading coefficients of the polynomials do not vanish on
the sample point, the evaluated univariate polynomials will also be
square-free and relatively prime.
\end{itemize}

\vskip 0.5cm
\noindent
{\bf Idea 4: Sturm sequence for free}.
Again utilizing the fact that the leading coefficients of the
polynomials do not vanish on the sample point, we can obtain a Sturm
sequences for them almost for free if we have computed the projection
polynomials by subresultant sequence or Sturm-Habicht sequence\cite{Gonz89}.
This is because the sequences have a good specialization property
when the leading coefficients do not vanish. Thus the obtained Sturm
sequence can be used for real root isolation.

Let us make this idea more precise for a subresultant sequence.
We first need some notation and notions.
\begin{itemize}[noitemsep]
\item Let $\mathbb{D}$ denote an integral domain. When necessary, it will
also be assumed that $\mathbb{D}$ is ordered.
\item Let $A_1,A_2\in\mathbb{D}[x]$ be such that deg$(A_1)=n_1$,
deg$(A_2)=n_2$, and $n_1 \ge n_2 \ge 0$.
\item A {\sl polynomial remainder sequence} (prs) of $A_1$ and $A_2$ is a
sequence $A_1,\ldots,A_r$ satisfying
\[e_iA_i = Q_iA_{i+1}+f_iA_{i+2}\]
where $A_i,Q_i\in\mathbb{D}[x]$, $e_i,f_i\in\mathbb{D}$,
${\rm deg}(A_{i+1}) > {\rm deg}(A_{i+2})$,
$A_{r+1}=0$ and $e_if_i\ne 0$.
\item A {\sl negative} prs of $A_1$ and $A_2$ is a prs such that
$e_if_i < 0$ for every $i$
\item A {\sl Sturm sequence} of $A\in\mathbb{D}[x]$ is a negative prs
of $A$ and $A^\prime$.
\item The $k$-th {\sl Sylvester matrix} of $A_1$ and $A_2$, $0 \le k \le n_2$,
is the matrix whose rows are the coefficients of the polynomials:
\[x^{n_2-1-k}A_1,\ldots,A_1,x^{n_1-1-k}A_2,\ldots,A_2\]
written in the basis:
$x^{n_1+n_2-1-k},\ldots,1$
\item The $k$-th {\sl subresultant} of $A_1$ and $A_2$, $0 \le k \le n_2$,
written as $S_k$, is defined by
\[S_k=\sum_{i=0}^k {\rm det}(M_k^{(i)})x^i\]
wher $M_k^{(i)}$ is the submatrix of the $k$-th Sylviester matrix of
$A_1$ and $A_2$ obtained by taking the first
$n_1+n_2-1-2k$ columns and the $(n_1+n_2-k-i)$-th column.
\item The sequence $A_1,A_2,S_{n_2-1},S_{n_3-1},\ldots,S_{n_{r-1}-1}$
is called the {\sl first kind subresultant} prs of $A_1$ and $A_2$
$\blacksquare$
\end{itemize}
Let $\phi$ stand for an evaluation homomorphism that preserves the
degree (that is, the leading coefficients do not vanish under it).
Let $A_1,A_2\in\mathbb{D}[y]$. The following theorem shows how to
obtain the first kind subresultant prs of $\phi(A_1)$ and $\phi(A_2)$
from that of $A_1$ and $A_2$.

\vskip 0.5cm
\noindent
{\bf Theorem 7 (Homomorphism on first kind)}
{\sl Let $A_1,A_2,\ldots,A_r$ be the first kind subresultant {\rm prs}
of $A_1$ and $A_2$. Let $\tilde{A}_1,\ldots,\tilde{A}_{\tilde{r}}$
be the sequence obtained from $\phi(A_1),\ldots,\phi(A_r)$ by deleting
the vanishing ones, and deleting the second one in case there are two of them
with the same degree. (At most two can be of the same degree.) More
specifically if ${\rm deg}(\phi(A_k)) = {\rm deg}(\phi(A_j))$ for
$i < j$, then we delete $\phi(A_j)$. Then the sequence is the first kind
subresultant {\rm prs} of $\phi(A_1)$ and $\phi(A_2)$\quad$\blacksquare$}

\vskip 0.5cm
Let $\tilde{A_1},\tilde{A_2},\ldots,\tilde{A}_r$ be the first kind
subresultant prs of $\tilde{A}$ and $\tilde{A}^\prime$, obtained as
described above. Let $\tilde{n}_i={\rm deg}(\tilde{A}_i)$,
$\delta_i=\tilde{n}_i-\tilde{n}_{i+1}$, and $\tilde{c}_i$ be the
leading coefficient of $\tilde{A}_i$. Then the following theorem shows
how to obtain a Sturm sequence of $\tilde{A}$ from the first kind
resultant prs of $\tilde{A}$ and $\tilde{A}^\prime$.

\vskip 0.5cm
\noindent
{\bf Theorem 8 (Sturm sequence from sres {\sl prs})} {\sl Let}
\[\begin{array}{rcl}
\lambda_i & = & {\rm the\ sign\ of\ }\tilde{c}_i{\rm\ for\ }
2 \le i \le r-1\\
\mu_i & = & \lambda_{i+1}^{\delta_i+1}{\rm\ for\ }
1 \le i \le r-2\\
\rho_1 & = & 1\\
\rho_i & = & \lambda_i^{\delta_{i-1}}\rho_{i-1}^{\delta_{i-1}-1}
{\rm\ for\ }2 \le i \le r-2\\
\nu_1 & = & 1\\
\nu_i & = & -\lambda_i(-\rho_i)^{\delta_i}
{\rm\ for\ }2 \le i \le r-2\\
\sigma_1 & = & 1\\
\sigma_2 & = & 1\\
\sigma_i & = & -\mu_{i-2}\nu_{i-2}\sigma_{i-2}{\rm\ for\ } 3 \le i \le r\\
\tilde{A}_i & = & \sigma_i\tilde{A}_i{\rm\ for\ }1 \le i \le r
\end{array}\]
{\sl Then $\tilde{A}_1,\ldots,\tilde{A}_r$ is a Sturm sequence of}
$\tilde{A}$\quad$\blacksquare$

Now we present an algorithm for generic quantifier elimination, based
on the above ideas. The general structure of the algorithm is based on
that of the partial cylindrical algebraic decomposition
\cite{Hong90a}\cite{Coll91}. We use some standard terminology from there.

\vskip 0.5cm
\noindent
{\bf Algorithm 4 (Generic Quantifier Elimination)}

\vskip 0.5cm
\noindent
\begin{tabular}{ll}
{\sl Input}: & A formula $F^\prime = (Q_{f+1}x_{f+1})\cdots
(Q_rx_r)F(x_1,\ldots,x_r)$\\
& where $Q_i$ is either $\dot{\forall}$
or $\dot{\exists}$ and $F$ is a quantifier free formula.\\
{\sl Output}: & A quantifier-free formula $G$ which is generically
equivalent to $F$.
\end{tabular}

\begin{itemize}[noitemsep]
\item[(1)] [Projection]\\
Factorize the polynomials occurring in the formula $M$.\\
For $k=1,\ldots,r$, let $P_k$ be the set of the $i$-level factors.\\
For $k=r,\ldots,2$ do
\begin{itemize}[noitemsep]
\item[(a)] $J \longleftarrow{\rm GProj}(P_k)$.
{\sl Use the first kind subresultant polynomial remainder sequence to
compute} psc's
\item[(b)] Factorize the polynomials in $J$
\item[(c)] For $i=1,\ldots,k-1$, append the $i$-level factors to $P_i$
if they are not already there.
\end{itemize}
\item[(2)] [Stack Construction]\\
Initialize the tree as a single node whose truth is ? and whose sample
point is ().\\
While there is a candidate node do
\begin{itemize}[noitemsep]
\item[(a)] Choose a candidate node $c$. Let $k$ be the level of the node
and let $s=(s_1,\ldots,s_k)$ be its sample point
\item[(b)] Evaluate the polynomials in $P_{k+1}$ on $s$ obtaining
univariate polynomials
\item[(c)] Obtain Sturm sequences of the polynomials of the univariate
polynomials by specializing the first kind subresultant polynomials
remainder sequences computed during projection.
\item[(d)] Using the Sturm sequences, find rational numbers
$t_1,\ldots,t_\ell$ such that\\
$t_1 < \alpha_1 < t_2 < \alpha_2 < \cdots < \alpha_{\ell-1} < t_\ell$ 
where $\alpha_1,\ldots,\alpha_{\ell-1}$ are
the real roots of the univariate polynomials
\item[(e)] Attach $\ell$ nodes, say, $c_1,\ldots,c_\ell$, to the node $c$
where the sample point of $c_i$ is set to be $(s_1,\ldots,s_k,t_i)$
\item[(f)] For each child $c_i$, determine the signs of the polynomials
in $P_{k+1}$ on it
\item[(g)] For each child $c_i$, try to determine its truth by using the
signs
\item[(h)] Determine the truth values of $c$ if possible, and if so, the
truth values of as many ancestors of $c$ as possible, removing from the
tree the subtree of each cell whose truth value is thus determined
\end{itemize}
\item[(3)] [Solution Formula Construction]\\
By using the method described in \cite{Hong98},
construct a quantifier free formula $G$ such that it is true on all the
true cells and false on all the false cells.\quad$\blacksquare$
\end{itemize}

\subsection{Volume Approximate Quantifier Elimination}

\noindent

Hong introduced the notion of volume approximate quantifier
elimination and a method for it. Let us first clarify what is meant by
``volume approximate'' quantifier elimination. It was further improved
by Andreas Neubacher and completed by Stefan Ratschan\cite{Rats02}.

\subsubsection{Problem}

We define the volume approximate quantifier elimination problem as
follows. Devise an algorithm for

\noindent
\begin{tabular}{ll}
{\bf In:} & $F$, a formula\\
& $\epsilon$, a positive real number\\
{\bf Out:} & $N$, a quantifier-free formula necessary to $F$\\
& $S$, a quantifier-free formula sufficient to $S$ such that
$V(N)-V(S)\le \epsilon$
\end{tabular}

\noindent
where $V(G)$ stands for the volume of the set defined by the formula
$G$. Usually, $N$ and $S$ are required to belong to a small subset of the
full language. For instance, they are required to be a disjunctive
normal form of linear inequalities, that is, a collection of convex
polytopes. Or one can put even stronger restriction on them to be a
disjunctive normal form of ``de-coupled'' linear inequalities, that is,
a collection of boxes aligned with the coordinate axis. Throughout
this section, we will restrict our discussion to the case when $N$ and $S$
are required to be a collection of closed boxes. Further, we will
require that the boxes do not overlap. We will call such a collection
a {\sl box-set}.

Note that a box can viewed as a set of points or a formula that
defines the set. We will use these two views interchangeably. Thus,
sometimes we will speak of ``union'' of boxes and some other times we
will speak of ``disjunction'' of boxes. Obviously these are the same
operations.

The boxes that we will use are closed ones. This causes some
complications in defining the notion of ``disjointness''. We will not go
through the technical details in this tutorial. Intuitively, we will
interpret that two boxes are disjoint if their intersection is either
empty or belongs to the boundaries.

\subsubsection{Method}

\noindent
One natural idea is to approximate each atomic formula by two
box-sets: one as a necessary condition, the other as a sufficient
condition. Then carry out the logical operations (disjunction,
quantification, etc) on the box-sets. These logical operations are
much easier, though not trivial, to do on box-sets than on arbitrary
formulas. Also they can be done {\sl exactly}.

A question naturally arises: How accurately should we approximate each
atomic formula? One can take an ``eager'' approach. That is to carry out
an {\sl error-analysis} to estimate a lower bound on the accuracy that will
ensure that, after the logical operations on the box-sets, the volume
difference is smaller than $\epsilon$. The advantage of this scheme is
that once the error-analysis is done, we need to carry out the
approximation of atomic formulas and the logical operations on them
only once. But there are two serious disadvantages/difficulties.
\begin{itemize}
\item The error-analysis expects and prepares for the ``worst-case''
situation. Thus, in average, it can induce much more work than
actually needed.
\item A reasonable error-analysis is very difficult to do for
quantifications. A natural (geometric) method gives a totally useless
bound (too big). One can give better analysis using a root-separation
theorem, gap theorem, etc. But such analysis is computationally too
costly.
\end{itemize}

\noindent 
A better way is to take the so-called ``lazy''
approach. This means to compute only when necessity arises.  In that
sense it is ``demand-driven''. Thus, at first, we carry out a very
``coarse'' box-approximation of atomic formulas. If, after logical
operations on the boxes, the volume difference is already less than
$\epsilon$, then we can stop. If not, we can refine the initial
box-approximation of atomic formulas and iterate the same process
until the volume difference is smaller than $\epsilon$.


Further, during the execution of the method, we would like to utilize
any intermediate information as soon as they become available, in
particular for pruning/reducing the ``search-space''. As an example,
consider the formula $F\equiv F_1\land F_2$. Now we would like to approximate
it by a necessary box-set $N$ and a sufficient box-set $S$. One elegant
way is to approximate $F_1$ and $F_2$ independently and and carry out the
conjunction on the resulting box-sets $N_1,S_1$ and $N_2,S_2$. But
here, the result of the approximation of $F_1$ is not used during the
approximation of $F_2$. This can cause useless work. For instance, it
might the case that $N_1$ is empty (that is false). Then obviously the
result $N$ is also false, no matter what $N_2$ is. Thus a better way is to
use $N_1$ and $S_1$ during the approximation of $F_2$.

Now we turn the ideas discussed above into algorithms. We will use the
following notations:
\begin{itemize}
\item[$+$] {\sl disjoint disjunction} (or {\sl sum}). $B_1+B_2$ is the
same as $B_1 \lor B_2$ except that it also states that $B_1$ and $B_2$
are disjoint. Theus, this operation can be trivially done by collecting
all the boxes in the two box-sets
\item[$\times$] {\sl independent conjunction} (or {\sl product}).
$B_1\times B_2$ is the same as $B_1 \land B_2$ except that it also states
that $B_1$ and $B_2$ are independent (they do not share variables). Thus,
it is also trivial to carry out.
\end{itemize}

A technical remark. Though in the end, we would like to obtain a
necessary box-set and a sufficient box, it is more
convenient/efficient to keep track of ``yes box-set'', ``unknown box
set'', ``no box-set'' where the ``yes box-set'' is inside the solution set
of $F$, the ``no box-set'' is outside the solution set of $F$, the
``unknown box-set'' is not known yet whether it is inside or
outside. These will be denoted as $B_y$, $B_u$, $B_n$ respectively.

\vskip 0.5cm
\noindent
{\bf Algorithm 5}

\vskip -0.2cm
\noindent
\hrulefill
\begin{center}
$(N,S) \longleftarrow VolumeApproxQE(F)$
\end{center}

\noindent
\begin{tabular}{ll}
{\sl Input}: & $F$ is a formula\\
{\sl Output}: & $N$ is a necessary condition of $F$ and $S$ is a sufficient
condition of $F$\\
& such that $V(N)-V(S)\le \epsilon$.
\end{tabular}

\begin{itemize}
\item[(1)] $B_y \longleftarrow$ the empty set (which represents the
logical false)\\
$B_n \longleftarrow$ the empty set\\
$B_u \longleftarrow$ the whole free variable space
\item[(2)] While $V(B_u) > \epsilon$ repeat\\
$(B_{uy},B_{uu},B_{un}) \longleftarrow ApproxFormula(F,B_u)$\\
$B_y \longleftarrow B_y+B_{uy}$\\
$B_u \longleftarrow B_n+B_{un}$\\
$B_u \longleftarrow B_{uu}$
\item[(3)] $N \longleftarrow B_y+B_u$\\
$S \longleftarrow B_y$ \quad $\blacksquare$
\end{itemize}
\vskip -0.4cm
\noindent
\hrulefill

Now we describe the sub-algorithm {\sl ApproxFormula}. We will use the
following notational convention: $B_{pq}$ where $p$, $q$ can be one of
$y$, $n$, $u$, $*$, which respectively stand for yes, no, unknown, any
of them. The first index $p$ tells us the status of the box with respect
to a formula $F_1$ and the second index $q$ to $F_2$ . For instance $B_{yu}$
means that the box-set is a yes box-set with respect to $F_1$ and a
unknown box-set with respect to $F_2$.

\vskip 0.5cm
\noindent
{\bf Algorithm 6}

\vskip -0.2cm
\noindent
\hrulefill
\begin{center}
$(B_y,B_u,B_n) \longleftarrow ApproxFormula(F,B)$
\end{center}

\noindent
\begin{tabular}{ll}
{\sl Input}: & $F$ is a formula and $B$ is a box-set\\
{\sl Output}: & $B_y$, $B_u$, $B_n$ are box-sets that partition $B$ such
that $B_y \Longrightarrow F$ and $B_n \Longrightarrow \lnot F$
\end{tabular}

\begin{itemize}
\item[(1)] $F$ is an atomic formula\\
$(B_y,B_u,B_n) \longleftarrow ApproxAtomic(F,B)$
\item[(2)] $F \equiv F_1 \land F_2$\\
$(B_{y*},B_{u*},B_{n*}) \longleftarrow ApproxFormula(F_1,B)$\\
$(B_{yy},B_{yu},B_{yn}) \longleftarrow ApproxFormula(F_2,B_{y*})$\\
$(B_{uy},B_{uu},B_{un}) \longleftarrow ApproxFormula(F_2,B_{u*})$\\
$B_y \longleftarrow B_{yy}$\\
$B_u \longleftarrow B_{yu}+B_{uy}+B_{uu}$\\
$B_n \longleftarrow B_{n*}+B_{yn}+B_{un}$\\
\item[(3)] $F \equiv F_1 \lor F_2$\\
$(B_{y*},B_{u*},B_{n*}) \longleftarrow ApproxFormula(F_1,B)$\\
$(B_{ny},B_{nu},B_{nn}) \longleftarrow ApproxFormula(F_2,B_{n*})$\\
$(B_{uy},B_{uu},B_{un}) \longleftarrow ApproxFormula(F_2,B_{u*})$\\
$B_y \longleftarrow B_{y*}+B_{ny}+B_{uy}$\\
$B_u \longleftarrow B_{nu}+B_{un}+B_{uu}$\\
$B_n \longleftarrow B_{nn}$
\item[(4)] $F \equiv \lnot F$\\
$(B_{y*},B_{u*},B_{n*}) \longleftarrow ApproxFormula(F_1,B)$\\
$B_y \longleftarrow B_{n*}$\\
$B_u \longleftarrow B_{u*}$\\
$B_n \longleftarrow B_{y*}$
\item[(5)] $F \equiv F_1 \Longrightarrow F_2$\\
$(B_{y*},B_{u*},B_{n*}) \longleftarrow ApproxFormula(F_1,B)$\\
$(B_{yy},B_{yu},B_{yn}) \longleftarrow ApproxFormula(F_2,B_{y*})$\\
$(B_{uy},B_{uu},B_{un}) \longleftarrow ApproxFormula(F_2,B_{u*})$\\
$B_y \longleftarrow B_{n*}+B_{yy}+B_{uy}$\\
$B_u \longleftarrow B_{yu}+B_{un}+B_{uu}$\\
$B_n \longleftarrow B_{yn}$
\item[(6)] $F \equiv F_1 \Longleftrightarrow F_2$\\
$(B_{y*},B_{u*},B_{n*}) \longleftarrow ApproxFormula(F_1,B)$\\
$(B_{yy},B_{yu},B_{yn}) \longleftarrow ApproxFormula(F_2,B_{y*})$\\
$(B_{ny},B_{nu},B_{nn}) \longleftarrow ApproxFormula(F_2,B_{n*})$\\
$B_y \longleftarrow B_{yy}+B_{nn}$\\
$B_u \longleftarrow B_{u*}+B_{yu}+B_{nu}$\\
$B_n \longleftarrow B_{yn}+B_{ny}$
\item[(7)] $F \equiv \forall x F_1$ where $x$ is a vector of $n$ variables\\
$(B_{y*},B_{u*},B_{n*}) \longleftarrow 
ApproxFormula(F_1,B\times\mathbb{R}^n)$\\
$B_y \longleftarrow \forall x B_{y*}$\\
$B_n \longleftarrow \exists x B_{n*}$\\
$B_u \longleftarrow B-(B_y+B_n)$
\item[(8)] $F \equiv \exists x F_1$ where $x$ is a vector of $n$ variables\\
$(B_{y*},B_{u*},B_{n*}) \longleftarrow 
ApproxFormula(F_1,B\times\mathbb{R}^n)$\\
$B_y \longleftarrow \exists x B_{y*}$\\
$B_n \longleftarrow \forall x B_{n*}$\\
$B_u \longleftarrow B-(B_y+B_n)$
\end{itemize}
\vskip -0.4cm
\noindent
\hrulefill

\noindent
In the cases (7) and (8), the notation ``$B\times\mathbb{R}^n$'' means to
embed each box in $B$ into the $m+n$ dimensional space by letting the $n$
new variables to range over $(-\infty,\infty)$.

Correctness of this algorithm is not difficult to prove and is left to the
reader. Now we give brief and informal descriptions of the sub-algorithms:

\noindent
\begin{tabular}{ll}
$\bullet ApproxAtomic$ & approximation of atomic formula\\
$\bullet -$ & subtraction of box-sets\\
$\bullet \lor$ & disjunciton of box-sets\\
$\bullet \land$ & conjuncton of box-sets\\
$\bullet \exists$ & existential quantification of box-sets\\
$\bullet \forall$ & universal quantification of box-sets\\
\end{tabular}

\noindent
In order to keep the presentation of the ideas simple, we will
intentionally avoid optimizations, except straightforward ones. But
currently various optimizations, improvements and different methods
are being researched.

\vskip 0.5cm
\noindent
{\bf Approximating Atomic Formula}:
Consider an atomic formula $P > 0$ and a box-set $B$. Without losing
generality let us assume that $B$ consists of just one box. If not, we
can apply the method described below to each box in the box-set (or
just one box from it). Our goal is to partition $B$ into three box-sets
$B_y$, $B_u$, $B_n$ such that every point of $B_y$ satisfies $P > 0$ and no
point of $B_n$ satisfies $P > 0$.

We first check if the whole $B$ satisfies the atomic formula. This can
be done by computing an interval $I$ that bounds the range of the
polynomial $P$ on $B$, that is, $P(B) \subseteq I$. The range estimation can
be done by using various methods from interval mathematics. If $I > 0$,
then we can set $B_y$ to be $B$ and the others empty. If $I \le 0$, then we
can set $B_n$ to be $B$ and the other empty. Otherwise, we split the box $B$
into several boxes and do the same on each box.

But this method can be very inefficient because in general it will
result in numerous small boxes (exponential in the number of
variables). A better approach is to adapt the {\sl tightening} method of
Hong\cite{Hong94}. The method roughly works as follows. Let 
$P\in\mathbb{R}[x_1,\ldots,x_r]$
and $B$ is a box in the $r$-dimensional space. We choose a variable, say
$x_k$. We view $P$ as a univariate polynomial in $x_k$ . Then the
coefficients are polynomials in the other variables. We, using
interval methods, compute intervals that bound the coefficients on the
box\\ 
\[B_1\times\ldots\times B_{k-1}\times B_{k+1}\times\ldots\times B_r\]
As a result, we obtain
a univariate polynomial in $x_k$ with interval coefficients. Next, we
compute the ``root intervals'' of the polynomial, say $R_1,\ldots,R_\ell$.
Let $C_1,\ldots,C_u$ be the intervals that form the complement of
the root intervals with respect to $B_k$. Let $B^i$ be the box:
\[B_1\times\ldots\times B_{k-1}\times C_i\times B_{k+1}\times\ldots\times B_r\]
Then the sign of $P$ is constant
throughout each $B^i$, and the sign is either positive or negative. If
$P$ is positive on $B^i$, then we put $B^i$ into the box-set $B_y$, and if
negative, into the box-set $B_n$. We put the remaining parts of the box
$B$ into $B_u$. Specifically we put into $B_u$ the boxes 
\[B_1\times\ldots\times B_{k-1}\times R_i\times B_{k+1}\times\ldots\times B_r\]

\vskip 0.5cm
\noindent
{\bf Subtraction of Box-sets}:
Let $B$ and $C$ be two box-sets. We would like to compute a box-set $D$
such that $D \Longleftrightarrow B-C$, that is, $B \land \lnot C$.

If $B$ is empty then $D$ is empty. If $C$ is empty then $D=B$. Thus assume
that $B$ is not empty and $C$ is not empty. Let $B=B^1+B^*$ and
$C=C^1+C^*$. Observe:
\[\begin{array}{rcl}
B-C & \Longleftrightarrow & (B^1 + B^*) - C\\
& \Longleftrightarrow & (B^1 - C) + (B^*-C)\\
& \Longleftrightarrow & (B^1 - (C^1+C^*)) + (B^*-C)\\
& \Longleftrightarrow & ((B^1-C^1)-C^*)+(B^*-C)
\end{array}\]
Thus, the subtraction between two box-sets is reduced to the subtraction
between two boxes: $B^1-C^1$.

So let us now find out how to do subtraction between two boxes. Let $B$
consist of only one box and $C$ also consist of only one box.
Let $B=B_1\times B_*$ and let $C=C_1\times C_*$. Observe:
\[\begin{array}{rcl}
B-C & \Longleftrightarrow & (B_1\times B_*) - (C_1\times C_*)\\
& \Longleftrightarrow & (B_1\times B_*) \land \lnot (C_1\times C_*)\\
& \Longleftrightarrow & (B_1\times B_*) \land (\lnot C_1 \lor \lnot C_*)\\
& \Longleftrightarrow & (B_1\times B_*) \land 
(\lnot C_1 + (C_1 \land \lnot C_*))\\
& \Longleftrightarrow & ((B_1 \land \lnot C_1)\times B_*) +
((B_1 \land C_1) \times (B_* \land \lnot C_*))\\
& \Longleftrightarrow & ((B_1-C_1)\times B_*)+
((B_1 \land C_1)\times (B_* -C_*))
\end{array}\]
Thus the subtraction between two boxes is reduced to the subtraction
of two intervals $(B_1 - C_1)$ and the conjunction of two intervals 
$(B_1 \land C_1)$. These are trivial to do.

\vskip 0.5cm
\noindent
{\bf Disjunction of Box-sets}: Let $B$ and $C$ be two box-sets. We would
like to compute a box-set such that $D \Longleftrightarrow B \lor C$.
We can immediately reduce this to the subtraction problem as:
\[B \lor C \Longleftrightarrow B + (C - B)\]

\vskip 0.5cm
\noindent
{\bf Conjunction of Box-sets} Let $B$ and $C$ be two box-sets. We would
like to compute a box-set $D$ such that $D \Longleftrightarrow B \land C$.
Let $B=B^1+\cdots+B^\mu$ and let $C=C^1+\cdots+C^\nu$. We have
\[\begin{array}{rcl}
B \land C & \Longleftrightarrow & 
(+_{i=1}^\mu B^i) \land (+_{j=1}^\nu C^j)\\
& \Longleftrightarrow & 
+_{i=1}^\mu +_{j=1}^\nu (B^i \land C^j)\\
& \Longleftrightarrow & 
+_{i=1}^\mu +_{j=1}^\nu
(\times_{k=1}^n B_k^i \land \times_{k=1}^n C_k^j)\\
& \Longleftrightarrow & 
+_{i=1}^\mu +_{j=1}^\nu \times_{k=1}^n (B_k^i \land C_k^j)\\
\end{array}\]
Thus the conjunction of box-sets is reduced to the conjuntions of
intervals, which are trivial to do.

\vskip 0.5cm
\noindent
{\bf Existential Quantifier Elimination on Box-sets}:
Consider $F \equiv \exists x B$ where $x$ is a vector of $n$ variables
and $B$ is a box-set in the $\mathbb{R}^{m+n}$ dimensional space. First
note that $B$ is a disjunction of boxes, say $B^1,\ldots,B^\ell$.
Since the disjunction commutes with the existential quantification, we have
\[F \Longleftrightarrow \bigvee_{i=1}^\ell \exists x B^i\]
Now let $B^i=B_1^i \land\cdots\land B_{m+n}^i$ where $B_j^i$ is an interval in
the $x_j$-line. Let $\underline{B^i}$ %be the conjunction of the $m$
intervals corresponding to the free variables and $\overline{B^i}$ be the
conjunction of the $n$ intervals corresponding to the bound variables.
Note that
\[\exists x B^i \Longleftrightarrow \exists x\underline{B^i}\land\overline{B^i}
\Longleftrightarrow \underline{B^i} \land \exists x \overline{B^i}
\Longleftrightarrow \underline{B^i}\]
So we trivially have
\[F \Longleftrightarrow \bigvee_{i=1}^\ell \underline{B^i}\]
Thus the problem is reduced to the disjunction problem.

\vskip 0.5cm
\noindent
{\bf Universal Quantifier Elimination on Box-sets}:
Consider $F \equiv \forall x B$. 
One way is to turn this into existential problem
through double negation. But there is a better way. Let $B^1$ be a box
in the box-set $B$ and let $B^$ be the box-set consisting of the
remaining boxes. Now observe:
\[\begin{array}{rcl}
F & \equiv & \forall x B\\
& \Longleftrightarrow & \forall x [B^1 \lor B^*]\\
& \Longleftrightarrow & \forall x [(\underline{B^1}\land\overline{B^1})
\lor B^*]\\
& \Longleftrightarrow & \forall x [(\underline{B^1} \lor B^*) \land
(\overline{B^1} \lor B^*)]\\
& \Longleftrightarrow & \forall x [\underline{B^1} \lor B^*] \land
\forall x [\overline{B^1} \lor B^*]\\
& \Longleftrightarrow & (\underline{B^1} \lor \forall x B^*) \land
\forall x [\overline{B^1} \lor B^*]\\
& \Longleftrightarrow & (\underline{B^1} \land 
\forall x [\overline{B^1} \lor B^*]) \lor 
(\forall x B^* \land \forall x [\overline{B^1} \lor B^*])\\
& \Longleftrightarrow & (\underline{B^1} \land 
\forall x [\overline{B^1} \lor B^*]) \lor \forall x B^*
\end{array}\]
Note that we have divided the formula $F$ into two smaller
formulas. This suggests a recursive-divide-conquer algorithm. For this
goal, let us rewrite the above observation in a slightly more general
way (so that we can do recursion):
\[\begin{array}{rl}
& L \land \forall x [U \lor B]\\
\Longleftrightarrow & (L \land \underline{B^1} \land
\forall x [U \lor \overline{B^1} \lor B^*]) \lor
(L \land \forall x [U \lor B^*])
\end{array}\]
where $L$ is a box in the free variable space. and $U$ is a box-set in the
bound variable space. We rewrite it again to make the recursion even
more explicit:
\[\begin{array}{rl}
& L \land \forall x [U \lor B]\\
\Longleftrightarrow & L^* \land \forall x [U^* \lor B^*] \quad\lor\quad
L \land \forall x [U \lor B^*]
\end{array}\]
where $L^* \equiv L \land \underline{B^1}$ and 
$U^* \equiv U \lor \overline{B^1}$.
The recursion terminates since the number of boxes in $B$ decreases.

Now we have the following obvious recursive algorithm. For efficiency,
we do some easy check in the step (1) in order to detect trivial cases
for which the recursion is not necessary.

\vskip 0.5cm
\noindent
{\bf Algorithm 7}

\vskip -0.2cm
\noindent
\hrulefill
\begin{center}
$D \longleftarrow UnivQE(x,B,L,U)$
\end{center}

\noindent
\begin{tabular}{ll}
{\sl Input}: & $x$ is the vector of variables.\\
& $B$ is a box-set.\\
& $L$ is a box in the free variable space.\\
& $U$ is a box-set in the bound variable space.\\
{\sl Output}: & $D$ is a box-set in the free variable space such that
$D \Longleftrightarrow L \land \forall x [U \lor B]$
\end{tabular}

\begin{itemize}
\item[(1)] [Prune/Recursion base]\\
If $L$ is empty then set $D$ to be empty and return\\
If $U$ is the whole bound variable space then set $D$ to be $L$ and return\\
If $B$ is empty then set $D$ to be empty and return
\item[(2)] [Recursion]\\
Choose a box $B^1$ from $B$\\
Let $B^*$ be the box-set of the remaining boxes.\\
$L^* \longleftarrow L \land \underline{B^1}$\\
$U^* \longleftarrow U \lor \overline{B^1}$\\
$D \longleftarrow UnivQE(x,B^*,L^*,U^*) \lor UnivQE(x,B^*,L,U)
\quad\blacksquare$
\end{itemize}
\vskip -0.4cm
\noindent
\hrulefill

On the top-level, this algorithm is called with $L$ being the whole
free-variable space and $U$ being empty, so that the output is a box-set
equivalent to $\forall x B$. Some optimization can be done.
\begin{itemize}
\item First, the line in Step (2)
\[U^* \longleftarrow U \lor \overline{B^1}\]
can be replaced by the easier operation
\[U^* \longleftarrow U + \overline{B^1}\]
even though $U$ and $\overline{B^1}$ might overlap.\\
The reason is as follows. Whenever $U$ and $\overline{B^1}$ overlap,
$L$ and $\underline{B^1}$ are disjoint (making $L^*$ empty), and
thus, the recursive call will not use $U^*$. So, it is safe to carry
out the ``illegal'' operation.
\item Next, the line in Step (2)
\[D \longleftarrow UnivQE(x,B^*,L^*,U^*) \lor UnivQE(x,B^*,L,U)\]
can be replaced by the easier one
\[D \longleftarrow UnivQE(x,B^*,L^*,U^*) + UnivQE(x,B^*,L,U)\]
It is because the outputs of the two calls to {\sl UnivQE} are disjoint, as
we show now. When $U$ and $\overline{B^1}$ overlap, 
$L$ and $\underline{B^1}$ are disjoint, and thus
the output of the first call is empty. So the disjunction can be
trivially replaced by $+$. Thus, now assume that $U$ and $\overline{B^1}$ 
do not overlap. Let us assume the two outputs overlap, then there exists a
point, say $v$, which satisfies both outputs. 
Then, $v\in \underline{B^1}$. Let $w\in \overline{B^1}$.
Then $(v,w)\in B^1$. Since $B^1$ is disjoint from $U$ and $B^*$, $(v,w)$
is not in $U\lor B^*$. This contradicts the fact that $\forall x[U\lor B^*]$
is true on $v$. Thus, the two outputs do not overlap.

\end{itemize}

\subsubsection{Termination}

Does the algorithm ({\sl VolumeApproxQE}) terminate always? The answer is
{\sl almost always}. Now we will discuss a few causes of non-termination and
solutions for them.

\vskip 0.5cm
\noindent
{\bf Non-termination 1}

Assume that the solution set of a formula 
$F(x,y)$ is $\{(x,y)|0 \le x-y \le 1\}$
By plotting the solution set and trying to approximate it with
a finite number of boxes, one will discover that there is no way to
make the volume-error finite. In general, the method might not
terminate if the solution set is un-bounded.

We can avoid such difficulty if we restrict the input to a box-set
(finite boxes). This can be done by modifying the algorithm
{\sl VolumeApproxQE} so that it has one more input argument, say a box-set
$B$, and that it initializes $B_u$ to be $B$. The outputs will be
necessary/sufficient conditions for $F \land B$.

For the same reason, we also restrict the range of the bound
variables. Thus, instead of $\forall x$, we have
$\forall x \in C$ where $C$ is a finite box.

For the same reason, we also restrict the range of the bound
variables. Thus, instead of $\forall x$, we have
$\forall x \in C$ where $C$ is a finite box. Likewise, instead of
$\exists x$, we have $\exists x\in C$. One can easily make
necessary modifications on the algorithms.

\vskip 0.5cm
\noindent
{\bf Non-termination 2} There is one more cause for non-termination.
Consider the formula $F$ and the initial box $B$:
\[\begin{array}{rcl}
F & : & \forall y \in [0,1]~x\ne y\\
B & : & [0,1]
\end{array}\]

By plotting the solution set, one will discover that the
volume-difference will not shrink, no matter how accurately the atomic
formula $x\ne y$ is approximated. Thus, the algorithm will loop
forever. We can avoid such difficulty in several ways.
\begin{itemize}
\item In $\forall x F(x,y)$, 
we restrict that the solution set (in the $x$-space) of $F$
is a closed set for all values of $y$.  Likewise in
$\exists x F(x,y)$, we restrict that the solution set (in the
$x$-space) of $F$ is an open set for all values of $y$.
Then, the algorithm will terminate.
\item Replace the quantifiers with the generic quantifiers (as defined the
section on Generic Quantifier Elim- ination) and use minimal root
separation theorems to decide when to abort looping.
\item Replace the quantifiers with the measure-quantifier $\mathcal{M}^p$,
defined as the sentence
\[\mathcal{M}^p x \in C~F(x)\]
means that $V(F)/V(C)\ge p$. If $0 < p < 1$, then the algorithm terminates.
\item Use locally more powerful methods such as: resultants, quadratic
forms, Krawszyk's operator, etc
\end{itemize}
Among them, the method of the measure-quantifier seems to be the most
useful in practice.

\chapter{Potential Future Algebra}
{\center{\includegraphics[scale=0.70]{ps/v101toe.eps}}}
\chapter{Groebner Basis}
Groebner Basis
\chapter{Greatest Common Divisor}
Greatest Common Divisor
\chapter{Polynomial Factorization}
Polynomial Factorization


\chapter{Differential Forms}
This is quoted from Wheeler \cite{Whee12}.

\section{From differentials to differential forms}

In a formal sense, we may define differentials as the vector space of
linear mappings from curves to the reals, that is, given a
differential $df$ we may use it to map any curve, C $ \in \mathit{C}$
to a real number simply by integrating:
\[df:C \rightarrow R\]
\[ x = \int_C{df}\]
This suggests a generalization, since we know how to integrate over
surfaces and volumes as well as curves. In higher dimensions we also
have higher order multiple integrals. We now consider the integrands
of arbitrary multiple integrals
\[\int{f(x)}dl,\quad\int\int{f(x)}dS,\quad\int\int\int{f(x)}dV\]
Much of their importance lies in the coordinate invariance of the
resulting integrals.

One of the important properties of integrands is that they can all be
regarded as oriented. If we integrate a line integral along a curve
from $A$ to $B$ we get a number, while if we integrate from $B$ to $A$
we get minus the same number,
\[\int_A^B{f(x)}dl= -\int_B^A{f(x)}dl\]
We can also demand oriented surface integrals, so the surface integral
\[\int\int{\bf A\cdot n}~dS\]
changes sign if we reverse the direction of the normal to the surface.
This normal can be thought of as the cross product of two basis
vectors within the surface. If these basis vectors' cross product is
taken in one order, {\bf n} has one sign. If the opposite order is
taken then {\bf -n} results. Similarly, volume integrals change sign
if we change from a right- or left-handed coordinate system.

\subsection{The wedge product}

We can build this alternating sign into our convention for writing
differential forms by introducing a formal antisymmetric product,
called the {\sl wedge} product, symbolized by $\wedge$, which is
defined to give these differential elements the proper signs. Thus,
surface integrals will be written as integrals over the products
\[{\bf dx} \wedge {\bf dy},
  {\bf dy} \wedge {\bf dz},
  {\bf dz} \wedge {\bf dx}\]
with the convention that $\wedge$ is antisymmetric:
\[{\bf dx} \wedge {\bf dy} = -{\bf dy} \wedge {\bf dx}\]
under the interchange of any two basis forms. This automatically gives
the right orientation of the surface. Similarly, the volume element
becomes
\[{\bf V} = {\bf dx} \wedge {\bf dy} \wedge {\bf dz}\]
which changes sign if any pair of the basis elements are switched.

We can go further than this by formalizing the full integrand. For
a line integral, the general form of the integrand is a linear
combination of the basis differentials,
\[{\bf A}_x{\bf dx} + {\bf A}_y{\bf dy} + {\bf A}_z{\bf dz}\]
Notice that we simply add the different parts. Similary, a general
surface integrand is
\[{\bf A}_z {\bf dx \wedge dy} + 
  {\bf A}_y {\bf dz \wedge dx} + 
  {\bf A}_x {\bf dy \wedge dz }\]
while the volume integrand is
\[f(x)~{\bf dx \wedge dy \wedge dz}\]
These objects are called {\sl differential forms}.

Clearly, differential forms come in severaly types. Functions are
called 0-forms, line elements 1-forms, surface elements 2-forms, and
volume elements are called 3-forms. These are all the types that exist
in 3-dimensions, but in more than three dimensions we can have
$p$-forms with $p$ ranging from zero to the dimension, $d$, of the
space. Since we can take arbitrary linear combinations of $p$-forms,
they form a vector space, $\Lambda_p$.

We can always wedge together any two forms. We assume this wedge
product is associative, and obeys the usual distributive laws. The
wedge product of a $p$-form with a $q$-form is a $(p+q)$-form.

Notice that the antisymmetry is all we need to rearrange any
combination of forms. In general, wedge products of even order forms
with any other forms commute while wedge products of pairs of
odd-order forms anticommute. In particular, functions (0-forms)
commute with all $p$-forms. Using this, we may interchange the order
of a line element and a surface area, for if
\[{\bf l} = A~{\bf dx}\]
\[{\bf S} = B~{\bf dy \wedge dz}\]
then
\[\begin{array}{rcl}
{\bf l \wedge S}&=& (A~{\bf dx}) \wedge (B~{\bf dy \wedge dz})\\
&=&A~{\bf dx} \wedge B~{\bf dy \wedge dz}\\
&=&AB~{\bf dx \wedge dy \wedge dz}\\
&=&-AB~{\bf dy \wedge dx \wedge dz}\\
&=&AB~{\bf dy \wedge dz \wedge dx}\\
&=&{\bf S \wedge l}
\end{array}\]
but the wedge product of two line elements changes sign, for if
\[{\bf l}_1 = A~{\bf dx}\]
\[{\bf l}_2 = B~{\bf dy} + C~{\bf dz}\]
then
\[\begin{array}{rcl}
{\bf l}_1 \wedge {\bf l}_2&=&(A~{\bf dx}) \wedge(B~{\bf dy}+C~{\bf dz})\\
&=&A~{\bf dx} \wedge B~{\bf dy} + A~{\bf dx} \wedge C~{\bf dz}\\
&+&AB~{\bf dx \wedge dy} + AC~{\bf dx \wedge dz}\\
&=&-AB~{\bf dy \wedge dx} - AC~{\bf dz \wedge dz}\\
&=&-B~{\bf dy} \wedge A~{\bf dx} - C~{\bf dz} \wedge A~{\bf dx}\\
&=&-{\bf l}_2 \wedge {\bf l}_1
\end{array}\]
For any odd-order form, $\omega$, we immediately have
\[\omega \wedge\omega = -\omega \wedge\omega = 0\] In 3-dimensions there
are no 4-forms because anything we try to construct must contain a
repeated basis form. For example,
\[\begin{array}{rcl}
{\bf l} \wedge {\bf V}&=&(A~{\bf dx}) \wedge(B~{\bf dx \wedge dy \wedge dz})\\
&=&AB~{\bf dx \wedge dx \wedge dy \wedge dz}\\
&=&0
\end{array}\]
since ${\bf dx \wedge dx}=0$. The same occurs for anything we try. Of
course, if we have more dimensions then there are more independent
directions and we can find nonzero 4-forms. In general, in
$d$-dimensions we can find $d$-forms, but no $(d+1)$-forms.

Now suppose we want to change coordinates. How does an integrand change?
Suppose Cartesian coordinates (x,y) in the plane are given as some
functions of new coordinates (u,v). Then we already know that
differentials change according to 
\[{\bf dx} = {\bf dx}(u,v) = 
  \frac{\partial x}{\partial u}{\bf du} +
  \frac{\partial x}{\partial v}{\bf dv}\]
and similarly for ${\bf dy}$, applying the usual rules for partial
differentiation. Notice what happens when we use the wedge
product to calculate the new area element:
\[\begin{array}{rcl}
{\bf dx} \wedge{\bf dy}&=&
\displaystyle\left(\frac{\partial x}{\partial u}{\bf du}+
      \frac{\partial x}{\partial v}{\bf dv}\right) \wedge
\displaystyle\left(\frac{\partial y}{\partial u}{\bf du}+
      \frac{\partial y}{\partial v}{\bf dv}\right)\\
&&\\
&=&\displaystyle\frac{\partial x}{\partial v}\frac{\partial y}{\partial u}
   {\bf dv \wedge du} +
   \displaystyle\frac{\partial x}{\partial u}\frac{\partial y}{\partial v}
   {\bf du \wedge dv} \\
&&\\
&=&\left(
\displaystyle\frac{\partial x}{\partial u}\frac{\partial y}{\partial v}-
\displaystyle\frac{\partial x}{\partial v}\frac{\partial y}{\partial u}
\right) {\bf du \wedge dv}\\
&&\\
&=&\mathit{J}~{\bf du \wedge dv}
\end{array}\]
where
\[J=\textrm{det}\left(
\begin{array}{rcl}
\displaystyle\frac{\partial x}{\partial u} & 
\displaystyle\frac{\partial x}{\partial v}\\
&\\
\displaystyle\frac{\partial y}{\partial u} & 
\displaystyle\frac{\partial y}{\partial v}
\end{array}
\right)\]
is the Jacobian of the coordinate transformation. This is exactly the
way that an area element changes when we change coordinates. Notice
the Jacobian coming out automatically. We couldn't ask for more -
the wedge product not only gives us the right signs for oriented
areas and volumes, but gives us the right transformation to new
coordinates. Of course the volume change works, too.

Under a coordinate transformation
\[ x \rightarrow x(u,v,w)\]
\[ y \rightarrow y(u,v,w)\]
\[ z \rightarrow z(u,v,w)\]
the new volume element is the full Jacobian times the new volume form,
\[{\bf dx \wedge dy \wedge dz} = J(xyz;uvw)~{\bf du \wedge dv \wedge dw}\]

So the wedge product successfully keesp track of $p$-dim volumes and
their orientations in a coordinate invariant way. Now any time we have
an integral, we can regard the integrand as being a differential form.
But all of this can go much further. Recall our proof that 1-forms form
a vector space. Thus, the differential, ${\bf dx}$ of $x(u,v)$ given
above is just a gradient. It vanishes along surfaces where $x$ is
constant, and the components of the vector
\[\displaystyle\left(
\frac{\partial x}{\partial u},\frac{\partial x}{\partial v}\right)
\]
point in a direction normal to those surfaces. So symbols like
${\bf dx}$ or ${\bf du}$ contain directional information. Writing
them with a boldface {\bf d} indicates this vector character. Thus,
we write
\[{\bf A} = A_i{\bf dx^i}\]

Let
\[f(x,y)=axy\]
The vector with components
\[\displaystyle\left(
\frac{\partial f}{\partial u},\frac{\partial f}{\partial v}\right)
\]
is perpendicular to the surfaces of constant $f$.

We have defined forms, have written down their formal properties, and have
used those properties to write them in components. Then, we define the
wedge product, which enables us to write $p$-dimensional integrands as
$p$-forms in such a way that the orientation and coordinate transformation
properties of the integrals emerges automatically.

Though it is 1-forms, $A_i{\bf dx^i}$ that corresponding to vectors,
we have defined a product of basis forms that we can generalize to
more complicated objects. Many of these objects are already
familiar. Consider the product of two 1-forms.
\[\begin{array}{rcl}
{\bf A} \wedge {\bf B}
&=&A_i~{\bf dx}^i \wedge B_j~{\bf dx}^j\\
&=&A_iB_j~{\bf dx}^i \wedge {\bf dx}^j\\
&=&\displaystyle\frac{1}{2}
A_iB_j~({\bf dx}^i \wedge {\bf dx}^j-{\bf dx}^j \wedge {\bf dx}^i)\\
&&\\
&=&\displaystyle\frac{1}{2}
(A_iB_j~{\bf dx}^i \wedge {\bf dx}^j-A_iB_j~{\bf dx}^j \wedge {\bf dx}^i)\\
&&\\
&=&\displaystyle\frac{1}{2}
(A_iB_j~{\bf dx}^i \wedge {\bf dx}^j-A_jB_i~{\bf dx}^i \wedge {\bf dx}^j)\\
&&\\
&=&\displaystyle\frac{1}{2}(A_iB_j-A_jB_i)~{\bf dx}^i \wedge {\bf dx}^j
\end{array}\]
The coefficients
\[A_iB_j-A_jB_i\]
are essentially the components of the cross product. We will see this in
more detail below when we discuss the curl.

\subsection{The exterior derivative}

We may regard the differential of any function, say $f(x,y,z)$, as the
1-form:
\[\begin{array}{rcl}
{\bf d}f&=&
\displaystyle\frac{\partial f}{\partial x}{\bf d}x+
\displaystyle\frac{\partial f}{\partial y}{\bf d}y+
\displaystyle\frac{\partial f}{\partial z}{\bf d}z\\
&&\\
&=&\displaystyle\frac{\partial f}{\partial x^i}{\bf d}x^i
\end{array}\]

Since a fnction is a 0-form then we can imagine an operator {\bf d} that
differentiates any 0-form to give a 1-form. In Cartesian coordinates,
the coefficients of this 1-form are just the Cartesian components of the
gradient.

The operator {\bf d} is called the {\sl exterior derivative}, and we may
apply it to any $p$-form to get a $(p+1)$-form. The extension is defined
as follows. First consider a 1-form
\[{\bf A}=A_i~{\bf dx}^i\]
We define
\[{\bf dA}={\bf d}A_i \wedge {\bf dx}^i\]
Similarly, since an arbitrary $p$-form in $n$-dimensions may be written as
\[\omega=A_{i_1,i_2,\cdots,i_p} \wedge {\bf dx}^{i_1} \wedge {\bf dx}^{i_2}
\cdots \wedge {\bf dx}^{i_p}\]
we define the exterior derivative of $\omega$ to be a $(p+1)$-form
\[{\bf d}\omega=
{\bf d}A_{i_1,i_2,\cdots,i_p} \wedge {\bf dx}^{i_1} \wedge {\bf dx}^{i_2}
\cdots \wedge {\bf dx}^{i_p}\]

Let's see what happens if we apply ${\bf d}$ twice to the Cartesian
coordinate, $x$ regarded as a function of $x,y$ and $z$:
\[\begin{array}{rcl}
{\bf d}^2x&=&{\bf d}({\bf d}x)\\
&=&{\bf d}(1{\bf d}x)\\
&=&{\bf d}(1) \wedge{\bf d}x\\
&=&0
\end{array}\]
since all derivatives of the constant function $f=1$ are zero. The
same applies if we apply {\bf d} twice to {\sl any} function:
\[\begin{array}{rcl}
{\bf d}^2f &=&{\bf d}({\bf d}f)\\
&=&\displaystyle{\bf d}
\left(\frac{\partial f}{\partial x^i}{\bf d}x^i\right)\\
&&\\
&=&\displaystyle{\bf d}
\left(\frac{\partial f}{\partial x^i} \wedge {\bf d}x^i\right)\\
&&\\
&=&\displaystyle\left(\frac{\partial^2 f}{\partial x^j\partial x^i}
{\bf d}x^j\right) \wedge {\bf d}x^i\\
&&\\
&=&\displaystyle
\frac{\partial^2 f}{\partial x^j\partial x^i}{\bf d}x^j \wedge{\bf d}x^i
\end{array}\]
By the same argument we used to get the components of the curl, we may
write this as
\[\begin{array}{rcl}
{\bf d}^2f&=&\displaystyle\frac{1}{2}\left(
\displaystyle\frac{\partial^2f}{\partial x^j\partial x^i}-
\displaystyle\frac{\partial^2f}{\partial x^i\partial x^j}\right)
{\bf d}x^j \wedge{\bf d}x^i\\
&=&0
\end{array}\]
since partial derivatives commute.

Poincar\'e Lemma: ${\bf d}^2\omega=0$ where $\omega$ is an arbitrary $p$-form.
\index{Poincar\'e Lemma}

Next, consider the effect on {\bf d} on an arbitrary 1-form. We have
\[\begin{array}{rcl}
{\bf dA}&=&{\bf d}(A_i{\bf d}x^i)\\
&&\\
&=&\displaystyle\left(\frac{\partial A_i}{\partial x^j}{\bf d}x^j\right)
 \wedge{\bf d}x^i\\
&&\\
&=&\displaystyle\frac{1}{2}\left(
\frac{\partial A_i}{\partial x^j}-\frac{\partial A_j}{\partial x^i}\right)
{\bf d}x^j \wedge{\bf d}x^i
\end{array}\]
We have the components of the curl of the vector {\bf A}. We must be
careful here, however, because these are the components of the curl
only in Cartesian coordinates. Later we will see how these components
relate to those in a general coordinate system. Also, recall that the
components $A_i$ are distinct from the usual vector components $A^i$.
These differences will be resolved when we give a detailed discussion
of the metric. Ultimately, the action of {\bf d} on a 1-form gives us
a coordinate invariant way to calculate the curl.

Finally, suppose we have a 2-form expressed as
\[{\bf S}=A_z~{\bf d}x \wedge {\bf d}y+A_y~{\bf d}z \wedge {\bf d}x+
A_x~{\bf d}y \wedge {\bf d}z\]
Then apply the exterior derivative gives
\[\begin{array}{rcl}
{\bf d}S&=&{\bf d}A_z \wedge{\bf d}x \wedge{\bf d}y+
{\bf d}A_y \wedge{\bf d}z \wedge{\bf d}x+
{\bf d}A_x \wedge{\bf d}y \wedge{\bf d}z\\
&&\\
&=&\displaystyle\frac{\partial A_z}{\partial z}{\bf d}z \wedge{\bf d}x
 \wedge{\bf d}y+
\displaystyle\frac{\partial A_y}{\partial y}{\bf d}y \wedge{\bf d}z
 \wedge{\bf d}x+
\displaystyle\frac{\partial A_x}{\partial x}{\bf d}x \wedge{\bf d}y
\wedge{\bf d}z\\
&&\\
&=&\displaystyle\left(
\frac{\partial A_z}{\partial z}+
\frac{\partial A_y}{\partial y}+
\frac{\partial A_x}{\partial x}\right)~{\bf d}x \wedge{\bf d}y \wedge{\bf d}z
\end{array}\]
so that the exterior derivative can also reproduce the divergence.

\subsection{The Hodge dual}

To truly have the curl we need a way to turn a 2-form into a vector, i.e.,
a 1-form and a way to turn a 3-form into a 0-form. This leads us to 
introduce the Hodge dual
\index{Hodge dual}, or star, operator $\star$.

Notice that in 3-dim, both 1-forms and 2-forms have three independent
components, while both 0- and 3-forms have one component. This suggests
that we can define an invertible mapping between these pairs. In
Cartesian coordinates, suppose we set
\[\begin{array}{rcl}
\star({\bf dx} \wedge {\bf dy})&=&{\bf dz}\\
\star({\bf dy} \wedge {\bf dz})&=&{\bf dx}\\
\star({\bf dz} \wedge {\bf dx})&=&{\bf dy}\\
\star({\bf dx} \wedge {\bf dy} \wedge {\bf dz})&=&1
\end{array}\]
and further require that the star be its own inverse
\[\star\star = 1\]
With these rules we can find the Hodge dual of any form in 3-dim.

The dual of the general 1-form
\[{\bf A} = A_i{\bf dx}^i\]
is the 2-form
\[S=A_z~{\bf dx} \wedge {\bf dy} + A_y~{\bf dz} \wedge {\bf dx} +
A_x~{\bf dy} \wedge {\bf dz}\]

For an arbitrary (Cartesian) 1-form
\[{\bf A} = A_i{\bf dx}^i\]
that
\[\star{\bf d}\star{\bf A} = div {\bf A}\]

The curl of {\bf A} 
\[curl({\bf A}) =
\displaystyle\left(
\frac{\partial A_y}{\partial z}-
\frac{\partial A_z}{\partial y}\right){\bf dx}+
\displaystyle\left(
\frac{\partial A_z}{\partial x}-
\frac{\partial A_x}{\partial z}\right){\bf dy}
\displaystyle\left(\frac{\partial A_x}{\partial y}-
\frac{\partial A_y}{\partial x}\right){\bf dz}\]

Three operations - the wedge product $\wedge$, the exterior derivative 
{\bf d}, and the Hodge dual $\star$ - together encompass the usual dot
and cross products as well as the divergence, curl and gradient. In fact,
they do much more - they extend all of these operations to arbitrary
coordinates and arbitrary numbers of dimensions. To explore these
generalizations, we must first explore properties of the metric and
look at coordinate transformations. This will allow us to define the
Hodge dula in arbitrary coordinates.

\chapter{Pade approximant}
Pade approximant
\chapter[Schwartz-Zippel lemma]
{Schwartz-Zippel lemma and testing polynomial identities}
Schwartz-Zippel lemma and testing polynomial identities
\chapter{Chinese Remainder Theorem}
Chinese Remainder Theorem
\chapter{Gaussian Elimination}
Gaussian Elimination
\chapter{Diophantine Equations}
Diophantine Equations

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{axiom}
\bibliography{axiom}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Index}
\printindex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
