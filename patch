books/bookvolbib added references

Goal: Proving Axiom Sane

\index{Benoit, Alexandre}
\index{Chyzak, Frederic}
\index{Darrasse, Alexis}
\index{Gregoire, Thomas}
\index{Koutschan, Christoph}
\index{Mezzarobba, Marc}
\index{Salvy Bruno}
\begin{chunk}{axiom.bib}
@misc{DDMF19,
  author = "Benoit, Alexandre and Chyzak, Frederic and Darrasse, Alexis
            and Gregoire, Thomas and Koutschan, Christoph and
            Mezzarobba, Marc and Salvy Bruno",
  title = {{Digital Dictionary of Mathematical Functions}}
  year = "2019",
  link = "\url{ddfm.msr-inria.inria.fr/1.9.1/ddmf}",
  abstract =
    "Interactive site on Mathematical Functions with properties,
    truncated expansions, numerical evaluations, plots, and more. The
    functions currently presented are elementary functions with
    special functions of a single variable. More functions -- special
    functions with parameters, orthogonal polynomials, sequences --
    will be added with the project advances.",
  paper = "DDMF19.pdf",
  keywords = "axiomref"
}

\end{chunk}

\index{Stoutemyer, David R.}
\begin{chunk}{axiom.bib}
@misc{Stou12a,
  author = "Stoutemyer, David R.",
  title = {{Can the Eureqa Symbolic Regression Program, Computer
            Algebra and Numerical Analysis help each other?}},
  link = "\url{https://arxiv.org/pdf/1203.1023.pdf}",
  year = "2012",
  abstract =
    "The free Eureqa program has recently received extensive press
    praise. A representitive quote is
    \begin{quote}
      There are very clever 'thinking machines' in existence today,
      such as Watson, the IBM computer that conquered {\sl Jeopardy!}
      last year. But next to Eureqa, Watson is merely a glorified
      search engine.
    \end{quote}
    
    The program is designed to work with noisy experimental data,
    searching for then returning a set of result expressions that
    attempt to optimally trade off conciseness with accuracy.

    However, if the data is generated from a formula for which there
    exists more concise equivalent formulas, sometimes some of the
    candidate Eureqa expressions are one or more of those more concise
    equivalents expressions. If not, perhaps one or more of the
    returned Eureqa expressions might be a sufficiently accurate
    approximation that is more concise than the given
    formula. Moreover, when there is no known closed form expression,
    the data points can be generated by numerical methods, enabling
    Eureqa to find expressions that concisely fit those data points
    with sufficient accuracy. In contrast to typical regression
    software, the user does not have to explicitly or implicitly
    provide a specific expression or class of expressions containing
    unknown constants for the software to determine.

    Is Eureqa useful enough in these regards to provide an additional
    tool for experimental mathematics, computer algebra users and
    numerical analysts? Yes, if used carefully. Can computer algebra
    and numerical methods help Eureqa? Definitely.",
  paper = "Stou12a.pdf"
}

\end{chunk}

\index{Barthe, Gilles}
\begin{chunk}{axiom.bib}
@article{Bart85,
  author = "Barthe, Gilles",
  title = {{Implicit Coercions in Type Systems}},
  journal = "LNCS",
  volume = "1158",
  pages = "1-15",
  year = "1985",
  abstract =
    "We propose a notion of pure type system with implicit
    coercions. In our framework, judgements are extended with a
    context of coerions $\Delta$ and the application rule is modified
    so as to allow coercions to be left implicit. The setting supports
    multiple inheritance and can be applied to all type theories with
    $\Pi$-types. One originality of our work is to propose a
    computational interpretation of implict coercions. In this paper,
    we demonstrate how this interpretation allows a strict control on
    the logical properties of pure type systems with implicit coercions.",
  paper = "Bart85.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Barthe, Gilles}
\index{Ruys, Mark}
\index{Barendregt, Henk}
\begin{chunk}{axiom.bib}
@article{Bart85a,
  author = "Barthe, Gilles and Ruys, Mark and Barendregt, Henk",
  title = {{A Two-Level Approach Towards Lean Proof-Checking}},
  journal = "LNCS",
  volume = "1158",
  pages = "16-35",
  year = "1985",
  abstract =
    "We present a simple and effective methodology for equational
    reasoning in proof checkers. The method is based on a two-level
    approach distinguishing between syntax and semantics of
    mathematical theories. The method is very general and can be
    carried out in any system with inductive and oracle types. The
    potential of our two-level approach is illustrated by some
    examples developed in Lego.",
  paper = "Bart85a.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Lou, Zhaohui}
\begin{chunk}{axiom.bib}
@article{Loux08,
  author = "Lou, Zhaohui",
  title = {{Coercions in a Polymorphic Type System}},
  journal = "Math. Struct. in Comp. Science",
  volume = "18",
  pages = "729-751",
  year = "2008",
  abstract =
    "We incorporate the idea of coercive subtyping, a theory of
    abbreviation for dependent type theories, into the polymorphic
    type system in functional programming languages. The traditional
    type system with let-polymorphism is extended with argument
    coercions and function coercions, and a corresponding type
    inference algorithm is presented and proved to be sound and complete.",
  paper = "Loux08.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Berger, U.}
\index{Schwichtenberg, H.}
\begin{chunk}{axiom.bib}
@article{Berg85,
  author = "Berger, U. and Schwichtenberg, H.",
  title = {{The Greatest Common Divisor: A Case Study for Program
            Extraction from Classical Proofs}},
  journal = "LNCS",
  volume = "1158",
  pages = "36-46",
  year = "1985",
  paper = "Berg85.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Lou, Zhaohui}
\begin{chunk}{axiom.bib}
@article{Loux12,
  author = "Lou, Zhaohui",
  title = {{Formal Semantics in Modern Type Theories with Coercive Semantics}},
  journal = "Linguistics and Philosophy",
  volume = "35",
  pages = "491-513",
  year = "2012",
  abstract =
    "In the formal semantics based on modern type theories, common
    nouns are interpreted as types, rather than as predicates on
    entities as in Montague's semantics. This brings about important
    advantages in linguistic interpretations but also leads to a
    limitation of expressive power because there are fewer operations
    on types as compared with those on predicates. The theory of
    coercive subtyping adequately extends the modern type theories
    and, as shown in this paper, plays a very useful role in making
    type theories more expressive for formal semantics. It not only
    gives a satisfactory solution to the basic problem of 'multiple
    categorisations' caused by interpreting common nouns as types, but
    provides a powerful formal framework to model interesting
    linguistic phenomena such as copredication, whose formal treatment
    has been found difficult in a Montagovian setting. In particular,
    we show how to formally introduce dot-types in a type theory with
    coercive subtyping and study some type-theoretic constructs that
    provide useful representational tools for reference transfers and
    multiple word meanings in formal lexical semantics.",
  paper = "Loux12.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Jedynak, Wojciech}
\index{Biernacka, Malgorzata}
\index{Biernacki, Dariusz}
\begin{chunk}{axiom.bib}
@inproceedings{Jedy13,
  author = "Jedynak, Wojciech and Biernacka, Malgorzata and 
            Biernacki, Dariusz",
  title = {{An Operational Foundation for the Tactic Language of Coq}},
  booktitle = "Proc. 15th Symp. on Principles and Practices of
               Declarative Programming",
  publisher = "ACM",
  pages = "25-36",
  year = "2013",
  isbn = "978-1-4503-2154-9",
  abstract =
    "We introduce a semantic toolbox for Ltac, the tactic language of
    the popular Coq proof assistant. We present three formats of
    operational semantics, each of which has its use in the practice
    of tactic programming: a big-step specification in the form of
    natural semantics, a model of implementation in the form of an
    abstract machine, and a small-step characterization of computation
    in the form of reduction semantics. The three semantics are
    provably equivalent and have been obtained via off-the-shelf
    derivation techniques of the functional correspondence and the
    syntactic correspondence. We also give examples of Ltac programs
    and discuss some of th eissues that the formal semantics help to
    clarify. 
    
    With this work we hope to enhance the operational understanding of
    Ltac as well as to set up a framework to reason about Coq scripts
    and to build tools supporting tactic programming based on rigorous
    semantics.", 
  paper = "Jedy13.pdf"
}

\end{chunk}

\index{Asperti, Andrea}
\index{Ricciotti, Wilmer}
\index{Coen, Claudio Sacerdoti}
\index{Tassi, Enrico}
@article{Aspe12b
  author = "Asperti, Andrea and Ricciotti, Wilmer and 
            Coen, Claudio Sacerdoti and Tassi, Enrico",
  title = {{Formal Metatheory of Programming Languages in the Matita
            Interactive Theorem Prover}},
  journal = "Journal of Automated Reasoning",
  volume = "49",
  number = "3",
  pages = "427-451",
  year = "2012",
  abstract =
    "This paper is a report about the use of Matita, an interactive
    theorem prover under development at the University of Bologna, for
    the solution of the POPLmark Challenges, part 1a. We provide three
    different formalizations, including two direct solutions using
    pure de Bruijn and locally nameless encodings of bound variables,
    and a formalization using named variables, obtained by means of a
    sound translation to the locally nameless encoding. According to
    this experience, we also discuss some of the proof principles used
    in our solutions, which have led to the development of a
    generalized inversion tactic for Matita.",
  paper = "Aspe12b.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Strecker, Martin}
\begin{chunk}{axiom.bib}
@phdthesis{Stre99,
  author = "Strecker, Martin",
  title = {{Construction and Deduction in Type Theories}},
  school = "Unversitat Ulm",
  year = "1999",
  abstract =
    "This dissertation is concerned with interactive proof
    construction and automated proof search in type theories, in
    particular the Calculus of Constructions and its subsystems.
 
    Type theories can be conceived as expressive logics which combine
    a functiona programming language, strong typing and a higher-order
    logic. They are therefore a suitable formalism for specification
    and verification systems. However, due to their expressiveness, it
    is difficult to provide appropriate deductive support for type
    theories. This dissertation first examines general methods for
    proof construction in type theories and then explores how these
    methods can be refined to yield proof search procedures for
    specialized fragments of the language.

    Proof development in type theories usually requires the
    construction of a term having a given type in a given context. For
    the term to be constructed, a {\sl metavariable} is introduced
    which is successively instantiated in the course of the proof. A
    naive use of metavariables leads to problems, such as
    non-commutativity of reduction and instantiation and the
    generation of ill-typed terms during reduction. For solving these
    problems, a calculus with {\sl explicit substitutions} is
    introduced, and it is shown that this calculus preserves
    properties such as strong normalisation and decidability of typing.

    In order to obtain a calculus appropriate for proof search, the
    usual natural deduction presentation of type theories is replaced
    by a {\sl sequent style presentation}. It is shown that the
    calculus thus obtained is correct with respect to the original
    calculus. Completeness (proved with a cut-elimination argument) is
    shown for all predicative fragments of the lambda cube.

    This dissertation concludes with a discussion of some techniques
    that make proof search practically applicable, such as unification
    and pruning of the proof search space by exploiting
    impermutabilities of the sequent calculus.",
  paper = "Stre99.pdf"
}

\end{chunk}

\index{Lou, Zhaohui}
\begin{chunk}{axiom.bib}
@phdthesis{Loux90,
  author = "Lou, Zhaohui",
  title = {{An Extended Calculus of Constructions}},
  school = "University of Edinburgh",
  year = "1990",
  abstract =
    "This thesis presents and studies a unifying theory of dependent
    types ECC Extended Calculus of Constructions. ECC integrates
    Coquand-Huet's (impredicative) calculus of constructions and
    Martin-Lof's (predicative) type theory with universes, and turns
    out to be a strong and expressive calculus for formalization of
    mathematics, structured proof development and program specification.

    The meta theory of ECC is studied and we show that the calculus
    has good meta-theoretic properties. The main proof theoretic
    result is the {\sl strong normalization theorem} which makes
    explicit the predicativity of the predicative universes. The
    strong normalization result shows the proof theoretic consistency
    of the calculus; in particular, it implies the consistency of the
    embedded intuitionistic higher-order logic and the decidability of
    the theory. The meta-theoretic results establish the theoretical
    foundations both for pragmatic applications in theorem proving and
    program specification and for computer implementations of the
    theory. ECC has been implemente in the proof development system
    LEGO developed by Pollack.

    In ECC, dependent $\Sigma$ types are non-propositional types
    residing in the predicative universes and propositions are lifted
    as higher-level types as well. This solves the known difficulty
    that adding strong $\Sigma$ types to an impredicative system
    results in logical paradox and enables $\Sigma$ types to be used
    to express the intuitionistic notion of subsets. $\Sigma$ types
    together with type universes hence provide useful abstraction and
    module mechanisms for abstract description of mathematical
    theories and basic mechanisms for program specification and
    adequate formalization of abstract mathematics (e.g. abstract
    algebras and notions in category theory). A notion of (abstract)
    mathematical theory can be described and leads to a promising
    approach to {\sl abstract reasoning} and {\sl structured
    reasoning}. Program specifications can be expressed by $\Sigma$
    types, using propositions in the embedded logic to describe
    program properties (for example, by an equality reflection result,
    computational equality can be modeled by the propositional
    Leibniz's equality definable in the theory). These developments
    allow comprehensive structuring of formal or rigorous development
    of proofs and programs.

    Also discussed is how the calculus can be understood
    set-theoretically. We explain an $\omega-Set$ (realizability)
    model of the theory. In particular, propositions can be
    interpreted as partial equivalence relations and the predicative
    type universes as corresponding to large set universes.",
  paper = "Loux90.pdf"
}

\end{chunk}

\index{von Henke, F.W.}
\index{Luther, M.}
\index{Pfeifer, H.}
\index{Ruess, H.}
\index{Schwier, D.}
\index{Strecker, M.}
\index{Wagner, M.}
\begin{chunk}{axiom.bib}
@article{Henk96,
  author = "von Henke, F.W. and Luther, M. and Pfeifer, H. and Ruess, H.
            and Schwier, D. and Strecker, M. and Wagner, M.",
  title = {{The TYPELAB Specification and Verification Environment}},
  journal = "LNCS",
  volume = "1101",
  pages = "604-607",
  year = "1996",
  paper = "Henk96.pdf"
}

\end{chunk}

\index{von Henke, F.W.}
\index{Dold, A.}
\index{Ruess, H.}
\index{Schwier, D.}
\index{Strecker, M.}
\begin{chunk}{axiom.bib}
@article{Henk94,
  author = "von Henke, F.W. and Dold, A. and Ruess, H. and Schwier, D.
            and Strecker, M.",
  title = {{Construction and Deduction Methods for the Formal
            Development of Software}},
  journal = "LNCS",
  number = "1009",
  year = "1994",
  abstract =
    "In this paper we present an approach towards a framework based on
    the type theory ECC (Extended Calculus of Constructions) in which
    specifications, programs and operators for modular development by
    stepwise refinement can be formally described and reasoned
    about. We show that generic software development steps can be
    expressed as higher-order functions and demonstrate that proofs
    about their asserted effects can be carried out in the underlying
    logical calculus.

    For transformations requiring syntactic manipulations of objects,
    a two-level system comprising a Meta- and an Object-level is
    provided, and it is shown how transformations can be formalized
    that faithfully represent operators on the object level.",
  paper = "Henk94.pdf",
  keywords = "printed"
}  

\end{chunk}

\index{Kamareddine, Fairouz}
\index{Laan, Twan}
\index{Nederpelt, Rob}
\begin{chunk}{axiom.bib}
@book{Kama05,
  author = "Kamareddine, Fairouz and Laan, Twan and Nederpelt, Rob",
  title = {{A Modern Perspective on Type Theory}},
  comment = "Applied Logic Series 29",
  publisher = "Kluwer Academic Publishers",
  isbn = "1-4020-2335-9",
  year = "2005",
  paper = "Kama05.pdf"
}

\end{chunk}

\index{Andrews, Peter B.}
\begin{chunk}{axiom.bib}
@book{Andr02,
  author = "Andrews, Peter B.",
  title = {{An Introduction to Mathematical Logic and Type Theory: To
            Truth Through Proof}},
  comment = "Applied Logic Series 27",
  publisher = "Springer",
  year = "2002",
  isbn = "978-94-015-9934-4",
  paper = "Andr02.pdf"
}

\end{chunk}

\index{Strecker, M.}
\index{Luther, M.}
\index{von Henke, F.}
\begin{chunk}{axiom.bib}
@inbook{Stre98,
  author = "Strecker, M. and Luther, M. and von Henke, F.",
  title = {{Interactive and Automated Proof Construction in Type Theory}},
  publisher = "Springer",
  chapter = "3",
  pages = "73-96",
  isbn = "978-94-017-0435-9",
  year = "1998",
  abstract =
    "This chapter gives a survey of TYPELAB, a specification and
    verification environment that integrates interactive proof
    development and automated proof search. TYPELAB is based on a
    constructive type theory, the Calculus of Constructions, which can
    be understood as a combination of a typed $\lambda$-calculus and
    an expressive higher-order logic. Distinctive features of the type
    system are dependent function types ($\Pi$ types) for modeling
    polymorphism and dependent record types ($\Sigma$ types) for
    encoding specifications and mathematical theories.",
  paper = "Stre98.pdf"
}
  
\end{chunk}

\index{Kreitz, Christoph}
\begin{chunk}{axiom.bib}
@inbook{Krei98,
  author = "Kreitz, Christoph",
  title = {{Program Synthesis}},
  booktitle = "Automated Deduction -- A Basis for Applications (Vol III)",
  publisher = "Springer",
  year = "1998",
  chapter = "5",
  pages = "105-134",
  isbn = "978-94-017-0437-3",
  comment = "Applied Logic Series, volume 10",
  paper = "Krei98.pdf"
}

\end{chunk}

\index{Wadler, Philip}
\begin{chunk}{axiom.bib}
@misc{Wadl00,
  author = "Wadler, Philip",
  title = {{Proofs are Programs: 19th Century Logic and 21st Century
            Computing}},
  link = "\url{https://homepages.inf.ed.ac.uk/wadler/papers/frege/frege.pdf}",
  year = "2000",
  paper = "Wadl00.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Wang, Paul S.}
\begin{chunk}{axiom.bib}
@article{Wang80,
  author = "Wang, Paul S.",
  title = {{The EEZ-GCD Algorithm}},
  journal = "SIGSAM Bulletin",
  volume = "14",
  number = "2",
  pages = "50-60",
  year = "1980",
  abstract =
    "An enhanced gcd algorithm based on the EX-GCD algorithm is
    described. Implementational aspects are emphasized. It is
    generally faster and is particularly suited for computing gcd of
    sparse multivariate polynomials. The EEZ-GCD algorithm is
    characterized by the following features:
    \begin{enumerate}
    \item avoiding unlucky evaluations,
    \item predetermining the correct leading coefficient of the
    desired gcd,
    \item using the sparsity of the given polynomials to determine
    terms in the gcd and
    \item direct methods for dealing with the ``common divisor problem.''
    \end{enumerate}
    The common divisor problem occurs when the gcd has a different
    common divisor with each of the cofactors. The EZ-GCD algorithm
    does a square-free decomposition in this case. It can be avoided
    resulting in increased speed. One method is to use parallel p-adic
    construction of more than two factors. Machine examples with
    timing data are included.",
  paper = "Wang80.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Tsuji, Kuniaki}
\begin{chunk}{axiom.bib}
@article{Tsuj09,
  author = "Tsuji, Kuniaki",
  journal = "Journal of Symbolic Computation",
  title = {{An Improved EZ-GCD Algorithm for Multivariate Polynomials}},
  volume = "44",
  number = "1",
  year = "2009",
  pages = "99-110",
  abstract =
    "The EZ-GCD algorithm often has a bad-zero problem, which has a
    remarkable influence on polynomials with higher-degree terms. In
    this paper, by applying special ideals, the EZ-GCD algorithm for
    sparse polynomials is improved. This improved algorithm greatly
    reduces computational complexity because of the sparseness of
    polynomials. The author expects that the use of these ideals will
    be useful as a resolution for obtaining a GCD of sparse
    multivariate polynomials with higher-degree terms.",
  paper = "Tsuj09.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Sanuki, Masaru}
\index{Inaba, Daiju}
\index{Sasaki, Tateaki}
\begin{chunk}{axiom.bib}
@inproceedings{Sanu15,
  author = "Sanuki, Masaru and Inaba, Daiju and Sasaki, Tateaki",
  title = {{Computation of GCD of Sparse Multivariate Polynomials by
            Extended Hensel Construction}},
  booktitle = "17th Int. Symp. on Symbolic and Numeric Algorithms for
               Scientific Computing",
  publisher = "IEEE",
  year = "2015",
  abstract =
    "Let $F(x,u_1,\ldots,u_i)$ be a squarefree multivariate polynomial
    in main variable $x$ and sub-variables $u_1\ldots u_i$. We say
    that the leading coefficient (LC) of $F$ is singular if it
    vanishes at the origin of the sub-variables. A representative
    algorithm for non-sparse multivariate polynomial GCD is the EZ-GCD
    algorithm, which is based on the generalized Hensel construction
    (GHC). In order to apply the GHC easily, we requires 1) the LC of
    $F$ is non-singular, 2) $F(x,0,\ldots,0)$ is squarefree, and 3)
    the initial Hensel factor of GCD is ``lucky''. These requirements
    are usually satisfied by the ``nonzero substitution'', i.e. to
    shift the origin of sub-variables. However, the nonzero
    substitution may cause a drastic increase of the number of terms
    of $F$ if $F$ is sparse. In 1993, Sasaki and Kako proposed the
    extended Hensel construction (EHC) which does not perform the
    nonzero substitution even if the LC is singular. Using the EHC,
    Inaba implemented an algorithm of multivariate polynomial
    factorization and verified that it is very useful for sparse
    polynomials. In this paper, we apply the EHC for the computation
    of GCD of sparse multivariate polynomials. In order to find a
    lucky initial factor, we utilize the weighting of sub-variables,
    etc. Our naive implementation in Maple shows that our algorithm is
    comparable in performance to Maple's GCD routine base on the
    sparse interpolation.",
  paper = "Sanu15.pdf"
}
    
\end{chunk}

\index{Griesmer, J.H.}
\begin{chunk}{axiom.bib}
@article{Grie76,
  author = "Griesmer, James",
  title = "{{Symbolic Mathematical Computation: A Survey}},
  journal = "SIGSAM Bulletin",
  volume = "10",
  number = "2",
  pages = "30-32",
  year = "1976",
  paper = "Grie76.pdf",
  keywords = "axiomref"
}

\end{chunk}

\index{Luther, Marko}
\index{Strecker, Martin}
\begin{chunk}{axiom.bib}
@misc{Luth98,
  author = "Luther, Marko and Strecker, Martin",
  title = {{A Guided Tour through TYPELAB}},
  year = "1998",
  abstract =
    "This report gives a survey of TYPELAB, a specification and
    verification environment that integrates interactive proof
    development and automated proof search. TYPELAB is based on a
    constructive type theory, the Calculus of COnstructions, which can
    be understood as a combinations of a typed $\lambda$-calculus and
    an expressive higher-order logic. Distinctive features of the type
    system are dependent function types for modeling polymorphism and
    dependent record types for encoding specifications and
    mathematical theories. After presenting an extended example which
    demonstrates how program development by stepwise refinement of
    specifications can be carried out, the theory underlying the
    prover component of TYPELAB is described in detail. A calculus
    with metavariables and explicit substitutions is introduced, and
    the meta-theoretic properties of this calculus are
    analyzed. Furthermore, it is shown that this calculus provides an
    adequate foundation for automated proof search in fragments of the
    logic.",
  paper = "Luth98.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Ehrig, H.}
\index{Kreowski, H.-J.}
\index{Mahr, B.}
\index{Padawitz, P.}
\begin{chunk}{axiom.bib}
@article{Ehri82,
  author = "Ehrig, H. and Kreowski, H.-J. and Mahr, B. and Padawitz, P.",
  title = {{Algebraic Implementation of Abstract Data Types}},
  journal = "Theoretical Computer Science",
  volume = "20",
  pages = "209-263",
  year = "1982",
  abstract =
    "Starting with a review of the theory of algebraic specifications
    in the sense of the ADJ-group a new theory for algebraic
    implementation of abstract data types is presented.

    While main concepts of this new theory were given already at
    several conferences this paper provides the full theory of
    algebraic implementations developed in Berlin except of complexity
    considerations which are given in a separate paper. This new
    concept of algebraic implementations includes implementations for
    algorithms in specific programming languages and on the other hand
    it meets also the requirements for stepwise refinement of
    structured programs and software systems as introduced by Dijkstra
    and Wirth. On the syntactical level an algebraic implementation
    corresponds to a system of recursive programs while the semantical
    level is defined by algebraic constructions, called SYNTHESIS,
    RESTRICTION and IDENTIFICATION. Moreover the concept allows
    composition of implementations and a rigorous study of
    correctness. The main results of the paper are different kinds of
    correctness criteria which are applied to a number of illustrating
    examples including the implementation of sets by hash-tables. 
    Algebraic implementations of larger systems like a histogram or a
    parts system are given in separate case studies which, however,
    are not included in this paper.",
  paper = "Ehri82.pdf"
}

\end{chunk}

\index{de Moura, Leonardo}
\index{Avigad, Jeremy}
\index{Kong, Soonho}
\index{Roux, Cody}
\begin{chunk}{Mour15,
@misc{Mour15,
  author = "de Moura, Leonardo and Avigad, Jeremy and Kong, Soonho
            and Roux, Cody",
  title = {{Elaboration in Dependent Type Theory}},
  link = "\url{https://arxiv.org/pdf/1505.04324.pdf}",
  year = "2015",
  abstract =
    "To be usable in practice, interactive theoremprovers need to
    provide convenient and efficient means of writing expressions,
    definitions, and proofs. This involves inferring information that
    is often left implicit in an ordinary mathematical text, and
    resolving ambiguities in mathematical expressions. We refer to the
    rpocess of passing from a quasi-formal and partially-specified
    expression to a completely precise formal one as {\sl
    elaboration}. We describe an elaboration algorithm for dependent
    type theory that has been implemented in the Lean theorem
    prover. Lean's elaborator supports higher-order unification, type
    class inference, ad hoc overloading, insertion of coercions, the
    use of tactics, and the computational reduction of terms. The
    interactions between these components are subtle and complex, and
    the elaboration algorithm has been carefully designed to balance
    efficiency and usability. We describe the central design goals,
    and the means by which they are achieved.",
  paper = "Mour15.pdf",
  keywords = "printed"
}

\end{chunk}

\index{de Moura, Leonardo}
\index{Kong, Soonho}
\index{Avigad, Jeremy}
\index{van Doorn, Floris}
\index{von Raumer, Jakob}
@misc{Mour19,
  author = "de Moura, Leonardo and Kong, Soonho and Avigad, Jeremy
            and van Doorn, Floris and von Raumer, Jakob",
  title = {{The Lean Theorem Prover (system description)}},
  link = "\url{http://florisvandoorn.com/papers/lean_description.pdf}",
  year = "2019",
  abstract =
    "Lean is a new open source theorem prover being developed at
    Microsoft Research and Carnegie Mellon University, with a small
    trusted kernel based on dependent type theory. It aims to bridge the
    gap between interactive and automated theorem proving, by situating
    automated tools and methods in a framework that supports user
    interaction and the construction of fully specified axiomatic
    proofs. Lean is an ongoing and long-term effort, but it already
    provides many useful components, integrated development
    environments, and a rich API which can be used to embed it into
    other systems. It is currently being used to formalize category
    theory, homotopy type theory, and abstract algebra. We describe the
    project goals, system architecture, and main features, and we
    discuss applications and continuing work.",
  paper = "Mour19.pdf",
  keywords = "printed, DONE"
}

\end{chunk}

\index{Dybjer, Peter}
\begin{chunk}{axiom.bib}
\article{Dybj94,
  author = "Dybjer, Peter",
  title = {{Inductive Families}},
  journal = "Formal Aspects of Computing",
  volume = "6",
  number = "4",
  pages = "440-465",
  year = "1994",
  abstract =
    "A general formulation of inductive and recursive definitions in
    Martin-Lof's type theory is presented. It extends Backhouse's
    'Do-It-Yourself Type Theory' to include inductive definitions of
    families of sets and definitions of functions by recursion on the
    way elements of such sets are generated. The formulation is in
    natural deduction and is intended to be a natural generalization
    to type theory of Martin-Lof's theory of iterated inductive
    definitions of predicate logic.

    Formal criteria are given for correct formation and introduction
    rules of a new set former capturing definition by strictly
    positive, iterated, generalized induction. Moreover, there is an
    inversion principle for deriving elimination and equality rules
    from the formation and introduction rules. Finally, there is an
    alternative schematic presentation of definition by recursion. 

    The resulting theory is a flexible and powerful language for
    programming and constructive mathematics. We hint at the wealth of
    possible applications by showing several basic examples: predicate
    logic, generalized induction, and a formalization of the untyped
    lambda calculus.",
  paper = "Dybj94.pdf",
  keywords = "printed"
}

\end{chunk}  

\index{Selsam, Daniel}
\begin{chunk}{axiom.bib}
@misc{Sels19,
  author = "Selsam, Daniel",
  title = {{CS240H: A Standalone Proof-checker for the Lean Theorem Prover}},
  year = "2019",
  link = "\url{http://www.scs.stanford.edu/16wi-cs240h/projects/selsam.pdf}",
  paper = "Sels19.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Birkhoff, Garrett}
\begin{chunk}{axiom.bib}
@article{Birk35,
  author = "Birkhoff, Garrett",
  title {{On the Structure of Abstract Algebra}},
  journal = "Proc. of the Cambridge Philosophical Society",
  volume = "31",
  year = "1935",
  paper = "Birk35.pdf"
}

\end{chunk}

\index{de Moura, Leonardo}
\begin{chunk}{axiom.bib}
@misc{Mour16,
  author = "de Moura, Leonardo",
  title = {{The Lean Theorem Prover}},
  link = "\url{https://www.youtube.com/watch?v=69ytTKfSSgc}",
  conference = "PLSE '16",
  comment "video",
  year = "2016",
  keywords = "DONE"
}

\end{chunk}

\index{Awodey, Steve}
\begin{chunk}{axiom.bib}
@misc{Awod12,
  author = "Awodey, Steve",
  title = {{Category Theory Foundations. Lectures 1-4}},
  year = "2012",
  comment = "Oregon Programming Language Summer School 2013",
  link =
  "\url{http://www.youtube.com/watch?v=ZKmodCApZwk&list=PL8Ky8IYL8-Oh7awp0sqa82o7Ggt4AGhyf}"
}

\end{chunk}

\index{Weirich, Stephanie}
\index{Choudhury, Pritam}
\index{Voizard, Antoine}
\index{Eisenberg, Richard A.}
\begin{chunk}{axiom.bib}
@misc{Weir19,
  author = "Weirich, Stephanie and Choudhury, Pritam and Voizard,
            Antoine and Eisenberg, Richard A.",
  title = {{A Role for Dependent Types in Haskell (Extended Version)}},
  link = "\url{https://arxiv.org/pdf/1905.13706.pdf}",
  year = "2019",
  abstract = 
    "Modern Haskell supports zero-cost coercions, a mechanism where
    types that share the same run-time representation may be freely
    converted between. To make sure such conversions are safe and
    desirable, this feature relies on a mechanism of roles to prohibit
    invalid coercions. In this work, we show how to integrate roles
    with dependent type systems and prove, using the Coq proof
    assistant, that the resulting system is sound. We have designed
    this work as a foundation for the addition of dependent types to
    the Glasgow Haskell Compiler, but we also expect that it will be
    of use to designers of other dependently-typed languages who might
    want to adopt Haskell's safe coercion feature.",
  paper = "Weir19.pdf"
}

\end{chunk}

\index{Tennent, R.D.}
\begin{chunk}{axiom.bib}
@article{Tenn76,
  author = "Tennent, R.D.",
  title = {{The Denotational Semantics of Programming Languages}},
  journal = "Communications of the ACM",
  volume = "19",
  number = "8",
  pages = "437-453",
  year = "1976",
  abstract =
    "This paper is a tutorial introduction to the theory of
    programming language semantics developed by D. Scott and
    C. Strachey. The application of the theory to formal language
    specification is demonstrated and other applications are
    surveyed. The first language considered, LOOP, is very elementary
    and its definition merely introduces the notion and methodology of
    the approach. Then the semantic concepts of environments, stores,
    and continuations are introduced to model classes of programming
    language features and the underlying mathematical theory of
    computation due to Scott is motivated and outlined. Finally, the
    paper presents a formal definition of the language GEDANKEN.",
  paper = "Tenn76.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Elliott, Conal}
\begin{chunk}{axiom.bib}
@inproceedings{Elli17,
  author = "Elliott, Conal",
  title = {{Compiling to Categories}},
  booktitle = "Proc. ACM Program. Lang. Vol 1",
  publisher = "ACM",
  year = "2017",
  link = "\url{http://conal.net/papers/compiling-to-categories/compiling-to-categories.pdf}",
  abstract =
    "It is well-known that the simply typed lambda-calculul is modeled
    by any cartesian closed category (CCC). This correspondence
    suggests giving typed functional programs a variety of
    interpretations, each corresponding to a different category. A
    convenient way to realize this idea is as a collection of
    meaning-preserving transformations added to an existing compiler,
    such as GHC for Haskell. This paper describes automatic
    differentiation, incremental computation, and interval
    analysis. Each such interpretation is a category easily defined in
    Haskell (outside of the compiler). The general technique appears
    to provide a compelling alternative to deeply embedded
    domain-specific languages.",
  paper = "Elli17.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Harrison, John}
\begin{chunk}{axiom.bib}
@misc{Harr13,
  author = "Harrison, John",
  title = {{A Survey of Automated Theorem Proving}},
  year = "2013",
  link = "\url{https://www.lektorium.tv/lecture/14805}"
}

\end{chunk}

\index{Ganesalingam, M.}
\index{Gowers, W.T.}
\begin{chunk}{axiom.bib}
@article{Gane17,
  author = "Ganesalingam, M. and Gowers, W.T.",
  title = {{A Fully Automatic Theorem Prover with Human-Style Output}},
  journal = "J. Automated Reasoning",
  volume = "58",
  pages = "253-291",
  year = "2017",
  abstract =
    "This paper describes a program that solves elementary
    mathematical problems, mostly in metric space theory, and presents
    solutions that are hard to distinguish from solutions that might
    be written by a human mathematician.",
  paper = "Gane17.pdf"
}

\end{chunk}

\index{Ganzinger, Harald}
\begin{chunk}{axiom.bib}
@article{Ganz80,
  author = "Ganzinger, Harald",
  title = {{Transforming Denotational Semantics into Practical
            Attribute Grammars}},
  journal = "LNCS",
  volume = "54",
  pages = "1-69",
  year = "1980",
  paper = "Ganz80.pdf"
}

\end{chunk}

\index{Jones, Neil D.}
\index{Schmidt, David A.}
\begin{chunk}{axiom.bib}
@article{Jone80,
  author = "Jones, Neil D. and Schmidt, David A.",
  title = {{Compiler Generation from Denotational Semantics}},
  journal = "LNCS",
  volume = "54",
  pages = "70-93",
  year = "1980",
  abstract =
    "A methodology is described for generating provably correct
    compilers from denotational definitions of programming
    languages. An application is given to produce compilers into STM
    code (an STM or state transition machine is a flow-chart-like
    program, low-level enough to be translated into efficient code on
    conventional computers). First, a compiler $\phi:LAMC\rightarrow
    STM$ from a lambda calculus dialect is defined. Any denotational
    defintion $\Delta$ of language $L$ defines a map
    $\over{\rightarrow}{\Delta}:L\rightarrow LAMC$, so 
    $\over{\rightarrow}{\Delta}\circ \phi$ compiles $L$ into STM
    code. Correctness follows from the correctness of $\phi$.

    The algebraic framework of Morris, ADJ, etc. is used. The set of
    STMs is given an algebraic structure so any
    $\over{\rightarrow}{\Delta} \circ \phi$ may be specified by giving
    a derived operator on STM for each syntax rule of $L$.

    This approach yields quite redundant object programs, so the paper
    ends by describing two flow analytic optimization methods. The
    first analyzes an already-produced STM to obtain information about
    its runtime behaviour which is used to optimize the STM. The
    second analyzer the generated compiling scheme to determine
    runtime properties of object programs in general which a compiler
    can use to produce less redundant STMs.",
  paper = "Jone80.pdf"
}

\end{chunk}

\index{Raskovsky, Martin}
\index{Collier, Phil}
\begin{chunk}{axiom.bib}
@article{Rask80,
  author = "Raskovsky, Martin and Collier, Phil",
  title = {{From Standard to Implementation Denotational Semantics}},
  journal = "LNCS",
  volume = "54",
  pages = "94-139",
  year = "1980",
  abstract =
    "We are developing a compiler compiler. It takes as input the
    formal definition of a programming language in Denotational
    Semantics and produces as output a fairly efficient compiler
    written in a system programming language which in turn will
    produce code for a real machine. This work mainly deals with the
    code generation parts.",
  paper = "Rask80.pdf"
}

\end{chunk}

\index{Gaudel, M.C.}
\begin{chunk}{axiom.bib}
@article{Gaud80,
  author = "Gaudel, M.C.",
  title = {{Specification of Compilers as Abstract Data Type
            Representations}}, 
  journal = "LNCS",
  volume = "54",
  pages = "140-164",
  year = "1980",
  abstract =
    "This paper presents a method for specifying and proving
    compilers. This method is based on the algebraic data types
    ideas. The main points are:
    \begin{itemize}
    \item to each language is associated an algebraic abstract data type
    \item the semantic value of a program is given as a term of this
    data type
    \item the translation of the semantic values of source programs
    into semantic values of target programs is specified and proved as
    the representation of an algebrayc data type by another one.
    \end{itemize}
    A compiler generator, PERLUETTE, which accepts such specifications
    as input is described. The proof technic is discussed.",
  paper = "Gaud80.pdf"
}

\end{chunk}

\index{Thatcher, James W.}
\index{Wagner, Eric G.}
\index{Wright, Jesse B.}
\begin{chunk}{axiom.bib}
@article{That80,
  author = "Thatcher, James W. and Wagner, Eric G. and Wright, Jesse B.",
  title = {{More on Advice on Structuring Compilers and Proving Them
            Correct}}, 
  journal = "LNCS",
  volume = "54",
  pages = "165-188",
  year = "1980",
  paper = "That80.pdf"
}

\end{chunk}

\index{Madsen, Ole Lehrmann}
\begin{chunk}{axiom.bib}
@article{Mads80,
  author = "Madsen, Ole Lehrmann",
  title = {{On Defining Semantics by means of Extended Attribute Grammars}},
  journal = "LNCS",
  volume = "54",
  pages = "259-299",
  year = "1980",
  paper = "Mads80.pdf"
}

\end{chunk}

\index{Jones, Neil D.}
\index{Madsen, Michael}
\begin{chunk}{axiom.bib}
@article{Jone80a,
  author = "Jones, Neil D. and Madsen, Michael",
  title = {{Attribute-Influenced LR Parsing}},
  journal = "LNCS",
  volume = "54",
  pages = "393-407",
  year = "1980",
  abstract =
    "Methods are described which make it possible, when given an
    arbitrary attribute grammar (or AG),
    \begin{enumerate}
    \item to analyze the AG to determine which of its attributes may
    be computed during LR parsing,
    \item to augment the parser with instructions and data structures
    to compute many attributes during parsing,
    \item to use attribute values to assist the parsing process
    (e.g. to use symbol table information to decide whether P(X) is an
    array element or a function call).
    \end{enumerate}",
  paper = "Jone80a.pdf"
}

\end{chunk}

\index{Bernstein, Daniel J.}
\index{Yang, Bo-Yin}
\begin{chunk}{axiom.bib}
@misc{Bern19,
  author = "Bernstein, Daniel J. and Yang, Bo-Yin",
  title = {{Fast Constant-Time GCD and Modular Inversion}},
  year = "2019",
  link = "\url{https://gcd.crypto.to/safegcd-20190413.pdf}",
  abstract =
    "This paper introduces streamlined constant-time variants of
    Euclid's algorithm, both for polynomial inputs and for integer
    inputs. As concrete applications, this paper saves time in (1)
    modular inversion for Curve25519, which was previously believed to
    be handled much more efficiently by Fermat's method, and (2) key
    generation for the ntruhrss701 and sntrup4591761 lattice-based
    cryptosystems.",
  paper = "Bern19.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Necula, George Ciprian}
\begin{chunk}{axiom.bib}
@phdthesis{Necu98,
  author = "Necula, George Ciprian",
  title = {{Compiling with Proofs}},
  school = "Carnegie Mellon University",
  year = "1998",
  link = "\url{https://www.cs.cmu.edu/~rwh/theses/necula.pdf}",
  abstract =
    "One of the major challenges of building software systems is to
    ensure that the various components fit together in a well-defined
    manner. This problem is exacerbated by the recent advent of
    software components whose origin is unknown or inherently
    untrusted, such as mobile code or user extensions for operating
    system kernels or database servers. Such extensions are useful for
    implementing an efficient interaction model between a client and a
    server because several data exchanges between them can be saved at
    the cost of a single code exchange.

    In this dissertation, I propose to tackle such system integrity
    and security problems with techniques from mathematical logic and
    programming language semantics. I propose a framework, called 
    {\sl proof-carrying code}, in which the extension provider sends
    along with the extension code a representation of a formal proof
    that the code meets certain safety and correctness
    requirements. Then, the code receiver can ensure the safety of
    executing the extension by validating the attached proof. The
    major advantages of proof-carrying code are that it requires a
    simple trusted infrastructure and that it does not impose run-time
    penalties for the purpose of ensuring safety.

    In addition to the concept of proof-carrying code, this
    dissertation contributes the idea of certifying compilation. A 
    {\sl certifying compiler} emits, in addition to optimized target
    code, function specifications and loop invariants that enable a
    theorem-proving agent to prove non-trivial properties of the
    target code, such as type safety. Such a certifying compiler,
    along with a proof-generating theorem prover, is not only a
    convenient producer of proof-carrying code but also a powerful
    software-engineering tool. The certifier also acts as an effective
    referee for the correctness of each compilation, thus simplifying
    considerably compiler testing and maintenance.

    A complete system for proof-carrying code must also contain a 
    {\sl proof-generating theorem prover} for the purpose of producing
    the attached proofs of safety. This dissertation shows how
    standard decision procedures can be adapted so that they can
    produce detailed proofs of the proved predicates and also how
    these proofs can be encoded compactly and checked
    efficiently. Just like for the certifying compiler, a
    proof-generating theorem prover has significant software
    engineering advantages over a traditional prover. In this case, a
    simple proof checker can ensure the soundness of each successful
    proving task and indirectly assist in testing and maintenance of
    the theorem prover.",
  paper = "Necu98.pdf"
}

\end{chunk}

\index{Cramer, Marcos}
\index{Koepke, Peter}
\index{Schroder, Bernhard}
\begin{chunk}{axiom.bib}
@article{Cram11,
  author = "Cramer, Marcos and Koepke, Peter and Schroder, Bernhard",
  title = {{Parsing and Disambiguation of Symbolic Mathematics in the
            Naproche System}},
  journal = "LNAI",
  number = "6824",
  pages = "180-195",
  year = "2011",
  publisher = "Springer",
  abstract = 
    "The Naproche system is a system for linguistically analysing and
    proof-checking mathematical texts written in a controlled natural
    language. The aim is to have an input language that is as close as
    possible to the language that mathematicians actually use when
    writing textbooks or papers.

    Mathematical texts consist of a combination of natural language
    and symbolic mathematics, with symbolic mathematics obeying its
    own syntactic rules. We discuss the difficulties that a program
    for parsing and disambiguating symbolic mathematics must face and
    present how these difficulties have been tackled in the Naproche
    system. One of these difficulties is the fact that information
    provided in the preceding context -- including information
    provided in natural language -- can influence the way a symbolic
    expression has to be disambiguated.",
  paper = "Cram11.pdf"
}

\end{chunk}

\index{Altenkirch, Thorsten}
\index{McBride, Conor}
\index{Swierstra, Wouter}
\begin{chunk}{axiom.bib}
@inproceedings{Alte07,
  author = "Altenkirch, Thorsten and McBride, Conor and Swierstra, Wouter",
  title = {{Observational Equality, Now!}},
  booktitle = "ACM Workshop Programming Languages meets Program
               Verification", 
  publisher = "ACM",
  pages = "57-68",
  year = "2007"
}

\end{chunk}

\index{Asperti, Andrea}
\index{Ricciotti, Wilmer}
\index{Coen, Claudio Sacerdoti}
\index{Tassi, Enrico}
\begin{chunk}{axiom.bib}
@article{Aspe09a,
  author = "Asperti, Andrea and Ricciotti, Wilmer and Coer, Claudio
            Sacerdoti and Tassi, Enrico",
  title = {{Hints in Unification}},
  journal = "LNCS",
  volume = "5674",
  pages = "84-98",
  year = "2009",
  isbn = "978-3-642-03358-2",
  abstract =
    "Several mechanisms such as Canonical Structures, Type Classes, or
    Pullbacks have been recently introduced with the aim to improve the
    power and flexibility of the type inference algorithm for interactive
    theorem provers. We claim that all these mechanisms are particular
    instances of a simpler and more general technique, just consisting in
    providing suitable hints to the unification procedure underlying type
    inference. This allows a simple, modular and not intrusive
    implementation of all the above mentioned techniques, opening at the
    same time innovative and unexpected perspectives on its possible
    applications.",
  paper = "Aspe09a.pdf"
}

\end{chunk}

\index{Avigad, Jeremy}
\begin{chunk}{axiom.bib}
@article{Avig07,
  author = "Avigad, Jeremy",
  title = {{A Formally Verified Proof of the Prime Number Theorem}},
  journal = "ACM Trans. Comput. Logic",
  volume = "9",
  number = "1",
  pages = "2",
  year = "2007"
}

\end{chunk}

\index{Barendregt, Hendrik Pieter}
\begin{chunk}{axiom.bib}
@article{Bare91,
  author = "Barendregt, Hendrik Pieter",
  title = {{An Introduction to Generalized Type Systems}},
  journal = "Journal of Functional Programming",
  volume = "1",
  number = "2",
  year = "1991",
  pages = "125-154",
  abstract =
    "Programming languages often come with type systems. Some of these are
    simple, others are sophisticated. As a stylistic representation of
    types in programming languages several versions of typed lambda
    calculus are studied. During the last 20 years many of these systems
    have appeared, so there is some need of classification. Working
    towards a taxonomy, Barendregt (1991) gives a fine-structure of the
    theory of constructions (Coquand and Huet 1988) in the form of a
    canonical cube of eight type systems ordered by inclusion. Berardi
    (1988) and Terlouw (1988) have independently generalized the method of
    constructing systems in the -cube. Moreover, Berardi (1988, 1990)
    showed that the generalized type systems are flexible enough to
    describe many logical systems. In that way the well-known
    propositions-as-types interpretation obtains a nice canonical form.",
  paper = "Bare91.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Bertot, Yves}
\index{Cast\'eran, Pierre}
\begin{chunk}{axiom.bib}
@book{Bert04,
  author = {Bertot, Yves Cast\'eran, Pierre},
  title = {{Interactive Theorem Proving and Program Development}},
  publisher = "Springer",
  year = "2004",
  isbn = "3-540-20854-2",
  abstract = "
    Coq is an interactive proof assistant for the development of
    mathematical theories and formally certified software. It is based on
    a theory called the calculus of inductive constructions, a variant of
    type theory.

    This book provides a pragmatic introduction to the development of
    proofs and certified programs using Coq. With its large collection of
    examples and exercies it is an invaluable tool for researchers,
    students, and engineers interested in formal methods and the
    development of zero-fault software."
}

\end{chunk}

\index{Bertot, Yves}
\index{Gonthier, Georges}
\index{Biha, Sidi Ould}
\index{Pasca, Ioana}
\begin{chunk}{axiom.bib}
@inproceedings{Bert08,
  author = "Bertot, Yves and Gonthier, Georges and Biha, Sidi Ould and
            Pasca, Ioana", 
  title = {{Canonical Big Operators}},
  booktitle = "Theorem Proving in Higher Order Logics",
  publisher = "Springer",
  pages = "86-101",
  year = "2008"
}

\end{chunk}

\index{Blanqui, Frederic}
\index{jouannaud, Jean-Pierre}
\index{Okada, Mitsuhiro}
\begin{chunk}{axiom.bib}
@inproceedings{Blan99,
  author = "Blanqui, Frederic and jouannaud, Jean-Pierre and Okada, Mitsuhiro",
  title = {{The Calculus of Algebraic Constructions}},
  booktitle = "Rewriting Techniques and Applications RTA-99",
  year = "1999",
  publisher = "LNCS 1631",
  link = "\url{https://hal.inria.fr/inria-00105545v1/document}",
  abstract = 
    "This paper is concerned with the foundations of the Calculus of
    Algebraic Constructions (CAC), an extension of the Calculus of
    Constructions by inductive data types. CAC generalizes inductive 
    types equipped with higher-order primitive recursion, by providing
    definition s of functions by pattern-matching which capture recursor
    definitions for arbitrary non-dependent and non-polymorphic inductive
    types satisfying a strictly positivity condition. CAC also
    generalizes the first-order framework of abstract data types by
    providing dependent types and higher-order rewrite rules.",
  paper = "Blan99.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Church, Alonzo}
\begin{chunk}{axiom.bib}
@article{Chur40,
  author = "Church, Alonzo",
  title = {{A Formulation of the Simple Theory of Types}},
  journal = "J. of Symbolic Logic",
  volume = "5",
  number = "2",
  year = "1940",
  pages = "56-68",
  abstract =
    "The purpose of the present paper is to give a formulation of the 
    simple theory of types which incorporates certain features of the
    calculus of $\lambda$-conversion. A complete incorporation of the
    calculus of $\lambda$-conversion into the theory of types is
    impossible if we require that $\lambda x$ and juxtaposition shall
    retain their respective meanings as an abstraction operator and as
    denoting the application of function to argument. But the present
    partial incorporation has certain advantages from the point of view of
    type theory and is offered as being of interest on this basis
    (whatever may be thought of the finally satisfactory character of the
    theory of types as a foundation for logic and mathematics).",
  paper = "Chur40.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Ciolli, Gianni}
\index{Gentili, Graziano}
\index{Maggesi, Marco}
\begin{chunk}{axiom.bib}
@article{Ciol11,
  author = "Ciolli, Gianni and Gentili, Graziano and Maggesi, Marco",
  title = {{A Certified Proof of the Cartan Fixed Point Theorem}},
  journal = "J. Autom. Reasoning",
  volume = "47",
  number = "3",
  pages = "319-336",
  year = "2011"
}

\end{chunk}

\index{Constable, R.L.}
\index{Allen, S.F.}
\index{Bromley, H.M.}
\index{Cremer, J.F.}
\index{Harper, R.W.}
\index{Howe, D.J.}
\index{Knoblock, T.B.}
\index{Mendler, N.P.}
\index{Panagaden, P.}
\index{Tsaaki, J.T.}
\index{Smith, S.F.}
\begin{chunk}{axiom.bib}
@book{Cons85,
  author = "Constable, R.L. and Allen, S.F. and Bromley, H.M. and Cremer, J.F.
            and Harper, R.W. and Howe, D.J. and Knoblock, T.B. and 
            Mendler, N.P. and Panagaden, P. and Tsaaki, J.T. and Smith, S.F.",
  title = {{Implementing Mathematics with The Nuprl Proof Development System}},
  publisher = "Prentice-Hall",
  year = "1985"
}

\end{chunk}

\index{Coquuand, Thierry}
\index{Huet, Gerard}
\begin{chunk}{axiom.bib}
@incollection{Coqu88,
  author = "Coquuand, Thierry and Huet, Gerard",
  title = {{The Calculus of Constructions}},
  booktitle = "Information and Computation, Volume 76",
  year = "1988",
  publisher = "Academic Press",
  paper = "Coqu88.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Coquand, Thierry}
\index{Paulin, Christine}
\begin{chunk}{axiom.bib}
@inproceedings{Coqu90,
  author = "Coquand, Thierry and Paulin, Christine",
  title = {{Inductively Defined Types}},
  booktitle = "Int. Conf. on Computer Logic",
  publisher = "Springer",
  pages = "50-66",
  year = "1990"
}

\end{chunk}

\index{Cramer, Marcos}
\index{Koepke, Peter}
\index{Schroder, Bernhard}
\begin{chunk}{axiom.bib}
@article{Cram11,
  author = "Cramer, Marcos and Koepke, Peter and Schroder, Bernhard",
  title = {{Parsing and Disambiguation of Symbolic Mathematics in the
            Naproche System}},
  journal = "LNAI",
  number = "6824",
  pages = "180-195",
  year = "2011",
  publisher = "Springer",
  abstract = 
    "The Naproche system is a system for linguistically analysing and
    proof-checking mathematical texts written in a controlled natural
    language. The aim is to have an input language that is as close as
    possible to the language that mathematicians actually use when
    writing textbooks or papers.

    Mathematical texts consist of a combination of natural language
    and symbolic mathematics, with symbolic mathematics obeying its
    own syntactic rules. We discuss the difficulties that a program
    for parsing and disambiguating symbolic mathematics must face and
    present how these difficulties have been tackled in the Naproche
    system. One of these difficulties is the fact that information
    provided in the preceding context -- including information
    provided in natural language -- can influence the way a symbolic
    expression has to be disambiguated.",
  paper = "Cram11.pdf"
}

\end{chunk}

\index{Hales, Thomas C.}
\index{Harrison, John}
\index{McLaughlin, Sean}
\index{Nipkow, Tobias}
\index{Obua, Steven}
\index{Zumkeller, Roland}
\begin{chunk}{axiom.bib}
@article{Hale10,
  author = "Hales, Thomas C. and Harrison, John and McLaughlin, Sean
            and Nipkow, Tobias and Obua, Steven and Zumkeller,
	    Roland",
  title = {{A Revision of the Proof of the Kepler Conjecture}},
  jounal = "44",
  volume = "1",
  pages = "1-34",
  year = "2010"
}

\end{chunk}

\index{Harrison, John}
\begin{chunk}{axiom.bib}
@inproceedings{Harr96a,
  author = "Harrison, John",
  title = {{HOL Light: A Tutorial Introduction}},
  booktitle = "First Int. Conf. on Formal Methods in Computer-Aided Design",
  publisher = unknownn",
  pages = "265-269",
  year = "1996"
}

\end{chunk}

\index{Harrison, John}
\begin{chunk}{axiom.bib}
@article{Harr09a,
  author = "Harrison, John",
  title = {{A Formalized Proof of Dirichlet's Theorem on Primes in
            Arithmetic Progression}},
  journal = "J. Formaliz. Reason.",
  volume = "2",
  number = "1",
  pages = "63-83",
  year = "2009"
}

\end{chunk}

\index{Harrison, John}
\begin{chunk}{axiom.bib}
@article{Harr09b,
  author = "Harrison, John",
  title = {{Formalizing an Analytic Proof of the Prime Number Theorem}},
  journal = "J. Automated Reasoning",
  volume = "43",
  pages = "243-261",
  year = "2009"
}

\end{chunk}

\index{Holzl, Johannes}
\index{Heller, Armin}
\begin{chunk}{axiom.bib}
@inproceedings{Holz11,
  author = "Holzl, Johannes and Heller, Armin",
  title = {{Three Chapters of Measure Theory in Isabelle / HOL}},
  booktitle = "Interactive Theorem Proving",
  publisher = "Springer",
  pages = "135-151",
  year = "2011"
}

\end{chunk}

\index{Huet, Gerard}
\index{Saibi, Amokrane}
\begin{chunk}{axiom.bib}
@inproceedings{Huet00,
  author = "Huet, Gerard and Saibi, Amokrane",
  title = {{Constructive Category Theory}},
  booktitle = "Proof, Language, and Interaction: Essays in Honour of
               Robin Milner",
  publisher = "MIT Press",
  pages = "235-275",
  year = "2000"
}

\end{chunk}

\index{Kornilowicz, Artur}
\begin{chunk}{axiom.bib}
@article{Korn07,
  author = "Kornilowicz, Artur",
  title = {{A Proof of the Jordan Curve Theorem via the Brouwer Fixed
            Point Theorem}},
  journal = "Mechanized Mathematics and Its Applications",
  volme = "6",
  number = "1",
  pages = "33-40",
  year = "2007"
}

\end{chunk}

\index{Martin-L\"of, P.}
\begin{chunk}{axiom.bib}
@inproceedings{Mart73,
  author = "Martin-L\"of, P.",
  title = {{An Intuitionistic Theory of Types: Predicative Part}},
  booktitle = "Logic Colloqium '73",
  publisher = "North-Holland",
  year = "1973"
}

\end{chunk}

\index{Mhamdi, Tarek}
\index{Hasan, Osman}
\index{Tahar, Sofiene}
\begin{chunk}{axiom.bib}
@inproceedings{Mham11,
  author = "Mhamdi, Tarek and Hasan, Osman and Tahar, Sofiene",
  title = {{Formalization of Entropy Measure in HOL}},
  booktitle = "Interactve Theorem Proving",
  publisher = "Springer",
  pages = "233-248",
  year = "2011"
}

\end{chunk}

\index{Nathanson, Melvyn B.}
\begin{chunk}{axiom.bib}
@article{Nath08,
  author = "Nathanson, Melvyn B.",
  title = {{Desperately Seeing Mathematical Proof}},
  journal = "Notices of the American Math. Society",
  volume = "55",
  number = "7",
  pages = "773",
  year = "2008"
}

\end{chunk}

\index{Nipkow, Tobias}
\index{Paulson, Lawrence C.}
\index{Wenzel, Markus}
\begin{chunk}{axiom.bib}
@book{Nipk02a,
  author = "Nipkow, Tobias and Paulson, Lawrence C. and Wenzel, Markus",
  title = {{Isabelle / HOL. A Proof Assistant for Higher-Order Logic}},
  publisher = "Springer",
  year = "2002"
}

\end{chunk}

\index{O'Connor, Russell}
\begin{chunk}{axiom.bib}
@inproceedings{Ocon05,
  author = "O'Connor, Russell",
  title = {{Essential Incompleteness of Arithmetic Verified by Coq}},
  booktitle = "Theorem Proving in Higher Order Logics",
  publisher = "Springer",
  pages = "245-260",
  year = "2005"
}

\end{chunk}

\index{Dowek, Gilles}
\begin{chunk}{axiom.bib}
@inbook{Dowe01,
  author = "Dowek, Gilles",
  title = {{Handbook of Automated Reasoning, Vol II}},
  publisher = "Elsevier Science",
  year = "2001",
  chapter = "16",
  pages = "1009-1062"
}

\end{chunk}

\index{Feit, Walter}
\index{Thompson, John G.}
\begin{chunk}{axiom.bib}
@article{Feit63,
  author = "Feit, Walter and Thompson, John G.",
  title = {{Solvability of Groups of Odd Order}},
  journal = "Pacific Journal of Mathematics",
  volume = "13",
  pages = "775-1029",
  year = "1963"
}

\end{chunk}

\index{Ganesalingam, Mohan}
\begin{chunk}{axiom.bib}
@phdthesis{Gane09,
  author = "Ganesalingam, Mohan",
  title = {{The Language of Mathematics}},
  school = "University of Cambridge",
  year = "2009"
}

\end{chunk}

\index{Gonthier, Goerges}
\begin{chunk}{axiom.bib}
@article{Gont08,
  author = "Gonthier, Goerges",
  title = {{Formal Proof -- The Four Color Theorem}},
  journal = "Notices Amer. Math. Soc.",
  volume = "55",
  number = "11",
  pages = "1382-1393",
  year = "2008"
}

\end{chunk}

\index{Gonthier, Goerges}
\begin{chunk}{axiom.bib}
@inproceedings{Gont11,
  author = "Gonthier, Goerges",
  title = {{Advances in the Formalization of the Odd Order Theorem}},
  booktitle = "Interactive Theorem Proving",
  publisher = "Springer",
  pages = "2",
  year = "2011"
}

\end{chunk}

\index{Gonthier, Goerges}
\begin{chunk}{axiom.bib}
@inproceedings{Gont11a,
  author = "Gonthier, Goerges",
  title = {{Point-Free, Set-Free Concrete Linear Algebra}},
  booktitle = "Interactive Theorem Proving",
  publisher = "Springer",
  pages = "103-118",
  year = "2011"
}

\end{chunk}

\index{Gonthier, Goerges}
\index{Mahboubi, Assia}
\begin{chunk}{axiom.bib}
@article{Gont10,
  author = "Gonthier, Goerges and Mahboubi, Assia",
  title = {{An Introduction to Small Scale Reflection in Coq}},
  journal = "J. Formaliz. Reason.",
  volume = "3",
  number = "2",
  pages = 95-152",
  year = "2010"
}

\end{chunk}

\index{Gonthier, Goerges}
\index{Mahboubi, Assia}
\index{Rideau, Laurence}
\index{Tassi, Enrico}
\index{Thery, Laurent}
\begin{chunk}{axiom.bib}
@inproceedings{Gont07,
  author = "Gonthier, Goerges and Mahboubi, Assia and Rideau, Laurence
            and Tassi, Enrico and Thery, Laurent",
  title = {{A Modular Formalisation of Finite Group Theory}},
  booktitle = "Theorem Proving in Higher Order Logics",
  publisher = "Springer",
  pages = 86-101",
  year = "2007"
}

\end{chunk}

\index{Gonthier, Goerges}
\index{Ziliani, Beta}
\index{Nanevski, Aleksandar}
\index{Dreyer, Derek}
\begin{chunk}{axiom.bib}
@inproceedings{Gont11b,
  author = "Gonthier, Goerges and Ziliani, Beta and Nanevski, Aleksandar
            and Dreyer, Derek",
  title = {{How to make Ad Hoc Proof Automation less Ad Hoc}},
  booktitle = "Int. Conf. on Functional Programming",
  publisher = "ACM",
  pages = "163-175",
  year = "2011"
}

\end{chunk}

\index{Grabowski, Adam}
\index{Kornilowicz, Artur}
\index{Naumowicz, Adam}
\begin{chunk}{axiom.bib}
@article{Grab10,
  author = "Grabowski, Adam and Kornilowicz, Artur and Naumowicz, Adam",
  title = {{Mizar in a Nutshell}},
  journal = "J. Formaliz. Reason.",
  volume = "3",
  number = "2",
  pages = "153-245",
  year = "2010"
}

\end{chunk}

\index{Hales, Thomas C.}
\begin{chunk}{axiom.bib}
@article{Hale07,
  author = "Hales, Thomas C.",
  title = {{The Jordan Curve Theorem, Formally and Informally}},
  journal = "Amer. Math. Monthly",
  volume = "114",
  number = "10",
  pages = "882-894",
  year = "2007"
}

\end{chunk}

\index{Saibi, Amokrane}
\begin{chunk}{axiom.bib}
@inproceedings{Saib97,
  author = "Saibi, Amokrane",
  title = {{Typing Algorithm in Type Theory with Inheritance}},
  booktitle = "Symp. on Principles of Programming Languages",
  publisher = "ACM",
  pages = "292-301",
  year = "1997"
}

\end{chunk}

\index{Shankar, Natarjan}
\index{Owre, Sam}
\begin{chunk}{axiom.bib}
@inproceedings{Shan00,
  author = "Shankar, Natarjan and Owre, Sam",
  title = {{Principles and Pragmatics of Subtyping in PVS}},
  booktitle = "Recent Trends in Algebraic Development Techniques",
  publisher = "Springer",
  pages = "37-52",
  year = "2000"
}

\end{chunk}

\index{Strub, Pierre-Yves}
\begin{chunk}{axiom.bib}
@inproceedings{Stru10,
  author = "Strub, Pierre-Yves",
  title = {{Coq Modulo Theory}},
  booktitle = "19th Annual Conf. on Computer Science Logic",
  publisher = "Springer",
  pages = "549-643",
  year = "2010"
}

\end{chunk}

\index{Troelstra, A.S.}
\index{van Dalen, Dirk}
\begin{chunk}{axiom.bib}
@book{Troe88,
  author = "Troelstra, A.S. and van Dalen, Dirk",
  title = {{Constructivism in Mathematics, Vol 2}},
  publisher = "North-Holland",
  year = "1988"
}

\end{chunk}

\index{Tait, William W.}
\begin{chunk}{axiom.bib}
@article{Tait86,
  author = "Tait, William W.",
  title = {{Truth and Proof: The Platonism of Mathematics}},
  link = "\url{logic.harvard.edu/EFI_Tait_PlatonisminMathematics.pdf}",
  journals = "Synthese",
  volume = "69",
  pages = "341-370",
  year = "1986",
  paper = "Tait86.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Werner, Benjamin}
\begin{chunk}{axiom.bib}
@inbook{Wern97,
  author = "Werner, Benjamin",
  title = {{Sets in Types, Types in Sets}},
  booktitle = "Theoretical Aspects of Computer Software",
  publisher = "Springer",
  chapter = "unknown",
  pages = "530-546",
  year = "1997"
}

\end{chunk}

\index{Parigot, Michel}
\begin{chunk}{axiom.bib}
@article{Pari92,
  author = "Parigot, Michel",
  title = {{$\lambda\mu$-Calculus: An Algorithmic Interpretation of
             Classical Natural Deduction}},
  journal = "LNCS",
  volume = "624",
  pages = "190-201",
  year = "1992",
  paper = "Pari92.pdf"
}

\end{chunk}

\index{Parigot, Michel}
\begin{chunk}{axiom.bib}
@article{Pari92a,
  author = "Parigot, Michel",
  title = {{Recursive Programming with Proofs}},
  journal = "Theoretical Computer Science",
  volume = "94",
  pages = "335-356",
  year = "1992",
  paper = "Pari92a.pdf"
}

\end{chunk}

\index{Murthy, Chetan R.}
\begin{chunk}{axiom.bib}
@techreport{Murt91,
  author = "Murthy, Chetan R.",
  title = {{Classical Proofs as Programs: How, What and Why}},
  type = "technical report",
  institution = "Cornell University",
  number = "TR91-1215",
  year = "1991",
  abstract =
    "We recapitulate Friedman's conservative extension result of
    (suitable) classical over constructive systems for $\prod_2^0$
    sentences, viewing it in two lights: as a translation of programs
    from an almost-functional language (with $C$) back to its
    functional core, and as a translation of a constructive logic for
    a functional language to a classical logic for an
    almost-functional language. We investigate the computational
    properties of the translation and of classical proofs and
    characterize the classical proofs which give constructions in
    concrete, computational terms, rather than logical terms. We
    characterize different versions of Friedman's translation as
    translating slightly different almost-functional languages to a
    functional language, thus giving a general method for arriving at
    a sound reduction semantics for an almost-functional language with
    a mixture of eager and lazy constructors and destructors, as well
    as integers, pairs, unions, etc. Finally, we describe how to use
    classical reasoning in a disciplined manner in giving classical
    (yet constructivizable) proofs of sentences of greater complexity
    than $\prod_2^0$. This direction offers the possibility of
    applying classical reasoning to more general programming problems.",
  paper = "Murt91.pdf"
}

\end{chunk}

\index{Turing, A. M.}
\begin{chunk}{axiom.bib}
@misc{Turi48,
  author = "Turing, A. M.",
  title = {{Intelligent Machinery}},
  year = "1948",
  link = "\url{https://weightagnostic.github.io/papers/turning1948.pdf}",
  abstract =
    "The possible ways in which machinery might be made to show
    intelligent behaviour are discussed. The analogy with the human
    brain is used as a guiding principle. It is pointed out that the
    potentialities of the human intelligence can only be realized if
    suitable education is provided. The investigation mainly centres
    round an analogous teaching process applied to machines. The idea
    of an unorganized machine is defined, and it is suggested that the
    infant human cortex is of this nature. Simple examples of such
    machines are given, and their education by means of rewards and
    punishments is discussed. In one case the education process is
    carried through until the organization is similar to that of an
    ACE.", 
  paper = "Turi48.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Bailleux, Olivier}
\begin{chunk}{axiom.bib}
@misc{Bail19,
  author = "Bailleux, Olivier",
  title = {{Subsumption-driven Clause Learning with DPLL+Restarts}},
  year = "2019",
  link = "\url{https://arxiv.org/pdf/1906.07508.pdf}",
  abstract =
    "We propose to use a DPLL+restart to solve SAT instances by
    successive simplifications based on the production of clauses that
    subsume the initial clauses. We show that this approach allows the
    refutation of pebbling formulae in polynomial time and linear
    space, as effectvely as with a CDCL solver.",
  paper = "Bail19.pdf"
}

\end{chunk}

\index{Clark, Kevin}
\begin{chunk}{axiom.bib}
@misc{Clar11,
  author = "Clark, Kevin",
  title = {{An Algorithm that Decides PRIMES in Polynomial Time}}
, year = "2011",
  link =
   "\url{https://sites.math.washington.edu/~morrow/336_11/papers/kevin.pdf}",
  paper = "Clar11.pdf",
  keywords = "printed"
}

\end{chunk}

\index{van Tonder, Rijnard}
\index{Le Goues, Claire}
\begin{chunk}{axiom.bib}
@inproceedings{Tond19,
  author = "van Tonder, Rijnard and Le Goues, Claire",
  title = {{Lightweight Multi-Language Syntax Transformation with
            Parser Parser Combinators}},
  booktitle = "PLDI",
  publisher = "ACM",
  isbn = "978-1-4503-6712-7",
  year = "2019",
  paper = "Tond19.pdf"
}

\end{chunk}

\index{Qiu, Xiaokang}
\index{Garg, Pranav}
\index{Stefanescu, Andrei}
\index{Madhusudan, P.}
\begin{chunk}{axiom.bib}
@misc{Qiux13,
  author = "Qiu, Xiaokang and Garg, Pranav and Stefanescu, Andrei and
            Madhusudan, P.",
  title = {{Natural Proofs for Structure, Data, and Separation}},
  year = "2013",
  link = "\url{http://madhu.cs.illinois.edu/dryad_full_version.pdf}",
  abstract =
    "We propose {\sl natural proofs} for reasoning with programs that
    manipulate data-structures against complex specifications --
    specifications that describe the structure of the heap, the data
    stored within it, and separation and framing of
    sub-structures. Natural proofs are a subclass of proofs that are
    amenable to completely automated reasoning, that provide sound but
    incomplete procedures, and that capture common reasoning tactics
    in program verification. We develop a dialect of separation logic
    over heaps, called DRYAD, with recursive definitions that avoids
    explicit quantification. We develop ways to reason with heaplets
    using classical logic over the theory of sets, and develop natural
    proofs for reasoning using proof tactics involving disciplined
    unfoldings and formula abstractions. Natural proofs are encoded
    into decidable theories of first-order logic so as to be
    discharged using SMT solvers.

    We also implement the technique and show that a large class of
    more than 100 correct programs that manipulate data-structures are
    amenable to full functional correctness using the proposed natural
    proof method. These programs are drawn from a variety of sources
    including standard data-structures, the Schorr-Waite algorithm for
    garbage collection, a large number of low-level C routines from
    the Glib library, the OpenBSD library and the Linux kernel, and
    routines from a secure verified OS-browser project. Our work is
    the first that we know of that can handle such a wide range of
    full functional verification properties of heaps automatically,
    given pre/post and loop invariant annotations. we believe that
    this work paves the way for the deductive verification technology
    to be used by programmers who do not (and need not) understand the
    internals of the underlying logic solvers, significantly
    increasing their applicability in building reliable systems.",
  paper = "Qiux13.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Propp, James}
\begin{chunk}{axiom.bib}
@misc{Prop13,
  author = "Propp, James",
  title = {{Real Analysis in Reverse}},
  year = "2013",
  link = "\url{https://arxiv.org/pdf/1204.4483.pdf}",
  abstract =
    "Many of the theorems of real analysis, against the background of
    the ordered field axioms, are equivalent to Dedekind completeness,
    and hence can serve as completeness axioms for the reals. In the
    course of demonstrating this, the article offers a tour of some
    less-familiar ordered fields, provides some of the relevant
    history, and considers pedagogical implications.",
  paper = "Prop13.pdf"
}

\end{chunk}

\index{Coquand, Thierry}
\begin{chunk}{axiom.bib}
@techreport{Coqu86,
  author = "Coquand, Thierry",
  title = {{An Analysis of Girard's Paradox}},
  year = "1986",
  institution = "INRIA Centre de Rocquencourt",
  number = "531",
  abstract =
    "We study the consistency of a few formal systems specially some
    extensions of Church's calculus and the construction system. We
    show that Church's calculus is not compatible with the notion of
    second-order type. We apply this result for showing that the
    calculus of construction wit four levels is inconsistent. We
    suggest finally some consistent extensions of these two calculi.",
  paper = "Coqu86.pdf"
}

\end{chunk}

\index{Grossman, Dan}
\begin{chunk}{axiom.bib}
@inproceedings{Gros02,
  author = "Grossman, Dan",
  title = {{Existential Types for Imperative Languages}},
  booktitle = "Euro. Symp. on Prog. Langs. and Systems",
  publisher = "Springer-Verlag",
  pages = "21-35",
  year = "2002",
  isbn = "3-540-43363-5",
  abstract =
    "We integrate existential types into a strongly typed C-like
    language. In particular, we show how a bad combination of
    existential types, mutation, and aliasing can cause a subtle
    violation of type safety. We explort two independent ways to
    strengthen the type system to restore safety. One restricts the
    mutation of existential packages. The other restricts the types of
    aliases of extential packages. We use our framework to explain why
    other languages with existential types are safe.",
  paper = "Gros02.pdf"
}

\end{chunk}

\index{Jung, Ralf}
\index{Jourdan, Jacques-Henri}
\index{Krebbers, Robbert}
\index{Dreyer, Derek}
\begin{chunk}{axiom.bib}
@inproceedings{Jung18,
  author = "Jung, Ralf and Jourdan, Jacques-Henri and 
            Krebbers, Robbert and Dreyer, Derek",
  title = {{RustBelt: Securing the Foundations of the Rust Programming
            Language}}, 
  booktitle = "POPL '18",
  publisher = "ACM",
  year = "2018",
  abstract =
    "Rust is a new systems programming language that promises to
    overcome the seemingly fundamental tradeoff between high-level
    safety guarantees and low-level control over resource
    management. Unfortunately, none of Rust's safety claims have been
    formally proven, and there is good reason to question whether they
    actually hold. Specifically, Rust employs a string,
    ownership-based, type system, but then extends the expressive
    power of this core type system through libraries that internally
    use unsafe features. In this paper, we give the first formal (and
    machine-checked) safety proof for a language representing a
    realistic subset of Rust. Our proof is extensible in the sense
    that, for each new Rust library that uses unsafe features, we can
    say what verification condition it must satisfy in order for it to
    be deemed a safe extension to the language. We have carried out
    this verification for some of the most important libraries that
    are used throughout the Rust ecosystem.",
  paper = "Jung18.pdf"
}  

\end{chunk}

\index{Grossman, Dan}
\begin{chunk}{axiom.bib}
@article{Gros06,
  author = "Grossman, Dan",
  title = {{Quantified Types for Imperative Languages}},
  journal = "Trans. on Prog. Lang. and Systems",
  volume = "28",
  number = "3",
  year = "2006",
  pages = "429-475",
  abstract =
    "We describe universal types, existential types, and type
    constructors in Cyclone, a strongly-typed C-like language. We show
    how the language naturally supports first-class polymorphism and
    polymorphic recursion while requiring an acceptable amount of
    explicit type information. More importantly, we consider the
    soundness of type variables in the presence of C-style mutation
    and the address-of operator. For polymorphic references, we
    describe a solution more natural for the C level than the ML-style
    ``value restriction''. For existential types, we discover and
    subsequently avoid a subtle unsoundness issue resulting from the
    address-of operator. We develop a formal abstract machine and
    type-safety proof that captures the essence of type variables at
    the C level.",
  paper = "Gros06.pdf"
}

\end{chunk}

\index{Sarkar, Dipanwita}
\index{Waddell, Oscar}
\index{Dybvig, R. Kent}
\begin{chunk}{axiom.bib}
@inproceedings{Sark04,
  author = "Sarkar, Dipanwita and Waddell, Oscar and Dybvig, R. Kent",
  title = {{A Nanopass Infrastructure for Compiler Education}},
  booktitle = "9th ACM SIGPLAN",
  publisher = "ACM",
  pages = "201-212",
  year = "2004",  
  isbn = "1-58113-905-5",
  abstract =
    "A compiler structured as a small number of monolithic passes is 
     difficult to understand and difficult to maintain. The steep
     learning curve is daunting, and even experienced developers find
     that modifying existing passes is difficult and often introduces
     subtle and tenacious bugs. These problems are especially
     frustrating when the developer is a student in a compiler
     class. An attractive alternative is to structure a compiler as a
     collection of many fine-grained passes, each of which performs a
     single task. This structure aligns the implementation of a
     compiler with its logical organization, simplifying development,
     testing, and debugging. This paper describes the methodology and
     tools comprising a framework for constructing such compilers.",
  paper = "Sark04.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Sanchez-Stern, Alex}
\index{Alhessi, Yousef}
\index{Saul, Lawrence}
\index{Lerner, Sorin}
\begin{chunk}{axiom.bib}
@misc{Sanc19,
  author = "Sanchez-Stern, Alex and Alhessi, Yousef and Saul, Lawrence 
            and Lerner, Sorin",
  title = {{Generating Correctness Proofs with Neural Networks}},
  year = "2019",
  link = "\url{https://arxiv.org/pdf/1907.07794.pdf}",
  abstract =
    "Foundational verification allows programmers to build software
    which has been empirically shown to have high levels of assurance
    in a variety of important domains. However, the cost of producing
    foundationally verified software remains prohibitively high for
    most projects, as it requires significant manual effort by highly
    trained experts. In this paper we present Proverbot9001 a proof
    search system using machine learning techniques to produce proofs
    of software correctness in interactive theorem provers. We
    deomonstrate Proverbot9001 on the proof obligations from a large
    practical proof project, the CompCert verified C compiler, and
    show that it can effectively automate what was previously manual
    proofs, automatically solving 15.77\% of proofs in our test
    dataset. This corresponds to an over 3X improvement over the prior
    state of the art machine learning technique for generating proofs
    in Coq.",
  paper = "Sanc19.pdf"
}

\end{chunk}

\index{Christiansen, David Thrane}
\begin{chunk}{axiom.bib}
@misc{Chri12,
  author = "Christiansen, David Thrane",
  title = {{Converting Regular Expressions to Discrete Finite Automata}},
  year = "2012",
  link = "\url{http://davidchristiansen.dk/tutorials/regex-to-nfa.pdf}"
  paper = "Chri12.pdf"
}

\end{chunk}

\index{Christiansen, David Thrane}
\begin{chunk}{axiom.bib}
@misc{Chri13,
  author = "Christiansen, David Thrane",
  title = {{Didirectional Typing Rules: A Tutorial}},
  year = "2013",
  link = "\url{http://davidchristiansen.dk/tutorials/bidirectional.pdf}"
  paper = "Chri13.pdf"
}

\end{chunk}

\index{Christiansen, David Thrane}
\begin{chunk}{axiom.bib}
@misc{Chri14,
  author = "Christiansen, David Thrane",
  title = {{A Tutorial on Polymorphic Type Derivations}},
  year = "2014",
  link = "\url{http://davidchristiansen.dk/tutorials/type-rule-tutorial.pdf}"
  paper = "Chri14.pdf"
}

\end{chunk}

\index{Christiansen, David Thrane}
\begin{chunk}{axiom.bib}
@misc{Chri18,
  author = "Christiansen, David Thrane",
  title = {{A Little Taste of Dependent Types}},
  year = "2018",
  link = "\url{https://www.youtube.com/watch?v=VxINoKFm-S4}",
  abstract =
    "Dependent types let us use the same programming language for
    compile-time and run-time code, and are inching their way towards the
    mainstream from research languages like Coq, Agda and Idris. Dependent
    types are useful for programming, but they also unite programming and
    mathematical proofs, allowing us to use the tools and techniques we
    know from programming to do math.
    
    The essential beauty of dependent types can sometimes be hard to find
    under layers of powerful automatic tools. The Little Typer is an
    upcoming book on dependent types in the tradition of the The Little
    Schemer that features a tiny dependently typed language called Pie. We
    will demonstrate a proof in Pie that is also a program."
}

\end{chunk}

\index{Christiansen, David Thrane}
\begin{chunk}{axiom.bib}
@misc{Chri18a,
  author = "Christiansen, David Thrane",
  title = {{Coding for Types: The Universe Pattern in Idris}},
  year = "2018",
  link = "\url{https://www.youtube.com/watch?v=AWeT_G04a0A}"
}

\end{chunk}

\index{Christiansen, David Thrane}
\begin{chunk}{axiom.bib}
@misc{Chri19,
  author = "Christiansen, David Thrane",
  title = {{Bidirectional Type Checking}},
  year = "2019",
  link = "\url{http://www.youtube.com/watch?v=utyBNDj7s2w}"
}

\end{chunk}

\index{Coquand, Thierry}
\begin{chunk}{axiom.bib}
@misc{Coqu96a,
  author = "Coquand, Thierry",
  title = {{An Algorithm for Type-Checking Dependent Types}},
  year = "1996",
  abstract =
    "We present a simple type-checker for a language with dependent
    types and let expressions, with a simple proof of correctness.",
  paper = "Coqu96a.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Pierce, Benjamin C.}
\index{Turner, David N.}
\begin{chunk}{axiom.bib}
@misc{Pier98,
  author = "Pierce, Benjamin C. and Turner, David N.",
  title = {{Local Type Inference}},
  year = "1998",
  link =
  "\url{http://www.cis.upenn.edu/~bcpierce/papers/lti-toplas.pdf}", 
  abstract =
    "We study two partial type inference methods for a language
    combining subtyping and impredicative polymorphism. Both methods
    are local in the sense that missing annotations are recovered
    using only information from adjacent nodes in the syntax tree,
    without long-distance constraints such as unification
    variables. One method infers type arguments in polymorphic
    applications using a local constraint solver. The other infers
    annotations on bound variables in function abstractions by
    propagating type constraints downward from enclosing application
    nodes. We motivate our design choices by a statistical analysis of
    the uses of type inference in a sizable body of existing ML code.",
  paper = "Pier98.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Davies, Rowan}
\index{Pfenning, Frank}
\begin{chunk}{axiom.bib}
@misc{Davi00,
  author = "Davies, Rowan and Pfenning, Frank",
  title = {{Intersection Types and Computational Effects}},
  year = "2000",
  link = "\url{http://www.cs.cmu.edu/~fp/papers/icpf00.pdf}",
  abstract =
    "We show that standard formulations of intersection type systems
    are unsound in the presence of computational effects, and propose
    a solution similar to the value restriction for polymorphism
    adopted in the revised definition of Standard ML. It differs in
    that it is not tied to let-expressions and requires an additional
    weakening of the usual subtyping rules. We also present a
    bi-directional type-checking algorithm for the resulting language
    that does not require an excessive amount of type annotations and
    illustrate it through some examples. We further show that the type
    assignment system can be extended to incorporate parametric
    polymorphism. Taken together, we see our system and associated
    type-checking algorithm as a significant step towards the
    introduction of intersection types into realistic programming
    languages. The added expressive power would allow many more
    properties of programs to be stated by the programmer and
    statically verified by the compiler.",
  paper = "Davi00.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Odersky, Martin}
\index{Zenger, Christoph}
\index{Zenger, Matthias}
\begin{chunk}{axiom.bib}
@misc{Oder01,
  author = "Odersky, Martin and Zenger, Christoph and 
            Zenger, Matthias", 
  title = {{Colored Local Type Inference}},
  year = "2001",
  abstract =
    "We present a type system for a language based on $F_{\le}$, which
    allows certain type annotations to be elided in actual
    programs. Local type inference determines types by a combination
    of type propagation and local constraint solving, rather than by
    global constraint solving. We refine the previously existing local
    type inference system of Pierce and Turner by allowing partial
    type information to be propagated. This is expressed by coloring
    types to indicate propagation directions. Propagating partial type
    information allows us to omit type annotations for the visitor
    pattern, the analogue of pattern matching in languages without sum
    types.",
  paper = "Oder01.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Norell, Ulf}
\begin{chunk}{axiom.bib}
@phdthesis{Nore07,
  author = "Norell, Ulf",
  title = {{Towards a Practical Programming Language Based on
            Dependent Type Theory}},
  school = "Chalmers University",
  year = "2007",
  link = "\url{http://www.cse.chalmers.se/~ulfn/papers/thesis.pdf}",
  abstract =
    "Dependent type theories have a long history of being used for
    theorem proving. One aspect of type theory which makes it very
    powerful as a proof language is that it mixes deduction with
    computation. This also makes type theory a good candidate for
    programming -- the strength of the type system allows properties
    of programs to be stated and established, and the computational
    properties provide semantics for the programs.

    This thesis is concerned with bridging the gap between the
    theoretical presentations of type theory and the requirements of
    practical programming languages. Although there are many
    challenging research problems left to solve before we have an
    industrial scale programming language based on type theory, this
    thesis takes us a good step along the way.

    In functional programming languages pattern matching provides a
    concise notation for defining functions. In dependent type theory,
    pattern matching becomes even more powerful, in that inspecting
    the value of a particular term can reveal information about the
    types and values of other terms. In this thesis we give a type
    checking algorithm for definitions by pattern matching in type
    theory, supporting overlapping patterns, and pattern matching on
    intermediate results using the {\sl with} rule.

    Traditional presentations of type theory suffer from rather
    verbose notation, cluttering programs and proofs with, for
    instance, explicit type information. One solution to this problem
    is to allow terms that can be inferred automatically to be
    omitted. This is usually implemented by inserting metavariables in
    place of the omitted terms and using unification to solve these
    metavariables during type checking. We present a type checking
    algorithm for a theory with metavariables and prove its soundness
    independent of whether the metavariables are solved or not.

    In any programming language it is important to be able to
    structure large programs into separate units or modules and limit
    the interaction between these modules. In this thesis we present a
    simple, but powerful module system for a dependently typed
    language. The main focus of the module system is to manage the
    name space of a program, and an important characteristic is a
    clear separation between the module system and the type checker,
    making it largely independent of the underlying language.

    As a side track, not directly related to the use of type theory
    for programming, we present a connnection between type theory and
    a first-order logic theorem prover. This connection saves the user
    the burden of proving simple, but tedious first-order theorems by
    leaving them for the prover. We use a transparent translation to
    first-order logic which makes the proofs constructed by the
    theorem prover human readable. The soundness of the connection is
    established by a general metatheorem.

    Finally we put our work into practice in the implementation of a
    programming language, Agda, based on type theory. As an
    illustrating example, we show how to program a simple certified
    prover for equations in a commutative monoid, which can be used
    internally in Agda. Much more impressive examples have been done
    by others, showing that the ideas developed in this thesis are
    viable in practice.",
  paper = "Nore07.pdf"
}

\end{chunk}

\index{Dunfield, Joshua}
\index{Krishnaswami, Neelakantan R.}
\begin{chunk}{axiom.bib}
@misc{Dunf18,
  author = "Dunfield, Joshua and Krishnaswami, Neelakantan R.",
  title = {{Sound and Complete Bidirectional Typechecking for
            Higher-Rank Polymorphism with Existentials and 
            Indexed Types}},
  year = "2018",
  link = "\url{https://arxiv.org/pdf/1601.05106.pdf}",
  abstract =
    "Bidirectional typechecking, in which terms either synthesize a
    type or are checked against a known type, has become popular for
    its applicability to a variety of type systems, its error
    reporting, and its ease of implementation. Following principles
    from proof theory, bidirectional typing can be applied to many
    type constructs. The principles underlying a bidirectional
    approach to indexed types (\sl generalized algebraic datatypes}
    are less clear. Building on proof-theoretic treatments of
    equality, we give a declarative specification of typing based on
    {\sl focalization}. This approach permits declarative rules for
    coverage of pattern matching, as well as support for first-class
    existential types using a focalized subtyping judgment. We use
    refinement types to avoid explicitly passing equality proofs in
    our term syntax, making our calculus similar to languages such as
    Haskell and OCaml. We also extend the declarative specification
    with an explicit rules for deducing when a type is principal,
    permitting us to give a complete declarative specification for a
    rich type system with significant type inference. We also give a
    set of algorithmic typing rules, and prove that it is sound and
    complete with respect to the declarative system. The proof
    requires a number of technical innovations, including proving
    soundness and completeness in a mutually recursive fashion.",
  paper = "Dunf18.pdf"
}

\end{chunk}

\index{Pfenning, Frank}
\begin{chunk}{axiom.bib}
@misc{Pfen04a,
  author = "Pfenning, Frank",
  title = {{Lecture Notes on Bidirectional Type Checking}},
  year = "2004",
  paper = "Pfen04a.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Fieker, Claus}
\index{Hart, William}
\index{Hofmann, Tommy}
\index{Johansson, Fredrik}
\begin{chunk}{axiom.bib}
@inproceedings{Fiek17,
  author = "Fieker, Claus and Hart, William and Hofmann, Tommy and
            Johansson, Fredrik",
  title = {{Nemo/Hecke: Computer Algebra and Number Theory Package
            for the Julia Programming Language}},
  booktitle = "ISSAC'17",
  publisher = "ACM",
  year = "2017",
  pages = "157-164",
  abstract =
    "We introduce two new packages, Nemo and Hecke, written in the
    Julia programming language for computer algebra and number
    theory. We demonstrate that high performance generic algorithms
    can be implemented in Julia, without the need to resort to a
    low-level C implementation. For specialised algorithms, we use
    Julia's efficient native C interface to wrap existing C/C++
    libraries such as Flint, Arb, Antic and Singular. We give examples
    of how to use Hecke and Nemo and discuss some algorithms that we
    have implemented to provide high performance basic arithmetic.",
  paper = "Fiek17.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Yallop, Jeremy}
\index{White, Leo}
\begin{chunk}{axiom.bib}
@inproceedings{Yall19,
  author = "Yallop, Jeremy and White, Leo",
  title = {{Lambda: The Ultimate Sublanguage (Experience Report}},
  booktitle = "Inter. Conf. on Functional Programming",
  publisher = "ACM",
  year = "2019",
  abstract =
    "We describe our experience teaching an advanced typed functional
    programming course based around the use of Sysmte $F_\omega$ as a
    programming language.",
  paper = "Yall19.pdf"
}

\end{chunk}

\index{Zhao, Jinxu}
\index{Oliveira, Bruno C.D.S}
\index{Schrijvers, Tom}
\begin{chunk}{axiom.bib}
@inproceedings{Zhao19,
  author = "Zhao, Jinxu and Oliveira, Bruno C.D.S and 
            Schrijvers, Tom",
  title = {{A Mechanical Formalization of Higher-Ranked Polymorphic
            Type Interence}},
  booktitle = "Inter. Conf. on Functional Programming",
  publisher = "ACM"
  year = "2019",
  abstract =
    "Modern functional programming languages, such as Haskell or
    OCaml, use sophisticated forms of type inference. While an
    important topic in the Programming Languages research, there is
    little work on the mechanization of the metatheory of type
    inference in theorem provers. In particular we are unaware of any
    complete formalization of the type inference algorithms that are
    the backbone of modern functional languages.

    This paper presents the first full mechanical formalization of the
    metatheory for higher-ranked polymorphic type inference. The
    system that we formalize is the bidirectional type system by
    Dunfield and Krishnaswami (DK). The DK type system has two
    variants (a declarative and an algorithmic one) that have been
    manually proven sound, complete and decidable. We present a
    mechanical formalization in the Abella theorem provers of DK's
    declarative type system with a novel algorithmic system. We have a
    few reasons to use a new algorithm. Firstly, our new algorithm
    employs worklist judgments, which precisely capture the scope of
    variables and simplify the formalization of scoping in a theorem
    prover. Secondly, while DKs original formalizations comes with
    very well-written manual proofs, there are several details missing
    and some incorrect proofs, which complicate the task of writing a
    mechanized proof. Despite the use of a different algorithm we
    prove the same results as DK, although with significantly
    different proofs and proof techniques. Since such type inference
    algorithms are quite subtle and have a complex metatheory,
    mechanical formalizations are an important advance in type
    inference research.",
  paper = "Zhao19.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Swierstra, Wouter}
\index{Baanen, Tim}
\begin{chunk}{axiom.bib}
@inproceedings{Swie19,
  author = "Swierstra, Wouter and Baanen, Tim",
  title = {{A Predicate Transformer Semantics for Effects}},
  booktitle = "Inter. Conf. on Functional Programming",
  publisher = "ACM"
  year = "2019",
  abstract =
    "Reasoning about programs that use effects can be much harder than
    reasoning about their pure counterparts. This paper presents a
    predicate transformer semantics for a variety of effects,
    including exceptions, state, non-determinism, and general
    recursion. The predicate transformer semantics gives rise to a
    refinement relation that can be used to relate a program to its
    specification, or even calculate effectful programs that are
    correct by construction.",
  paper = "Swie19.pdf"
}

\end{chunk}

\index{Eremondi, Joseph}
\index{Tanter, Eric}
\index{Garcia, Ronald}
\begin{chunk}{axiom.bib}
@inproceedings{Erem19,
  author = "Eremondi, Joseph and Tanter, Eric and Garcia, Ronald",
  title = {{Approximate Normalization for Gradual Dependent Types",
  booktitle = "Inter. Conf. on Functional Programming",
  publisher = "ACM",
  year = "2019",
  abstract =
    "Dependent types help programmers write highly reliable
    code. However, this reliability comes at a cost: it can be
    challenging to write new prototypes in (or migrate old code to)
    dependently-typed programming languages. Gradual typing makes
    static type disciplines more flexible, so an appropriate notion of
    gradual dependent types could fruitfully lower this cost. However,
    dependent types raise unique challenges for gradual
    typing. Dependent typechecking involves the execution of program
    code, but gradually-typed code can signal runtime type errors or
    diverge. These runtime errors threaten the soundness guarantees
    that make dependen types so attractive, while divergence spoils
    the type-driven programming experience.

    this paper presents GDTL, a gradual dependently-typed language
    that emphasizes pragmatic dependently-typed programming. GDTL
    fully embeds both an untyped and dependently-typed language, and
    allows for smooth transitions between the two. In addition to
    gradual types we introduce gradual terms, which allow the user to
    be imprecise in type indices and to omit proof terms; runtime
    checks ensure type safety. To account for nontermination and
    failure, we distinguish between compile-type normalization and
    run-time execution: compile-time normalization is approximate but
    total, while runtime execution is exact, but may fail or
    diverge. We prove that GDTL has decidable typechecking and
    satisfies all the expected properties of gradual languages. In
    particular, GDTL satisfies the static and dynamic gradual
    guarantees: reducing type precision preserves typedness, and
    altering type precision does not change program behavior outside
    of dynamic type failures. To prove these properties, we were led
    to establish a novel normalization gradual guarantee thata
    captures the monotonicity of approximate normalization with
    respect to imprecision.",
  paper = "Erem19.pdf"
}

\end{chunk}

\index{Patterson, Daniel}
\index{Ahmed, Amal}
\begin{chunk}{axiom.bib}
@inproceedings{Patt19,
  author = "Patterson, Daniel and Ahmed, Amal",
  title = {{The Next 700 Compiler Correctness Theorems}},
  booktitle = "Inter. Conf. on Functional Programming",
  publisher = "ACM",
  year = "2019",
  abstract =
    "Compiler correctness is an old problem, with results stretching
    back beyond the last half-century. Founding the field, John
    McCarthy and James Painter set out to build ' completely
    trustworthy compiler'. And yet, until quite recently, even despite
    truly impressive verification efforts, the theorems being proved
    were only about the compilation of whole programs, a theoretically
    quite appealing but practically unrealistic simplification. For a
    compiler correctness theorem to assure complete trust, the theorem
    must reflect the reality of how the compiler will be used.

    There has been much recent work on more realistic 'compositional'
    compiler correctness aimed at proving correct compilation of
    components while supporting linking with components compiled from
    different languages using different compilers. However, the
    variety of theorems, stated in remarkably different ways, raises
    questions about what researchers even mean by a 'compiler is
    correct'. In this pearl, we develop a new framework with which to
    understand compiler correctness theorems in the presence of
    linking, and apply it to understanding and comparing this
    diversity of results. In doing so, not only are we better able to
    assess their relative strengths and weaknesses, but gain insight
    into what we as a community should expect from compiler
    correctness theorems of the future.",
  paper = "Patt19.pdf"
}

\end{chunk}

\index{Paraskevopoulou, Zoe}
\index{Appel, Andrew W.}
\begin{chunk}{axiom.bib}
@inproceedings{Para19,
  author = "Paraskevopoulou, Zoe and Appel, Andrew W.",
  title = {{Closure Conversion is Safe for Space}},
  booktitle = "Inter. Conf. on Functional Programming",
  publisher = "ACM",
  year = "2019",
  abstract =
    "We formally prove that closure conversion with flat environments
    for CPS lambda calculus is correct (preserves semantics) and safe
    for time and space, meaning that produced code preserves the time
    and space required for execution of the source program.

    We give a cost model to pre- and post-closure-conversion code by
    formalizing profiling semantics that keep track of the time and
    space resources needed for the execution of a program, taking
    garbage collection into account. To show preservation of time and
    space we set up a genera 'garbage-collection compatible' binary
    logical relation that establishes invariants on resource
    consumption of the related programs, along with functional
    correctness. Using this framework, we show semantics preservation
    and space and time safety for terminating source programs, and
    divergence preservation and space safety for diverging source
    programs. 

    This is the first formal proof of space-safety of a
    closure-conversion transformation. The transformation and the
    proof are parts of the CertiCoq compiler pipeline from Coq
    (Gallina) through CompCert Clight to assembly language. Our
    results are mechanized in the Coq proof assistant.",
  paper = "Para19.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Cong, Youyou}
\index{Osvald, Leo}
\index{Essertel, Gregory M.}
\index{Rompf, Tiark}
\begin{chunk}{axiom.bib}
@inproceedings{Cong19,
  author = "Cong, Youyou and Osvald, Leo and Essertel, Gregory M. 
            and Rompf, Tiark",
  title = {{Compiling with Continuations, or without? Whatever}},
  booktitle = "Inter. Conf. on Functional Programming",
  publisher = "ACM",
  year = "2019",
  abstract =
    "What makes a good compiler IR? In the context of functional
    languages, there has been an extensive debate on the advantages
    and disadvantages of continuation-passing style (CPS). The
    consensus seems to be that some form of explicit continuations is
    necessary to model jumps in a functional style, but that they
    should have a 2nd-class status, separate from regular functions,
    to ensure efficient code generation. Building on this observation,
    a recent study from PLDI 2017 proposed a direct-style IR with
    explicit join points, which essentially represent local
    continuations, i.e. functions that do not return or escape. While
    this IR can work well in practice, as evidenced by the
    implementation of join points in the Glasgow Haskell Compiler
    (GHC), there still seems to be room for improvement, especially
    with regard to the way continuations are handled in the course of
    optimization. 

    In this paper, we contribute to the CPS debate by developing a
    novel IR with the following features. First, we integrate a
    control operator that resembles Felleisen's C, eliminating certain
    redundant rewrites observed in the previous study. Second, we
    treat the non-returning and non-escaping aspects of continuations
    separately, allowing efficient compilation of well-behaved
    functions defined by the user. Third, we define a selective CPS
    translation of our IR, which erases control operators while
    preserving the meaning and typing of programs. These features
    enable optimizations in both direct style and full CPS, as well as
    in any intermediate style with selectively exposed continuations. 
    Thus, we change the spectrum of available options from 'CPS yes or
    no' to 'as much or as little CPS as you want, when you want it'.",
  paper = "Cong19.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Zavialov, Vladislav}
\begin{chunk}{axiom.bib}
@misc{Zavi18,
  author = "Zavialov, Vladislav",
  title = {{Why Dependent Haskell is the Future of Software Development}},
  link = "\url{https://serokell.io/blog/why-dependent-haskell}",
  year = "2018"
}

\end{chunk}

\index{Eisenberg, Richard A.}
\begin{chunk}{axiom.bib}
@phdthesis{Eise16,
  author = "Eisenberg, Richard A.",
  title = {{Dependent Types in Haskell: Theory and Practice}},
  school = "University of Pennsylvania",
  year = "2016",
  abstract =
    "Haskell, as implemented by the Glasgow Haskell Compiler (GHC),
    has been adding new type-level programming features for some
    time. Many of these features -- generalized algebraic datatypes
    (GADT)s, type families, kind polymorphism, and promoted datatypes
    -- have brought Haskell to the doorstep of dependent types. Many
    dependently typed programs can even currently be encoded, but
    often the constructions are painful.

    In this dissertation, I describe Dependent Haskell, which supports
    full dependent types via a backward-compatible extension to
    today's Haskell. An important contribution to this work is an
    implementation, in GHC, of a portion of Dependent Haskell, with
    the rest to follow. The features I have implemented are already
    released, in GHC 8.0. This dissertation contains several practical
    examples of Dependent Haskell code, a full description of the
    differences between Dependent Haskell and today's Haskell, a novel
    dependently typed lambda-calculus (called PICO) suitable for use
    as an intermediate language for compiling Dependent Haskell, and a
    type inference and elaboration algorithm, BAKE, that translates
    Dependent Haskell to type-correct PICO. Full proofs of type safety
    of PICO and the soundness of BAKE are included in the appendix.",
  paper = "Eise16.pdf"
}

\end{chunk}

\index{Pressler, Ron}
\begin{chunk}{axiom.bib}
@misc{Pres19,
  author = "Pressler, Ron",
  title = {{Correctness and Complexity}},
  year = "2019",
  link = "\url{https://pron.github.io/posts/correctness-and-complexity}"
}

\end{chunk}

\index{Rado, Tibor}
\begin{chunk}{axiom.bib}
@article{Rado61,
  author = "Rado, Tibor",
  title = {{On Non-Computable Functions}},
  journal = "Bell System Technical Journal",
  volume = "41",
  number = "3"
  year = "1961",
  abstract =
    "The construction of non-computable functions used in this paper
    is based on the priciple that a finite, non-empty set of
    non-negative integers has a largest element. Also, this principle
    is used only for sets which are exceptionally well-defined by
    current standards. No enumeration of computable functions is used,
    and in this sense the diagonal process is not employed. This, it
    appears that an apparently self-evident principle, of constant use
    in every area of mathematics, yields non-constructive entities.",
  paper = "Rado61.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Yedidia, Adam}
\index{Aaronson, Scott}
\begin{chunk}{axiom.bib}
@article{Yedi16,
  author = "Yedidia, Adam and Aaronson, Scott",
  title = {{A Relatively Small Turing Machine Whose Behavior Is
            Independent of Set Theory}},
  journal = "Complex Systems",
  volume = "25",
  number = "4",
  pages = "297-327",
  link = "\url{http://www.complex-systems.com/pdf/25-5-5.pdf}",
  year = "2016",
  abstract =
    "Since the definition of the Busy Beaver function in Rado in 1962,
    an interesting open question has been what the smallest value of
    $n$ for which $BB(n)$ is independent of ZFC set theory. Is this
    $n$ approximately 10, or closer to 1,000,000, or is it even
    larger? In this paper, we show that it is at most 7,918 by
    presenting an explicit description of a 7,918-state Turing machine
    $Z$ with 1 tape and a 2-symbol alphabet that cannot be proved to
    run forever in ZFC (even though it presumably does), assuming ZFC
    is consistent. The machine is based on work of Harvey Friedman on
    independent statements involving order-invariant graphs. In doing
    so, we give the first known upper bound on the highest provable
    Busy Beaver number in ZFC. We also present a 4,888-state Turing
    machine $G$ that halts if and only if there is a counterexample of
    Goldbach's conjecure, an at 5,372-state Turing machine $R$ that
    halts if and only if the Riemann hypothesis is false. To create
    $G$, $R$, and $Z$, we develop and use a higher-level language,
    Laconic, which is much more convenient than direct state
    manipulation.",
  paper = "Yedi16.pdf"
}

\end{chunk}

\index{Hartmanis, J.}
\index{Stearns, R.E.}
\begin{chunk}{axiom.bib}
@article{Hart63,
  author = "Hartmanis, J. and Stearns, R.E.",
  title = {{On the Computational Complexity of Algorithms}},
  journal = "Trans. American Mathematical Society",
  volume = "117",
  pages = "285-306",
  year = "1963",
  paper = "Hart63.pdf"
}

\end{chunk}

\index{Brooks, Frederick P.}
\begin{chunk}{axiom.bib}
@misc{Broo86a,
  author = "Brooks, Frederick P.",
  title = {{No Silver Bullet -- Essence and Accident in Software
            Engineering}}, 
  booktitle = "The Mythical Man-Month, Anniversary Edition",
  publisher = "Elsevier Science",
  pages = "1069-1076",
  year = "1986",
  comment = "chapter",
  abstract =
    "There is no single development, in either technology or
    management technique, which by itself promises even one
    order-of-magnitude improvement within a decade in productivity, in
    reliability, in simpliity",
  paper = "Broo86a.pdf",
  keywords = "printed, DONE"
}

\end{chunk}

\index{Turing, A. M.}
\begin{chunk}{axiom.bib}
@misc{Turi36,
  author = "Turing, A. M.",
  title = {{On Computable Numbers, with an Application to the
            Entscheidungsproblem}},
  year = "1936",
  link =
  "\url{https://www.cs.virginia.edu/~robins/Turing_Paper_1936.pdf}",
  abstract =
    "The ``computeble'' numbers may be described briefly as the real
    numbers whose expressions as a decimal are calculable by finite
    means. Although the subjec of this paper is ostensibly the
    computable {\sl numbers}, it is almost equally easy to define and
    investigate computable functions of an integral variable or a real
    or computable variable, computable predicates, and so forth. The
    fundamental problems involed are, however, the same in each case,
    and I have chosen the computable numbers for explicit treatment as
    involving the least cumbrous technique. I hope shortly to give an
    account of the relations of the computable numbers, functions, and
    so forth to one another. This will include a development of the
    theory of functions of a real variable expressed in terms of
    computable numbers. According to my definition, a number is
    computable if its decimal can be written down by a machine.",
  paper = "Turi36.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Turing, A. M.}
\begin{chunk}{axiom.bib}
@misc{Turi47,
  author = "Turing, A. M.",
  title = {{Lecture to the London Mathematical Society on 20 February
           1947}}, 
  year = "1947",
  link = "\url{http://www.vordenker.de/downloads/turing-vorlesung.pdf}",
  paper = "Turi47.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Demri, S.}
\index{Laroussinie, F.}
\index{Schnoebelen, Ph.}
\begin{chunk}{axiom.bib}
@article{Demr06,
  author = "Demri, S. and Laroussinie, F. and Schnoebelen, Ph.",
  title = {{A Parametric Analysis of the State-Explosion Problem in
            Model Checking}},
  journal = "Computer and System Sciences",
  volume = "72",
  pages = "547-575",
  year = "2006",
  abstract = 
    "In model checking, the state-explosion problem occurs when one
    checks a {\sl nonflat system}, i.e., a system implicitly described
    as a synchronized product of elementary subsystems. In this paper,
    we investigate the complexity of a wide variedty of model-checking
    problems for nonflat systems under the light of
    {\sl parameterized complexity}, taking the number of synchronized
    components as a parameter. We provide precise complexity measures
    (in the parameterized sense) for most of the problems we
    investigate, and evidence that the results are robust.",
  paper = "Demr06.pdf"
}

\end{chunk}

\index{Schnoebelen, Ph.}
\begin{chunk}{axiom.bib}
@article{Schn02,
  author = "Schnoebelen, Ph.",
  title = {{The Complexity of Temporal Logic Model Checking}},
  journal = "Advances in Modal Logic",
  volume = "4",
  pages = "1-44",
  year = "2002",
  paper = "Schn02.pdf"
}

\end{chunk}

\index{Noonan, Matt}
\begin{chunk}{axiom.bib}
@inproceedings{Noon18,
  author = "Noonan, Matt",
  title = {{Ghosts of Departed Proofs (Functional Pearl)}},
  booktitle = "Haskell '18",
  publisher = "ACM",
  isbn = "978-1-4503-5835-4",
  abstract =
    "Library authors often are faced with a design chice: should a
    function with preconditions be implemented as a partial function,
    or by returning a failure condition on incorrect use? Neither
    option is ideal. Partial functions lead to frustrating run-time
    errors. Failure conditions must be checked at the use-site,
    placing an unfair tax on the users who have ensured that the
    function's preconditions were correctly met.

    In this paper, we introduce an API design concept called `ghosts
    of departed proofs' based on the following observation:
    sophisticated preconditions can be encoded in Haskell's type
    system with no run-time overhead, by using proofs that inhabit
    phantom type parameters attached to new type wrappers. The user
    expresses correctness arguments by constructing proofs to inhabit
    these phantom types. Critically, this technique allows the library 
    {\sl user} to decide when and how to validate that the API's
    preconditions are met.

    The 'ghost of departed proofs' approach to API design can achieve
    many of the benefits of dependent types and refinement types, yet
    only requires some minor and well-understood extensions to Haskell
    2010. We demonstrate the utility of this approach through a series
    of case studies, showing how to enforce novel invariants for
    lists, maps, graphs, shared memory regions, and more.",
  paper = "Noon18.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Innes, Sean}
\index{Uu, Nicolas}
\begin{chunk}{axiom.bib}
@inproceedings{Inne19,
  author = "Innes, Sean and Uu, Nicolas",
  title = {{Tic Tak Types}},
  booktitle - "Int. Workshop on Type Driven Development",
  publisher = "ACM",
  year = "2019",
  abstract =
    "Tic-Tac-Toe is a simple, familiar, classic game enjoyed by
    many. This pearl is designed to give a flavour of the world of
    dependent types to the uninitiated functional programmer. We cover
    a journey from Tic-Tak-Terrible implementations in the harsh world
    of virtually untyped {\sl Strings}, through the safe haven of
    vectors that know their own length, and into a Tic-Tac-Titanium
    version that is too strongly typed for its own good. Along the way
    we discover something we knew all along: types are great, but in
    moderation. This lesson is quickly put to use in a more complex
    recursive version.",
  paper = "Inne19.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Naur, Peter}
\begin{chunk}{axiom.bib}
@misc{Naur85,
  author = "Naur, Peter",
  title = {{Programming as Theory Building}},
  link = "\url{http://pages.cs.wisc.edu/~remzi/Naur.pdf}",
  paper = "Naur85.pdf",
  keywords = "DONE"
}

\end{chunk}

\index{Elliott, Conal}
\index{Pfenning, Frank}
\begin{chunk}{axiom.bib}
@inproceedings{Elli90,
  author = "Elliott, Conal and Pfenning, Frank",
  title = {{A Semi-Functional Implementation of a Higher-Order Logic
            Programming Language}},
  year = "1990",
  link = "\url{http://www.cs.cmu.edu/~fp/papers/elpsml90.pdf}",
  comment = "\url{http://www.cs.cmu.edu/~fp/papers/elpsml-paper.tar.gz}",
  paper = "Elli90.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Carlsson, Mats}
\begin{chunk}{axiom.bib}
@article{Carl84,
  author = "Carlsson, Mats",
  title = {{On Implementing Prolog in Functional Programming}},
  journal = "New Generation Computing",
  volume = "2",
  pages = "347-359",
  year = "1984",
  abstract =
    "This report surveys techniques for implementing the programming
    language Prolog. It focuses on explaining the procedural semantics
    of the language in terms of functional programming constructs. The
    techniques {\sl success continuations} and {\sl proof streams} are
    introduced, and it is shown how Horn clause interpreters can be
    built upon them. Continuations are well known from denotational
    semantics theory, in this paper it is shown that they are viable
    constructs in actual programs.",
  paper = "Carl84.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Baker, Henry}
\begin{chunk}{axiom.bib}
@misc{Bake84,
  author = "Baker, Henry",
  title = {{The Nimble Type Inferencer for Common Lisp-84}},
  year = "1984",
  link = "\url{http://home.pipeline.com/~hbaker1/TInference.html}",
  abstract =
    "We describe a framework and an algorithm for doing type inference
    analysis on programs written in full Common Lisp-84 (Common Lisp
    without the CLOS object-oriented extensions). The objective of
    type inference is to determine tight lattice upper bounds on the
    range of runtime data types for Common Lisp program variables and
    temporaries. Depending upon the lattice used, type inference can
    also provide range analysis information for numeric
    variables. This lattice upper bound information can be used by an
    optimizing compiler to choose more restrictive, and hence more
    efficient, representations for these program variables. Our
    analysis also produces tighter control flow information, which can
    be used to eliminate redundant tests which result in dead
    code. The overall goal of type inference is to mechanically
    extract from Common Lisp programs the same degree of
    representation information that is usually provided by the
    programmer in traditiional strongly-typed languages. In this way,
    we can provide some classes of Common Lisp programs execution time
    efficiency expected only for more strongly-typed compiled languages.",
  paper = "Bake84.pdf",
  keywords = "printed, DONE"
}

\end{chunk}

\index{Dunfield, Joshua}
\index{Krishnaswami, Neel}
\begin{chunk}{axiom.bib}
@misc{Dunf19,
  author = "Dunfield, Joshua and Krishnaswami, Neel",
  title = {{Bidirection Typing}},
  year = "2019", 
  link = "\url{https://www.cl.cam.ac.uk/~nk480/bidir-survey.pdf}",
  abstract =
    "Bidirectional typing combines two modes of typing: type checking,
    which checks that a program satisfies a known type, and type
    synthesis, which determines a type from the program. Using
    checking enables bidirectional typing to break the decidability
    barrier of Damas-Milner approaches; using synthesis enables
    bidirectional typing to avoid the large annotation burden of
    explicitly typed languages. In addition, bidirectional typing
    improves error locality. We highlight the design principles that
    underlie bidirectional type systems, survey the development of
    bidirectional typing from the prehistoric period before Pierce and
    Turner's local type inference to the present day, and provide
    guidance for future investigations.",
  paper = "Dunf19.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Sannella, Donald}
\index{Tarlecki, Andrzej}
\begin{chunk}{axiom.bib}
@book{Sann12,
  author = "Sannella, Donald and Tarlecki, Andrzej",
  title = {{Foundations of Algebraic Specification and Formal Software
            Development}}, 
  publisher = "Springer",
  year = "2012",
  isbn = "978-3-642-17336-3",
  paper = "Sann12.pdf"
}

\end{chunk}

\index{Sannella, D.}
\index{Tarlecki, A.}
\begin{chunk}{axiom.bib}
@inproceedings{Sann91,
  author = "Sannella, D. and Tarlecki, A.",
  title = {{Formal Program Development in Extended ML for the Working
            Programmer}}, 
  booktitle = "3rd BCS/FACS Workshop on Refinement",
  publisher = Springer",
  pages = "99-130",
  year = "1991",
  abstract =
    "Extened ML is a framework for the formal development of programs
    in the Standard ML programming language from high-level
    specifications of their required input/output behavior. It
    strongly supports the development of modular programs consisting
    of an interconnected collection of generic and reusable units. The
    Extended ML framework includes a methodology for formal program
    development which establishes a number of ways of proceeding from
    a given specification of a programming task towards a
    program. Each such step gives rise to one or more proof
    oblisgations which must be proved in order to establish the
    correctness of that step. This paper is inteded as a user-oriented
    summary of the Extended ML language and mthodology. Theoretical
    technicalities are avoided whenever possible, with emphasis placed
    on the practical aspects of formal program development. An
    extended example of a complete program development in Extended ML
    is included.",
  paper = "Sann91.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Sannella, D.}
\index{Tarlecki, A.}
\begin{chunk}{axiom.bib}
@article{Sann99,
  author = "Sannella, Donald and Tarlecki, Andrzej",
  title = {{Algebraic Methods for Specification and Formal Development
            of Programs}},
  journal = "ACM Computing Surveys",
  volume = "31",
  year = "1999",
  paper = "Sann99.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Kahrs, Stefan}
\begin{chunk}{axiom.bib}
@techreport{Kahr95,
  author = "Kahrs, Stefan",
  title = {{On the Static Analysis of Extended ML}},
  type = "technical report",
  institution = "Lab for Foudations of Comp Sci. Univ. Edinburgh",
  number = "Research Note",
  abstract =
    "This is a short note describing differences in static analysis of
    EML, as defined in [KST94] and SML, as defined in [MTH90] and
    [MT91]. It is intended for use by people who are building an EML
    parser/type-checker by modifying an existing SML compiler.",
  paper = "Kahr95.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Kahrs, S.}
\index{Sannella, D.}
\index{Tarlecki, A.}
\begin{chunk}{axiom.bib}
@article{Kahr94,
  author = "Kahrs, S. and Sannella, D. and Tarlecki, A.",
  title = {{Interfaces and Extended ML}},
  journal = "SIGPLAN Notices",
  volume = "29",
  number = "8",
  pages = "111-118",
  year = "1994",
  abstract =
    "This is a position paper giving our views on the uses and makeup
    of module interfaces. The position espoused is inspired by our
    work on the Extended ML (EML) formal software development
    framework and by ideas in the algebraic foundations of
    specification and formal development. The present state of
    interfaces in EML is outlined and set in the context of plans for
    a more general EML-like framework with axioms in interfaces taken
    from an arbitrary logical system formulated as an 
    {\sl institution}. Some more speculative plans are sketched
    concerning the simultaneous use of multiple institutions in
    specification and development.",
  paper = "Kahr94.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Sannella, D.}
\index{Tarlecki, A.}
\begin{chunk}{axiom.bib}
@article{Sann91a,
  author = "Sannella, Donald and Tarlecki, Andrzej",
  title = {{Extended ML: Past, Present and Future}},
  journal = "LNCS",
  volume = "534",
  pages = "297-322",
  year = "1991",
  abstract =
    "An overview of past, present and future work on the Extended ML
    formal program development framework is given, with emphasis on
    two topics of current active research: the semantics of the
    Extened ML specification langauge, and tools to support formal
    program development.",
  paper = "Sann91a.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Sannella, Donald}
\begin{chunk}{axiom.bib}
@misc{Sann86a,
  author = "Sannella, Donald",
  title = {{Formal Specification of ML Programs}},
  link =
  "\url{http://www.lfcs.inf.ed.ac.uk/reports/86/ECS-LFCS-86-15/ECS-LFCS-86.15.ps}",  
  year = "1986",
  abstract =
    "These notes were written to accompany lectures on program
    specification which formed part of a course on functional
    programming in ML. Functions can be specified using a
    specification language obtained by extending ML with
    (non-executable) first-order axioms. Simple inductive proofs
    suffice to show that in ML function satisfies such a
    specification. This approach can also be used to specify and
    verify larger programs built from smaller pieces using ML's
    modularisation facilities. Examples are used to illustrate the
    methods discussed.",
  paper = "Sann86a.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Sannella, Donald}
\index{Tarlecki, Andrzej}
\begin{chunk}{axiom.bib}
@article{Sann97,
  author = "Sannella, Donald and Tarlecki, Andrzej",
  title = {{Essential Concepts of Algebraic Specification and Program
            Development}}, 
  journal = "Formal Aspects of Computing",
  volume = "9",
  pages = "229-269",
  year = "1997",
  abstract = 
    "The main ideas underlying work on the model-theoretic foundations
    of algebraic specification and formal program development are
    presented in an informal way. An attempt is made to offer an
    overall view, rather than new results, and to focus on the basic
    motivation behind the technicalities presented elsewhere.",
  paper = "Sann97.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Wright, Andrew K.}
\begin{chunk}{axiom.bib}
@inproceedings{Wrig95,
  author = "Wright, Andrew K.",
  title = {{Simple Imperative Polymorphism}},
  booktitle = "LISP and Symbolic Computation",
  publisher = "Kluwer Academic",
  pages = "242-256",
  year = "1995",
  abstract =
    "This paper describes a simple extension of the Hindley-Milner
    polymorphic type discipline to call-by-value languages that
    incorporate imperative features like references, exceptions, and
    continuations. This extension sacrifices the ability to type every
    purely functional expression that is typable in the Hindley-Milner
    system. In return, it assigns the same type to functional and
    imperative implementations of the same abstraction. Hence with a
    module system that separates specifications from implementations,
    imperative features can be freely used to implement polymorphic
    specifications. A study of a number of ML programs shows that the
    inability to type all Hindley-Milner typable expressions seldom
    impacts realistic programs. Furthermore, most programs that are
    rendered untypable by the new system can be easily repaired.",
  paper = "Wrig95.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Tofte, Mads}
\begin{chunk}{axiom.bib}
@phdthesis{Toft88,
  author = "Tofte, Mads",
  title = {{Operational Semantics and Polymorphic Type Inference}},
  school = "Univ. of Edinburgh",
  year = "1988",
  abstract =
    "Three languages with polymorphic type disciplines are discussed,
    namely the $\lambda$-calculus with Milner's polymorphic type
    discipline; a language with imperative features (polymorphic
    references); and a skeletal module language with structures,
    signatures and functors. In each of the two first cases we show
    that the type inference system is consistent with an operational
    dynamic semantics.

    On the module level, polymorphic types correspond to
    signatures. There is a notion of principal signatures. So-called
    signature checking is the module level equivalent of type
    checking. In particular, there exists an algorithm which either
    fails or produces a principal signature.",
  paper = "Toft88.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Reynolds, John C.}
\begin{chunk}{axiom.bib}
@inproceedings{Reyn83,
  author = "Reynolds, John C.",
  title = {{Types, Abstraction and Parametric Polymorphism}},
  booktitle = "Information Processing 83",
  publisher = "Elsevier Science Publishers",
  year = "1983",
  abstract =
    "We explore the thesis that type structure is a syntactic
    discipline for maintaining levels of abstraction. Traditionally,
    this view has beeen formalized algebraically, but the algebraic
    approach fails to encompass higher-order functions. For this
    purpose, it is necessary to generalize homomorphic functions to
    relations; the result is an ``abstraction'' theorem that is
    applicable to the typed lambda calculus and various extensions,
    including user-defined types.

    Finally, we consider polymorphic functions, and show that the
    abstraction theorem captures Strachey's concept of parametric, as
    opposed to ad hoc, polymorphism.",
  paper = "Reyn83.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Leroy, Xavier}
\begin{chunk}{axiom.bib}
@techreport{Lero92,
  author = "Leroy, Xavier",
  title = {{Polymorphic Typing of an Algorithmic Language}},
  type = "research report",
  institution = "INRIA",
  number = "N1778",
  year = "1992",
  abstract =
    "The polymorphic type discipline, as in the ML language, fits well
    within purely applicative languages, but does not extend naturally
    to the main feature of algorithmic languages: in-place update of
    data structures. Similar typing difficulties arise with other
    extensions of applicative languages: logical variables,
    communication channels, continuation handling. This work studies
    (in the setting of relational semantics) two new approaches to the
    polymorphic typing of these non-applicative features. The first
    one relies on a restriction of generalization over types (the
    notion of dangerous variables), and on a refined typing of
    functional values (closure typing). The resulting type system is
    compatible with the ML core language, and is the most expressive
    type systems for ML with imperative features so far. The second
    approach relies on switching to ``by-name'' sematics for the
    constructs of polymorphism, instead of the usual ``by-value''
    semanticcs. The resulting language differs from ML, but lends
    itself easily to polymorphic typing. Both approaches smoothly
    integrate non-applicative features and polymorphic typing.",
  paper = "Lero92.pdf"
}

\end{chunk}

\index{Hutton, Graham}
\begin{chunk}{axiom.bib}
@article{Hutt99,
  author = "Hutton, Graham",
  title = {{A Tutorial on the Universality and Expressiveness of
           Fold}}, 
  journal = "J. Functional Programming",
  volume = "9",
  number = "4",
  pages = "355-372",
  year = "1999",
  abstract =
    "In functional programming, {\sl fold} is a standard operator that
    encapsulates a simple pattern of recursion for processing
    lists. This article is a tutorial on two key aspects of the fold
    operator for lists. First of all, we emphasize the use of the
    universal property of fold both as a proof principle that avoids
    the need for inductive proofs, and as a definition principle that
    guides the transformation of recursive functions into definitions
    using fold. Secondly, we show that even though the pattern of
    recursion encapsulated by fold is simple, in a language with
    tuples and functions as first-class values the fold operator has
    greater expressive power than might first be expected.",
  paper = "Hutt99.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Cardelli, Luca}
\begin{chunk}{axiom.bib}
@article{Card88b,
  author = "Cardelli, Luca",
  title = {{Basic Polymorphic Typechecking}},
  journal = "Science of Computer Programming",
  volume = "8",
  number = "2",
  year = "1988",
  paper = "Card88b.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Hughes, John}
\begin{chunk}{axiom.bib}
@misc{Hugh19,
  author = "Hughes, John",
  title = {{How to Specify it!}},
  year = "2019",
  link = "\url{https://www.dropbox.com/s/tx2b84kae4bw1p4/paper.pdf}",
  abstract =
    "Property-based testing tools test software against a
    specification, rather than a set of examples. This tutorial paper
    presents five generic approaches to writing such specifications
    (for purely functional code). We discuss costs, benefits, and
    bug-finding power of each approach, with reference to a simple
    example with eight buggy variants. The lessons learned should help
    the reader to develope effective property-based tests in the future.",
  paper = "Hugh19.pdf",
  keywords = "printed, DONE"
}

\end{chunk}

\index{Goto, Kazushige}
\index{van de Geijn, Robert A.}
\begin{chunk}{axiom.bib}
@article{Goto19,
  author = "Goto, Kazushige and van de Geijn, Robert A.",
  title = {{Anatomy of High-Performance Matrix Multiplication}},
  journal = "Transactions on Mathematical Software",
  volume = "V",
  number = "N",
  year = "2019",
  abstract =
    "We present the basic principles which underlie the high
    performance implementation of the matrix multiplication that is
    part of the widely used GotoBLAS library. Design decisions are
    justified by successively refining a model of architectures with
    multilevel memories. A simple but effective algorithm for
    executing this operation results. Implementations on a broad
    selection of architectures are shown to achieve near-peak
    performacne.", 
  paper = "Goto19.pdf"
}

\end{chunk}

\index{Wernhard, Christoph}
\begin{chunk}{axiom.bib}
@misc{Wern19,
  author = "Wernhard, Christoph",
  title = {{PIE -- Proving, Interpolating and Eliminating on the Basis
           of First-Order Logic}},
  year = "2019",
  link = "\url{https://arxiv.org/pdf/1908.11137.pdf}",
  abstract =
    "PIE is a Prolog-embedded environment for automated reasoning on
    the basis of first-order logic. It includes a versatile formula
    macro system and supports the creation of documents that
    intersperse macro definitions, reasoner invocations and LaTeX
    formatted natural language text. Invocation of various reasoners
    is supported. External provers as well as sub-systems of PIE,
    which include preprocessors, a Prolog-based first-order prover,
    methods for Craig interpolation and methods for second-order
    quantifier elimination.",
  paper = "Wern19.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Aldrich, Jonathan}
\begin{chunk}{axiom.bib}
@inproceedings{Aldr13,
  author = "Aldrich, Jonathan",
  title = {{The Power of Interoperability: Why Objects are Inevitable}},
  booktitle = "Onward!",
  publisher = "ACM",
  year = "2013",
  link = "\url{https://www.cs.cmu.edu/~aldrich/papers/objects-essay.pdf}",
  abstract =
    "Three years ago, in this venue, Cook argued that in their
    essence, objects are what Reynolds called {\sl procedural data
    structures}. His observations raises a natural question: if
    procedural data structures are the essence of objects, has this
    contributed to the empirical success of objects, and if so, how?

    This essay attempts to answer that question. After reviewing
    Cook's definition, I propose the term {\sl service abstractions}
    to capture the essential nature of objects. This terminology
    emphasizes, following Kay, that objects are not primarily about
    representing and manipulating data, but are more about providing
    services in support of higher-level goals. Using examples taken
    from object oriented frameworks, I illustrate the unique design
    leverage that service abstractions provide: the ability to define
    abstractions that can be extended, and whose extensions are
    interoperable in a first-class way. The essay argues that the form
    of interoperable extension supported by service abstractions is
    essential to modern software: many modern frameworks and
    ecosystems could not have been built without service
    abstractions. In this sense, the success of objects was not a
    coincidence: it was an inevitable consequence of their service
    abstraction nature.",
  paper = "Aldr13.pdf",
  keywords = "printed, DONE"
}

\end{chunk}

\index{Chang, Stephen}
\index{Knauth, Alex}
\index{Greenman, Ben}
\begin{chunk}{axiom.bib}
@inproceedings{Chan17,
  author = "Chang, Stephen and Knauth, Alex and Greenman, Ben",
  title = {{Type Systems as Macros}},
  booktitle = "Principles of Programming Languages",
  publisher = "ACM",
  year = "2017",
  abstract =
    "We present TURNSTILE, a metalanguage for creating typed embedded
    languages. To implement the type system, programmers write type
    checking rules resembling traditional judgment syntax. To
    implement the semantics, they incorporate elaborations into these
    rules. TURNSTILE critically depends on the idea of linguistic
    reuse. It exploits a macro system in a novel way to simultaneously
    type check and rewrite a surface program into a target
    language. Reusing a macro system also yields modular
    implementations whose rules may be mixed and matched to create
    other languages. Combined with typical compiler and runtime reuse,
    TURNSTILE produces performant typed embedded languages with little
    effort.",
  paper = "Chan17.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Xi, Hongwei}
\begin{chunk}{axiom.bib}
@book{Xixx19,
  author = "Xi, Hongwei",
  title = {{Introduction to Programming in ATS}},
  publisher = "ATS Trustful Software, Inc",
  year = "2019",
  abstract =
    "As a programming language, ATS is both syntax-rich and
    feature-rich. This book introduces the reader to some core
    features of ATS, including basic functional programming, simple
    types, (recursively defined) datatypes, polymorphic types,
    dependent types, linear types, theorem proving, programming with
    theorem proving (PwTP), and template-based programming. Although
    the reader is not assumed to be familiar with programming in
    genera, the book is likely to be rather dense for someone without
    considerable programming experience",
  paper = "X1xx19.pdf"
}

\end{chunk}

\index{Loh, Andres}
\index{McBride, Conor}
\index{Swierstra, Wouter}
\begin{chunk}{axiom.bib}
@article{Lohx01,
  author = "Loh, Andres and McBride, Conor and Swierstra, Wouter",
  title = {{A Tutorial Implementation of a Dependently Typed Lambda
            Calculus}}, 
  journal = "Foundations Informaticae",
  volume = "XXI",
  pages = "1001-1031",
  year = "2001",
  abstract = 
    "We present the type rules for a dependently typed core calculus
    together with a straight-forward implementation in Haskell. We
    explicitly highlight the changes necessary to shift from a
    simply-typed lambda calculus to a dependently typed lambda
    calculus. We also describe how to extend our core language with
    data types and write several small example programs. The article
    is accompanied by an executable interpreter and example code that
    allows immediate experimentation with the system we describe.",
  paper = "Lohx01.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Bahr, Patrick}
\index{Hutton, Graham}
\begin{chunk}{axiom.bib}
@article{Bahr15,
  author = "Bahr, Patrick and Hutton, Graham",
  title = {{Calculating Correct Compilers}},
  journal = "Functional Programming",
  year = "2015",
  link = "\url{www.cs.nott.ac.uk/~pszgmh/ccc.pdf}",
  abstract =
    "In this article we present a new approach to the problem of
    calculating compilers. In particular, we develop a simple but
    general technique that allos us to derive correct compilers from
    high-level semantics by systematic calculation, with all details
    of the implementation of the compilers falling naturally out of
    the calculation process. Our approach is based upon the use of
    standard equational reasoning techniques, and has been applied to
    calculate compilers for a wide range of language features and
    their combination, including arithmetic expressions, exceptions,
    state, various forms of lambda calculi, bounded and unbounded
    loops, non-determinism, and interrupts. All the calculations in
    the article have been formalised using the Coq proof assistant,
    which serves as a convenient interactive tool for developing and
    verifying the calculations."
  paper = "Bahr15.pdf",
  keywords = "printed"
}  

\end{chunk}

\index{Bahr, Patrick}
\index{Hutton, Graham}
\begin{chunk}{axiom.bib}
@article{Bahr19,
  author = "Bahr, Patrick and Hutton, Graham",
  title = {{Calculating Correct Compilers II}},
  journal = "Functional Programming",
  year = "2019",
  link = "\url{www.cs.nott.ac.uk/~pszgmh/ccc2.pdf}",
  abstract =
    "In 'Calculating Correct Compilers' (Bahr and Hutton, 2015) we
    developed a new approach to calculating compilers directly from
    specifications of their correctness. Our approach only required
    elementary reasoning techniques, and has been used to calculate
    compilers for a wide range of language features and their
    combination. However, the methodology was focused on stack-based
    target machines, whereas real compilers often target
    register-based machines. In this article, we show how our approach
    can naturally be adapted to calculate compilers for register
    machines.",
  paper = "Bahr19.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Swords, Sol}
\index{Davis, Jared}
\begin{chunk}{axiom.bib}
@article{Swor11,
  author = "Swords, Sol and Davis, Jared",
  title = {{Bit_Blasiting ACL2 Theorems}},
  journal = "EPTCS",
  volume = "70",
  pages = "84-102",
  year = "2011",
  abstract =
    "Interactive theorem proving requires a lot of human
    guidance. Proving a property involes (1) figuring out why it
    holds, the (2) coaxing the theorem prover into believing it. Both
    steps can take a long time. We explain how to use GL, a framework
    for proving finite ACL2 theorems with BDD- and SAT-based
    reasoning. This approach makes it unnecessary to deeply understand
    why a property is true, and automates the process of admitting it
    as a theorem. We use GL at Centaur Technology to verify execution
    units for x86 Integer, MMX, SSE, and floating-point arithmetic.",
  paper = "Swor11.pdf"
}

\end{chunk}
